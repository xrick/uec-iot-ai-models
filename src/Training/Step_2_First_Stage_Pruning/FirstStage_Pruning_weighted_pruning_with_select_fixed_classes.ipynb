{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9872611-fa8c-4f4d-89dc-ebea772c16a8",
   "metadata": {},
   "source": [
    "# Weight Pruning Records and Notes\n",
    "## setting\n",
    "- current pruning ratio : 0.7\n",
    "- result: to-add how to observe weight-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15eb05a5-0216-4c21-9226-27bca5bb4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import numpy as np;\n",
    "import random;\n",
    "import time;\n",
    "import torch\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4fd5d8-421c-438e-a7b9-ea48ef39fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886f8b70-1961-4c4f-823f-55393ff9c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.opts as opt;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "import th.resources.train_generator as train_generator;\n",
    "import th.resources.pruning_tools.weight_pruning as weight_pruner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd58c73b-11bf-48dc-895b-0959625f38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import common.tlopts as tlopts\n",
    "import th.resources.calculator as calc;\n",
    "from datetime import datetime;\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb31f54-0d24-4942-ad4c-f498900e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log file object\n",
    "logObj = None;\n",
    "def ChkAndCreateSingleDir(dir_path):\n",
    "    if not pathlib.Path(dir_path).is_dir():\n",
    "        os.mkdir(dir_path);\n",
    "        print(f\"'{dir_path}' folder is created.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944e5af4-56fa-45e2-8ef8-a1b3beb25156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");\n",
    "\n",
    "def getDateStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H').replace('-',\"\").replace(' ',\"\")#.replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f626e5-5ed6-4aef-908f-2a39342bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([('52',1),('56',2),('99',3)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch_select_fixed_class(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        \n",
    "        return sounds, labels;\n",
    "\n",
    "    def generate_batch_select_fixed_class(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        #two variables recording alarm and moaning sounds count\n",
    "        alarm_selected = 0;\n",
    "        moaning_selected = 0;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                # print(\"enter while true\")\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                lbl1_int = np.int16(label1);\n",
    "                lbl2_int = np.int16(label2);\n",
    "                # print(f\"label1:{label1} and label2:{label2}\")\n",
    "                # print(f\"label1:{type(label1)} and label2:{type(label2)}\")\n",
    "                if label1 != label2:\n",
    "                    # print(\"enter first layer if\");\n",
    "                    if (lbl1_int == 52 and lbl2_int ==99) or (lbl1_int == 99 and lbl2_int ==52):\n",
    "                        # print(\"enter 52 second layer if\");\n",
    "                        if (alarm_selected < moaning_selected) or (alarm_selected == moaning_selected):\n",
    "                            # print(\"enter 52 third layer if\");\n",
    "                            alarm_selected += 1;\n",
    "                            # print(f\"alarm sound selected, label1:{label1}, label2:{label2}, alarm_selected:{alarm_selected}, moaning_selected:{moaning_selected}\");\n",
    "                            # print(\"perform break at 52\");\n",
    "                            break;\n",
    "                    if (lbl1_int == 56 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 56):\n",
    "                        # print(\"enter 56 second layer if\");\n",
    "                        if (moaning_selected < alarm_selected) or (alarm_selected == moaning_selected):\n",
    "                            # print(\"enter 56 third layer if\");\n",
    "                            moaning_selected += 1;\n",
    "                            # print(f\"alarm sound selected, label1:{label1}, label2:{label2}, alarm_selected:{alarm_selected}, moaning_selected:{moaning_selected}\");\n",
    "                            # print(\"perform break at 56\");\n",
    "                            break;\n",
    "            # print(f\"escape for loop\");\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random());\n",
    "            #######make wanted class mix ration above 0.5##########\n",
    "            # iLbl1 = np.int16(label1);\n",
    "            # iLbl2 = np.int16(label2);\n",
    "            # r = 1.0;\n",
    "            # p_ratio1 = 0.4\n",
    "            # p_ratio2 = 0.6\n",
    "            # while True:\n",
    "            #     r = np.array(random.random());\n",
    "            #     if r > p_ratio1 and iLbl1 != 99 :\n",
    "            #         break;\n",
    "            #     if r < p_ratio2 and iLbl2 != 99 :\n",
    "            #         break;\n",
    "            #######################End#######################\n",
    "            # print(f\"r:{r}, lbl1:{label1}, lbl2:{label2}\")  \n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            \n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"batchIndex is {batchIndex}, total sounds is {len(sounds)}\")\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "        # print(f\"alarm_selected:{alarm_selected}, moaning_selected:{moaning_selected}\");\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e473f603-d73b-4aff-bda7-2aaf0bdab6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs = self.ch_config[-1];\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetACDNetModel(input_len=30225, nclass=50, sr=20000, channel_config=None):\n",
    "    net = ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d298ee4-f3f7-4001-8224-54b8817a2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;\n",
    "###########################################\n",
    "\n",
    "class Customed_ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(Customed_ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs =  n_class #self.ch_config[-1];\n",
    "        ch_confing_10 = 512 #8 * 64\n",
    "        ch_n_class = n_class\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(ch_confing_10, ch_n_class, (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, ch_n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (2,4)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"sfeb:\\n{list(self.sfeb.children())}\");\n",
    "        # print(f\"input x shape:{x.size()}\");\n",
    "        \"\"\"\n",
    "        input dim should be input x shape:torch.Size([32, 1, 1, 30225])\n",
    "        if you got input x shape:[32, 30225, 1, 1], that is wrong.\n",
    "        \"\"\"\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetCustomedACDNetModel(input_len=30225, nclass=3, sr=20000, channel_config=None):\n",
    "    net = Customed_ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22423f68-d925-450f-a24b-567422ef736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "   \n",
    "    opt.batchSize = 64;\n",
    "    opt.LR = 0.1;\n",
    "    opt.momentum = 0.9;\n",
    "    opt.weightDecay = 5e-4;\n",
    "    opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "    opt.warmup = 10;\n",
    "    opt.nEpochs = 500;\n",
    "    # opt.LR = 0.1;\n",
    "    # opt.momentum = 0.09;\n",
    "    # opt.nEpochs = 1000;#2000;\n",
    "    # opt.schedule = [0.3, 0.6, 0.9];\n",
    "    # opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 3#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6e457-9c01-4aeb-bf23-6ae046b2d781",
   "metadata": {},
   "source": [
    "- <font size=2 color='#FF6600'>For the accuracy and model generation capacity it is better to add more data to the training and validation datasets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a4c5d5-e2b4-4775-b7f6-4de0c5147e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(\"../../../datasets/CurrentUse/generated_datasets/train/version6_office/single_fold_train_20240509143202.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8011c4-e99c-4675-b692-204ee0cfd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer:\n",
    "    global logObj\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        #Conditional compression settings\n",
    "        # self.opt.LR = 0.01;\n",
    "        # self.opt.momentum = 0.09;\n",
    "        # self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "        # self.opt.warmup = 0;\n",
    "        self.opt.prune_algo = 'l0norm';\n",
    "        self.opt.prune_interval = 1;\n",
    "        # self.opt.nEpochs = 1000;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "        self.trainGen = getTrainGen(opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        # if torch.device == \"cuda:0\":\n",
    "        #     self.device = '\"cuda:0\"'\n",
    "        # elif torch.device == \"mps\":\n",
    "        #     self.device = \"mps\"\n",
    "        # else:\n",
    "        #     self.device = \"cpu\"\n",
    "        self.device=\"cuda:0\"\n",
    "        print(f\"In PruningTrainer:: current used device:{self.device}\")\n",
    "        # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.start_time = time.time();\n",
    "\n",
    "    def PruneAndTrain(self):\n",
    "        self.load_test_data();\n",
    "        print(self.device);\n",
    "        loss_func = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "\n",
    "        #Load saved model dict\n",
    "        net = GetCustomedACDNetModel()#GetACDNetModel()\n",
    "        net.load_state_dict(torch.load(\"../../../trained_models/step_1_base_train/base_train_lr0.1_bs64_wd0.0005_20240510094900/sp_ai_model_3Classes_hacc93.04029083251953_valacc_92.67399597167969_tracc_90.26442307692307_730th_epoch.pt\", map_location=self.device)['weight']);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength))\n",
    "        net.eval();\n",
    "        val_acc, val_loss = self.__validate(net, loss_func);\n",
    "        print('Testing - Val: Loss {:.3f}  Acc(top1) {:.3f}%'.format(val_loss, val_acc));\n",
    "        net.train();\n",
    "\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True)\n",
    "\n",
    "        weight_name = [\"weight\"]# if not self.opt.factorize else [\"weightA\", \"weightB\", \"weightC\"]\n",
    "        layers_n = weight_pruner.layers_n(net, param_name=[\"weight\"])[1];\n",
    "        all_num = sum(layers_n.values());\n",
    "        print(\"\\t TOTAL PRUNABLE PARAMS: {}\".format(all_num));\n",
    "        print(\"\\t PRUNE RATIO :{}\".format(self.opt.prune_ratio));\n",
    "        sparse_factor = int(all_num * (1-self.opt.prune_ratio));\n",
    "        print(\"\\t SPARSE FACTOR: {}\".format(sparse_factor));\n",
    "        model_size = (sparse_factor * 4)/1024**2;\n",
    "        print(\"\\t MODEL SIZE: {:.2f} MB\".format(model_size));\n",
    "        prune_algo = getattr(weight_pruner, self.opt.prune_algo);\n",
    "        prune_func = lambda m: prune_algo(m, sparse_factor, param_name=weight_name);\n",
    "\n",
    "        for epoch_idx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epoch_idx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            net.train();\n",
    "            for batch_idx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batch_idx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.device);\n",
    "                y = torch.tensor(y).to(self.device);\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                # outputs = net(x);#in office and use cpu\n",
    "                x = x.type(torch.FloatTensor)\n",
    "                outputs = net(x);\n",
    "                res_y = y.argmax(dim=1)\n",
    "                res_y = res_y.type(torch.FloatTensor)\n",
    "                running_acc += ((( outputs.data.argmax(dim=1) == res_y)*1).float().mean()).item();\n",
    "                y = y.type(torch.FloatTensor)\n",
    "                loss = loss_func(outputs.log(), y);\n",
    "\n",
    "                loss.backward();\n",
    "                optimizer.step();\n",
    "\n",
    "                running_loss += loss.item();\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prune_func(net);\n",
    "\n",
    "            prune_func(net)\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, loss_func);\n",
    "            #Save best model\n",
    "            self.__save_model(val_acc, tr_acc, epoch_idx, net);\n",
    "\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epoch_idx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - self.start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load(\"../../../datasets/CurrentUse/generated_datasets/val/version6_office/final_single_val_20240509144233.npz\", allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            self.testX = torch.tensor(dataX).to(self.device);\n",
    "            self.testY = torch.FloatTensor(data['y']).to(self.device);\n",
    "\n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __validate(self, net, lossFunc):\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops\n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                x = torch.tensor(x)\n",
    "                x = x.type(torch.FloatTensor) # use apple mp2\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "            acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            # print(f\"y_pred:{type(y_pred)}, y_target:{type(y_target)}\")\n",
    "            y_target = y_target.cpu() #use apple m2, in office use cuda\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def __on_epoch_end(self, epoch_start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - epoch_start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        total_time = time.time() - self.start_time;\n",
    "        line = '{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            U.to_hms(total_time), epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def __save_model(self, acc, train_acc, epochIdx, net):\n",
    "        if acc > self.bestAcc and acc > self.opt.first_save_acc:\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            self.__do_save_model(acc, train_acc, self.bestAccEpoch, net);\n",
    "        else:\n",
    "            if acc > self.opt.save_val_acc and train_acc > self.opt.save_train_acc: \n",
    "                self.__do_save_model(acc, train_acc, epochIdx, net);\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def __do_save_model(self, acc, tr_acc, epochIdx, net):\n",
    "        save_model_name = self.opt.model_name.format(self.bestAcc, acc, tr_acc, epochIdx, genDataTimeStr());\n",
    "        save_model_fullpath = self.opt.save_dir + save_model_name;\n",
    "        print(f\"save model to {save_model_fullpath}\")\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, save_model_fullpath);\n",
    "        logObj.write(f\"save model:{self.opt.model_name}, bestAcc:{self.bestAcc}, valAcc:{acc}, trainAcc:{tr_acc}, record@{epochIdx}-epoch\");\n",
    "        logObj.write(\"\\n\");\n",
    "        logObj.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb69075-8799-469d-b9c1-a545cee964d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n-----------------------------------------------------------------------------------------------\\npruning ration : 0.9\\nfinal accuracy : \\nepoch: \\nself.opt.LR = 0.01;\\nopt.momentum = 0.9;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'l0norm';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n########################## Trainig Data Version 4 ##########################\\nopt.batchSize = 64;\\nopt.LR = 0.1;\\nopt.momentum = 0.9;\\nopt.weightDecay = 5e-4;\\nopt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\\nopt.warmup = 10;\\nopt.nEpochs = 600;\\n==============================================================\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------\n",
    "pruning ration : 0.9\n",
    "final accuracy : \n",
    "epoch: \n",
    "self.opt.LR = 0.01;\n",
    "opt.momentum = 0.9;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'l0norm';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "########################## Trainig Data Version 4 ##########################\n",
    "opt.batchSize = 64;\n",
    "opt.LR = 0.1;\n",
    "opt.momentum = 0.9;\n",
    "opt.weightDecay = 5e-4;\n",
    "opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "opt.warmup = 10;\n",
    "opt.nEpochs = 600;\n",
    "==============================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9fd15f0-cbdf-4b2f-b3c9-96f694eb4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global logObj;\n",
    "    opt = getOpts()\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    opt.trainer = None\n",
    "    opt.prune_ratio = 0.8\n",
    "    opt.first_save_acc = 90.0;\n",
    "    opt.save_val_acc = 88.0;\n",
    "    opt.save_train_acc = 86.0;\n",
    "    trainStartTime = getDateStr();\n",
    "    save_dir = \"../../../trained_models/step_2_first_stage_pruning/pruning_time_{}_prunratio{}/\".format(trainStartTime,opt.prune_ratio*100)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    opt.save_dir = save_dir;\n",
    "    opt.model_name = \"sp_ai_model_first_stage_prun_haacc_{}_valacc{}_tracc{}_epoch_{}_{}.pt\";\n",
    "    print(\"Initializing PruneAndTrain Object.....\")\n",
    "    trainer = PruningTrainer(opt)#TLTrainer(opt)\n",
    "    print(\"Start to pruning.....\")\n",
    "    logSaveDir = \"./first_stage_pruning_logs/\"\n",
    "    ChkAndCreateSingleDir(logSaveDir);\n",
    "    logName = \"FirstPruningLog_{}.log\".format(trainStartTime);\n",
    "    logObj = open(os.path.join(logSaveDir,logName),'w');\n",
    "    trainer.PruneAndTrain();\n",
    "    logObj.flush();\n",
    "    logObj.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8780ef94-ad0c-46bc-abb9-a5573b8b3480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PruneAndTrain Object.....\n",
      "length of samples:803\n",
      "In PruningTrainer:: current used device:cuda:0\n",
      "Start to pruning.....\n",
      "cuda:0\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (8, 1, 15109)         72    1,087,848\n",
      "  BatchNorm2d-2     (8, 1, 15109)     (8, 1, 15109)         16            0\n",
      "         ReLu-3     (8, 1, 15109)     (8, 1, 15109)          0      120,872\n",
      "       Conv2d-4     (8, 1, 15109)     (64, 1, 7553)      2,560   19,335,680\n",
      "  BatchNorm2d-5     (64, 1, 7553)     (64, 1, 7553)        128            0\n",
      "         ReLu-6     (64, 1, 7553)     (64, 1, 7553)          0      483,392\n",
      "    MaxPool2d-7     (64, 1, 7553)      (64, 1, 151)          0      483,200\n",
      "      Permute-8      (64, 1, 151)      (1, 64, 151)          0            0\n",
      "       Conv2d-9      (1, 64, 151)     (32, 64, 151)        288    2,783,232\n",
      " BatchNorm2d-10     (32, 64, 151)     (32, 64, 151)         64            0\n",
      "        ReLu-11     (32, 64, 151)     (32, 64, 151)          0      309,248\n",
      "   MaxPool2d-12     (32, 64, 151)      (32, 32, 75)          0      307,200\n",
      "      Conv2d-13      (32, 32, 75)      (64, 32, 75)     18,432   44,236,800\n",
      " BatchNorm2d-14      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-15      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "      Conv2d-16      (64, 32, 75)      (64, 32, 75)     36,864   88,473,600\n",
      " BatchNorm2d-17      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-18      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "   MaxPool2d-19      (64, 32, 75)      (64, 16, 37)          0      151,552\n",
      "      Conv2d-20      (64, 16, 37)     (128, 16, 37)     73,728   43,646,976\n",
      " BatchNorm2d-21     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-22     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "      Conv2d-23     (128, 16, 37)     (128, 16, 37)    147,456   87,293,952\n",
      " BatchNorm2d-24     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-25     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "   MaxPool2d-26     (128, 16, 37)      (128, 8, 18)          0       73,728\n",
      "      Conv2d-27      (128, 8, 18)      (256, 8, 18)    294,912   42,467,328\n",
      " BatchNorm2d-28      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-29      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "      Conv2d-30      (256, 8, 18)      (256, 8, 18)    589,824   84,934,656\n",
      " BatchNorm2d-31      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-32      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "   MaxPool2d-33      (256, 8, 18)       (256, 4, 9)          0       36,864\n",
      "      Conv2d-34       (256, 4, 9)       (512, 4, 9)  1,179,648   42,467,328\n",
      " BatchNorm2d-35       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-36       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "      Conv2d-37       (512, 4, 9)       (512, 4, 9)  2,359,296   84,934,656\n",
      " BatchNorm2d-38       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-39       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "   MaxPool2d-40       (512, 4, 9)       (512, 2, 4)          0       16,384\n",
      "      Conv2d-41       (512, 2, 4)         (3, 2, 4)      1,536       12,288\n",
      " BatchNorm2d-42         (3, 2, 4)         (3, 2, 4)          6            0\n",
      "        ReLu-43         (3, 2, 4)         (3, 2, 4)          0           24\n",
      "   AvgPool2d-44         (3, 2, 4)         (3, 1, 1)          0           24\n",
      "     Flatten-45         (3, 1, 1)            (1, 3)          0            0\n",
      "      Linear-46            (1, 3)            (1, 3)         12           12\n",
      "     Softmax-47            (1, 3)            (1, 3)          0            3\n",
      "==============================================================================\n",
      "Total Params: 4,708,682\n",
      "Total FLOPs : 544,226,191\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 17.96\n",
      "Total size (MB) : 18.08\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68023/3754084896.py:133: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Val: Loss nan  Acc(top1) 92.674%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TOTAL PRUNABLE PARAMS: 4704625\n",
      "\t PRUNE RATIO :0.8\n",
      "\t SPARSE FACTOR: 940924\n",
      "\t MODEL SIZE: 3.59 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68023/3754084896.py:133: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m18s Epoch: 1/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 79.49% | HA 0.00@0\n",
      "0m33s Epoch: 2/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 89.78% | Val: Loss nan  Acc(top1) 87.91% | HA 0.00@0\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc93.04029083251953_tracc88.58173076923077_epoch_3_20240510115029.pt\n",
      "0m48s Epoch: 3/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.10  Acc 88.58% | Val: Loss nan  Acc(top1) 93.04% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.27838897705078_tracc87.01923076923077_epoch_3_20240510115044.pt\n",
      "1m03s Epoch: 4/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.10  Acc 87.02% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.27838897705078_tracc86.65865384615384_epoch_4_20240510115059.pt\n",
      "1m18s Epoch: 5/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.10  Acc 86.66% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.84249114990234_tracc87.13942307692307_epoch_5_20240510115114.pt\n",
      "1m34s Epoch: 6/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.10  Acc 87.14% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "1m49s Epoch: 7/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.10  Acc 88.10% | Val: Loss nan  Acc(top1) 83.88% | HA 93.04@3\n",
      "2m04s Epoch: 8/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 63.37% | HA 93.04@3\n",
      "2m19s Epoch: 9/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.09  Acc 84.74% | Val: Loss nan  Acc(top1) 79.85% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc92.30769348144531_tracc87.5_epoch_9_20240510115215.pt\n",
      "2m34s Epoch: 10/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 92.31% | HA 93.04@3\n",
      "2m50s Epoch: 11/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 79.45% | Val: Loss nan  Acc(top1) 71.06% | HA 93.04@3\n",
      "3m05s Epoch: 12/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.27  Acc 74.04% | Val: Loss nan  Acc(top1) 83.15% | HA 93.04@3\n",
      "3m20s Epoch: 13/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.24  Acc 76.56% | Val: Loss nan  Acc(top1) 58.61% | HA 93.04@3\n",
      "3m35s Epoch: 14/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.23  Acc 77.64% | Val: Loss nan  Acc(top1) 71.06% | HA 93.04@3\n",
      "3m50s Epoch: 15/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.21  Acc 77.16% | Val: Loss nan  Acc(top1) 84.25% | HA 93.04@3\n",
      "4m05s Epoch: 16/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 79.45% | Val: Loss nan  Acc(top1) 46.15% | HA 93.04@3\n",
      "4m20s Epoch: 17/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 78.49% | Val: Loss nan  Acc(top1) 84.98% | HA 93.04@3\n",
      "4m36s Epoch: 18/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 77.88% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "4m51s Epoch: 19/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 77.40% | Val: Loss nan  Acc(top1) 83.15% | HA 93.04@3\n",
      "5m06s Epoch: 20/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.23  Acc 76.92% | Val: Loss nan  Acc(top1) 72.16% | HA 93.04@3\n",
      "5m21s Epoch: 21/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 77.52% | Val: Loss nan  Acc(top1) 80.95% | HA 93.04@3\n",
      "5m36s Epoch: 22/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.22  Acc 77.04% | Val: Loss nan  Acc(top1) 81.68% | HA 93.04@3\n",
      "5m52s Epoch: 23/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 81.25% | Val: Loss nan  Acc(top1) 81.32% | HA 93.04@3\n",
      "6m07s Epoch: 24/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.21  Acc 77.28% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "6m23s Epoch: 25/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 82.81% | Val: Loss nan  Acc(top1) 79.12% | HA 93.04@3\n",
      "6m38s Epoch: 26/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 80.29% | Val: Loss nan  Acc(top1) 83.15% | HA 93.04@3\n",
      "6m54s Epoch: 27/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 77.88% | Val: Loss nan  Acc(top1) 83.52% | HA 93.04@3\n",
      "7m09s Epoch: 28/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 78.97% | Val: Loss nan  Acc(top1) 83.15% | HA 93.04@3\n",
      "7m24s Epoch: 29/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 80.05% | Val: Loss nan  Acc(top1) 70.70% | HA 93.04@3\n",
      "7m39s Epoch: 30/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 80.29% | Val: Loss nan  Acc(top1) 83.88% | HA 93.04@3\n",
      "7m55s Epoch: 31/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 78.37% | Val: Loss nan  Acc(top1) 82.42% | HA 93.04@3\n",
      "8m10s Epoch: 32/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 79.33% | Val: Loss nan  Acc(top1) 72.16% | HA 93.04@3\n",
      "8m25s Epoch: 33/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 79.69% | Val: Loss nan  Acc(top1) 86.08% | HA 93.04@3\n",
      "8m40s Epoch: 34/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 80.41% | Val: Loss nan  Acc(top1) 82.78% | HA 93.04@3\n",
      "8m56s Epoch: 35/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 78.97% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "9m11s Epoch: 36/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 76.92% | Val: Loss nan  Acc(top1) 78.02% | HA 93.04@3\n",
      "9m26s Epoch: 37/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 81.13% | Val: Loss nan  Acc(top1) 81.68% | HA 93.04@3\n",
      "9m42s Epoch: 38/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 79.33% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "9m57s Epoch: 39/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 79.57% | Val: Loss nan  Acc(top1) 81.68% | HA 93.04@3\n",
      "10m12s Epoch: 40/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 79.93% | Val: Loss nan  Acc(top1) 71.79% | HA 93.04@3\n",
      "10m27s Epoch: 41/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 82.33% | Val: Loss nan  Acc(top1) 77.29% | HA 93.04@3\n",
      "10m43s Epoch: 42/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 79.81% | Val: Loss nan  Acc(top1) 77.66% | HA 93.04@3\n",
      "10m58s Epoch: 43/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 79.21% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "11m13s Epoch: 44/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 80.53% | Val: Loss nan  Acc(top1) 76.92% | HA 93.04@3\n",
      "11m28s Epoch: 45/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 78.12% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "11m44s Epoch: 46/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 79.09% | Val: Loss nan  Acc(top1) 77.66% | HA 93.04@3\n",
      "11m59s Epoch: 47/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 79.45% | Val: Loss nan  Acc(top1) 80.59% | HA 93.04@3\n",
      "12m15s Epoch: 48/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 80.29% | Val: Loss nan  Acc(top1) 80.59% | HA 93.04@3\n",
      "12m30s Epoch: 49/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 79.09% | Val: Loss nan  Acc(top1) 83.52% | HA 93.04@3\n",
      "12m45s Epoch: 50/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 79.33% | Val: Loss nan  Acc(top1) 80.22% | HA 93.04@3\n",
      "13m01s Epoch: 51/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 76.44% | Val: Loss nan  Acc(top1) 52.01% | HA 93.04@3\n",
      "13m16s Epoch: 52/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 78.61% | Val: Loss nan  Acc(top1) 74.73% | HA 93.04@3\n",
      "13m31s Epoch: 53/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 78.97% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "13m46s Epoch: 54/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 80.17% | Val: Loss nan  Acc(top1) 82.78% | HA 93.04@3\n",
      "14m01s Epoch: 55/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 81.37% | Val: Loss nan  Acc(top1) 82.42% | HA 93.04@3\n",
      "14m17s Epoch: 56/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 78.61% | Val: Loss nan  Acc(top1) 82.78% | HA 93.04@3\n",
      "14m32s Epoch: 57/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 79.81% | Val: Loss nan  Acc(top1) 85.35% | HA 93.04@3\n",
      "14m47s Epoch: 58/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 80.65% | Val: Loss nan  Acc(top1) 75.82% | HA 93.04@3\n",
      "15m03s Epoch: 59/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 78.25% | Val: Loss nan  Acc(top1) 81.32% | HA 93.04@3\n",
      "15m18s Epoch: 60/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.16  Acc 83.05% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "15m34s Epoch: 61/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 81.25% | Val: Loss nan  Acc(top1) 66.67% | HA 93.04@3\n",
      "15m49s Epoch: 62/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 77.52% | Val: Loss nan  Acc(top1) 72.89% | HA 93.04@3\n",
      "16m04s Epoch: 63/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 81.73% | Val: Loss nan  Acc(top1) 73.99% | HA 93.04@3\n",
      "16m20s Epoch: 64/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.22  Acc 77.76% | Val: Loss nan  Acc(top1) 62.64% | HA 93.04@3\n",
      "16m35s Epoch: 65/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.16  Acc 80.41% | Val: Loss nan  Acc(top1) 80.59% | HA 93.04@3\n",
      "16m50s Epoch: 66/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 82.81% | Val: Loss nan  Acc(top1) 81.68% | HA 93.04@3\n",
      "17m05s Epoch: 67/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 80.41% | Val: Loss nan  Acc(top1) 81.32% | HA 93.04@3\n",
      "17m21s Epoch: 68/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 79.09% | Val: Loss nan  Acc(top1) 76.92% | HA 93.04@3\n",
      "17m36s Epoch: 69/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.20  Acc 78.49% | Val: Loss nan  Acc(top1) 82.42% | HA 93.04@3\n",
      "17m51s Epoch: 70/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 80.65% | Val: Loss nan  Acc(top1) 87.18% | HA 93.04@3\n",
      "18m07s Epoch: 71/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.17  Acc 82.57% | Val: Loss nan  Acc(top1) 84.98% | HA 93.04@3\n",
      "18m22s Epoch: 72/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 79.33% | Val: Loss nan  Acc(top1) 82.42% | HA 93.04@3\n",
      "18m37s Epoch: 73/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.19  Acc 78.61% | Val: Loss nan  Acc(top1) 61.17% | HA 93.04@3\n",
      "18m53s Epoch: 74/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 81.49% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "19m08s Epoch: 75/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.1  Loss 0.18  Acc 78.49% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "19m23s Epoch: 76/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.16  Acc 84.13% | Val: Loss nan  Acc(top1) 86.81% | HA 93.04@3\n",
      "19m38s Epoch: 77/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.16  Acc 81.97% | Val: Loss nan  Acc(top1) 86.81% | HA 93.04@3\n",
      "19m54s Epoch: 78/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.15  Acc 82.09% | Val: Loss nan  Acc(top1) 87.18% | HA 93.04@3\n",
      "20m09s Epoch: 79/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.15  Acc 82.57% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "20m25s Epoch: 80/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 84.74% | Val: Loss nan  Acc(top1) 86.81% | HA 93.04@3\n",
      "20m40s Epoch: 81/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.15  Acc 83.77% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "20m55s Epoch: 82/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 84.62% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "21m11s Epoch: 83/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 82.33% | Val: Loss nan  Acc(top1) 86.81% | HA 93.04@3\n",
      "21m26s Epoch: 84/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.15  Acc 82.45% | Val: Loss nan  Acc(top1) 87.18% | HA 93.04@3\n",
      "21m42s Epoch: 85/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 82.57% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "21m57s Epoch: 86/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 85.10% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "22m13s Epoch: 87/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 85.10% | Val: Loss nan  Acc(top1) 81.68% | HA 93.04@3\n",
      "22m28s Epoch: 88/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 83.29% | Val: Loss nan  Acc(top1) 85.71% | HA 93.04@3\n",
      "22m44s Epoch: 89/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 84.62% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "22m59s Epoch: 90/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 84.01% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_90_20240510121255.pt\n",
      "23m15s Epoch: 91/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "23m30s Epoch: 92/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 84.13% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.29807692307693_epoch_92_20240510121326.pt\n",
      "23m45s Epoch: 93/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.30% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "24m01s Epoch: 94/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 85.46% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "24m16s Epoch: 95/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 84.50% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "24m31s Epoch: 96/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 85.58% | Val: Loss nan  Acc(top1) 84.62% | HA 93.04@3\n",
      "24m46s Epoch: 97/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 83.89% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.53846153846155_epoch_97_20240510121442.pt\n",
      "25m01s Epoch: 98/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "25m17s Epoch: 99/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 84.13% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "25m32s Epoch: 100/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 84.50% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "25m47s Epoch: 101/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 83.65% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.65865384615384_epoch_101_20240510121543.pt\n",
      "26m03s Epoch: 102/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.66% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "26m18s Epoch: 103/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 84.38% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "26m34s Epoch: 104/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 85.46% | Val: Loss nan  Acc(top1) 86.08% | HA 93.04@3\n",
      "26m49s Epoch: 105/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 84.98% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "27m04s Epoch: 106/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 84.50% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "27m19s Epoch: 107/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 83.89% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "27m35s Epoch: 108/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 83.77% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "27m50s Epoch: 109/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 82.93% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "28m06s Epoch: 110/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 83.65% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "28m21s Epoch: 111/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 87.74% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "28m36s Epoch: 112/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 85.71% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.27838897705078_tracc86.0576923076923_epoch_112_20240510121832.pt\n",
      "28m52s Epoch: 113/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.06% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "29m07s Epoch: 114/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 84.98% | HA 93.04@3\n",
      "29m22s Epoch: 115/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 84.50% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "29m38s Epoch: 116/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.14  Acc 82.69% | Val: Loss nan  Acc(top1) 86.08% | HA 93.04@3\n",
      "29m53s Epoch: 117/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 83.41% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "30m09s Epoch: 118/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 83.89% | Val: Loss nan  Acc(top1) 84.25% | HA 93.04@3\n",
      "30m24s Epoch: 119/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 85.34% | Val: Loss nan  Acc(top1) 82.05% | HA 93.04@3\n",
      "30m39s Epoch: 120/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 86.42% | Val: Loss nan  Acc(top1) 84.98% | HA 93.04@3\n",
      "30m54s Epoch: 121/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 85.70% | Val: Loss nan  Acc(top1) 91.58% | HA 93.04@3\n",
      "31m09s Epoch: 122/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 84.62% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc91.57508850097656_tracc86.65865384615384_epoch_122_20240510122105.pt\n",
      "31m25s Epoch: 123/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.66% | Val: Loss nan  Acc(top1) 91.58% | HA 93.04@3\n",
      "31m40s Epoch: 124/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 85.94% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "31m55s Epoch: 125/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 87.14% | Val: Loss nan  Acc(top1) 86.81% | HA 93.04@3\n",
      "32m11s Epoch: 126/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 84.98% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "32m26s Epoch: 127/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 84.38% | Val: Loss nan  Acc(top1) 85.71% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.01923076923077_epoch_127_20240510122222.pt\n",
      "32m41s Epoch: 128/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 87.02% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "32m57s Epoch: 129/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 84.38% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.84249114990234_tracc86.77884615384616_epoch_129_20240510122252.pt\n",
      "33m12s Epoch: 130/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.78% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "33m27s Epoch: 131/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 83.89% | Val: Loss nan  Acc(top1) 86.08% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.89903846153845_epoch_131_20240510122323.pt\n",
      "33m42s Epoch: 132/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.0576923076923_epoch_132_20240510122338.pt\n",
      "33m58s Epoch: 133/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "34m13s Epoch: 134/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 85.46% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.53846153846155_epoch_134_20240510122409.pt\n",
      "34m28s Epoch: 135/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.65865384615384_epoch_135_20240510122424.pt\n",
      "34m43s Epoch: 136/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.66% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "34m59s Epoch: 137/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 82.21% | Val: Loss nan  Acc(top1) 84.98% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.64469146728516_tracc87.3798076923077_epoch_137_20240510122454.pt\n",
      "35m14s Epoch: 138/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 87.38% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "35m29s Epoch: 139/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 85.71% | HA 93.04@3\n",
      "35m44s Epoch: 140/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 85.46% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "35m59s Epoch: 141/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 85.10% | Val: Loss nan  Acc(top1) 91.21% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.17788461538461_epoch_141_20240510122555.pt\n",
      "36m14s Epoch: 142/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.18% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "36m29s Epoch: 143/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 86.90% | Val: Loss nan  Acc(top1) 76.92% | HA 93.04@3\n",
      "36m44s Epoch: 144/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 87.14% | Val: Loss nan  Acc(top1) 86.45% | HA 93.04@3\n",
      "36m59s Epoch: 145/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 85.82% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "37m15s Epoch: 146/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.13  Acc 82.21% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.89903846153845_epoch_146_20240510122710.pt\n",
      "37m30s Epoch: 147/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "37m45s Epoch: 148/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.11  Acc 85.46% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc87.3798076923077_epoch_148_20240510122740.pt\n",
      "38m00s Epoch: 149/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 87.38% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "38m15s Epoch: 150/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.010000000000000002  Loss 0.12  Acc 84.25% | Val: Loss nan  Acc(top1) 86.45% | HA 93.04@3\n",
      "38m30s Epoch: 151/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.70% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "38m46s Epoch: 152/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 83.77% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "39m01s Epoch: 153/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "39m16s Epoch: 154/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 84.98% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.3798076923077_epoch_154_20240510122912.pt\n",
      "39m31s Epoch: 155/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 87.38% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "39m46s Epoch: 156/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.46% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.86057692307693_epoch_156_20240510122942.pt\n",
      "40m01s Epoch: 157/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 87.86% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "40m17s Epoch: 158/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 84.86% | Val: Loss nan  Acc(top1) 91.21% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.01923076923077_epoch_158_20240510123013.pt\n",
      "40m32s Epoch: 159/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.02% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "40m47s Epoch: 160/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 84.01% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "41m02s Epoch: 161/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.22% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc88.46153846153845_epoch_161_20240510123058.pt\n",
      "41m18s Epoch: 162/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 88.46% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "41m33s Epoch: 163/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.89903846153845_epoch_163_20240510123128.pt\n",
      "41m48s Epoch: 164/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.17788461538461_epoch_164_20240510123144.pt\n",
      "42m03s Epoch: 165/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.18% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "42m18s Epoch: 166/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 84.38% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "42m33s Epoch: 167/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 84.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "42m48s Epoch: 168/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 84.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.0576923076923_epoch_168_20240510123243.pt\n",
      "43m03s Epoch: 169/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.6201923076923_epoch_169_20240510123258.pt\n",
      "43m18s Epoch: 170/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc88.22115384615384_epoch_170_20240510123314.pt\n",
      "43m33s Epoch: 171/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 88.22% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "43m48s Epoch: 172/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 85.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.01923076923077_epoch_172_20240510123344.pt\n",
      "44m03s Epoch: 173/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.02% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.01923076923077_epoch_173_20240510123359.pt\n",
      "44m18s Epoch: 174/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.02% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "44m33s Epoch: 175/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.58% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.53846153846155_epoch_175_20240510123429.pt\n",
      "44m48s Epoch: 176/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc91.20879364013672_tracc87.01923076923077_epoch_176_20240510123444.pt\n",
      "45m03s Epoch: 177/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 87.02% | Val: Loss nan  Acc(top1) 91.21% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.25961538461539_epoch_177_20240510123459.pt\n",
      "45m18s Epoch: 178/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.26% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "45m33s Epoch: 179/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc88.10096153846155_epoch_179_20240510123529.pt\n",
      "45m48s Epoch: 180/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 88.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.53846153846155_epoch_180_20240510123544.pt\n",
      "46m03s Epoch: 181/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.54% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.6201923076923_epoch_181_20240510123559.pt\n",
      "46m18s Epoch: 182/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc88.22115384615384_epoch_182_20240510123614.pt\n",
      "46m33s Epoch: 183/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 88.22% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc88.22115384615384_epoch_183_20240510123629.pt\n",
      "46m48s Epoch: 184/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 88.22% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.86057692307693_epoch_184_20240510123644.pt\n",
      "47m03s Epoch: 185/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.86% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "47m18s Epoch: 186/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 84.25% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.3798076923077_epoch_186_20240510123714.pt\n",
      "47m33s Epoch: 187/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.38% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.89903846153845_epoch_187_20240510123729.pt\n",
      "47m48s Epoch: 188/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.89903846153845_epoch_188_20240510123743.pt\n",
      "48m03s Epoch: 189/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.90% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.6201923076923_epoch_189_20240510123758.pt\n",
      "48m18s Epoch: 190/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "48m33s Epoch: 191/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.70% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "48m48s Epoch: 192/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 85.46% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.77884615384616_epoch_192_20240510123843.pt\n",
      "49m03s Epoch: 193/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.78% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "49m17s Epoch: 194/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_194_20240510123913.pt\n",
      "49m32s Epoch: 195/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "49m47s Epoch: 196/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.84249114990234_tracc86.41826923076923_epoch_196_20240510123943.pt\n",
      "50m02s Epoch: 197/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.42% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.3798076923077_epoch_197_20240510123958.pt\n",
      "50m17s Epoch: 198/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.38% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "50m32s Epoch: 199/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 85.94% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "50m47s Epoch: 200/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 85.70% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_200_20240510124042.pt\n",
      "51m02s Epoch: 201/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.5_epoch_201_20240510124057.pt\n",
      "51m17s Epoch: 202/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.50% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.77884615384616_epoch_202_20240510124112.pt\n",
      "51m32s Epoch: 203/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc88.22115384615384_epoch_203_20240510124127.pt\n",
      "51m47s Epoch: 204/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 88.22% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "52m01s Epoch: 205/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 85.94% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.64469146728516_tracc88.10096153846155_epoch_205_20240510124157.pt\n",
      "52m17s Epoch: 206/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 88.10% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.89903846153845_epoch_206_20240510124212.pt\n",
      "52m32s Epoch: 207/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.90% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.01923076923077_epoch_207_20240510124227.pt\n",
      "52m47s Epoch: 208/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 87.02% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "53m02s Epoch: 209/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.13  Acc 84.50% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "53m17s Epoch: 210/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 84.86% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.65865384615384_epoch_210_20240510124312.pt\n",
      "53m32s Epoch: 211/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "53m47s Epoch: 212/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.12  Acc 85.82% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.29807692307693_epoch_212_20240510124342.pt\n",
      "54m02s Epoch: 213/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.0576923076923_epoch_213_20240510124358.pt\n",
      "54m17s Epoch: 214/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "54m32s Epoch: 215/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.89903846153845_epoch_215_20240510124428.pt\n",
      "54m47s Epoch: 216/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc87.25961538461539_epoch_216_20240510124443.pt\n",
      "55m02s Epoch: 217/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 87.26% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.41826923076923_epoch_217_20240510124458.pt\n",
      "55m17s Epoch: 218/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.42% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.53846153846155_epoch_218_20240510124513.pt\n",
      "55m32s Epoch: 219/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.54% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.17788461538461_epoch_219_20240510124528.pt\n",
      "55m47s Epoch: 220/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc88.22115384615384_epoch_220_20240510124543.pt\n",
      "56m02s Epoch: 221/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 88.22% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.0576923076923_epoch_221_20240510124558.pt\n",
      "56m17s Epoch: 222/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.06% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc88.10096153846155_epoch_222_20240510124613.pt\n",
      "56m32s Epoch: 223/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 88.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "56m47s Epoch: 224/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.41826923076923_epoch_224_20240510124643.pt\n",
      "57m02s Epoch: 225/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.0010000000000000002  Loss 0.10  Acc 86.42% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.13942307692307_epoch_225_20240510124658.pt\n",
      "57m17s Epoch: 226/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.14% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "57m32s Epoch: 227/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 84.86% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "57m47s Epoch: 228/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 85.10% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.98076923076923_epoch_228_20240510124743.pt\n",
      "58m02s Epoch: 229/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_229_20240510124758.pt\n",
      "58m17s Epoch: 230/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.0576923076923_epoch_230_20240510124813.pt\n",
      "58m32s Epoch: 231/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "58m47s Epoch: 232/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.46% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "59m02s Epoch: 233/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.82% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "59m17s Epoch: 234/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.46% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc89.0625_epoch_234_20240510124913.pt\n",
      "59m32s Epoch: 235/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.64469146728516_tracc86.77884615384616_epoch_235_20240510124928.pt\n",
      "59m47s Epoch: 236/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc88.10096153846155_epoch_236_20240510124942.pt\n",
      "1h00m Epoch: 237/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 88.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.41826923076923_epoch_237_20240510124957.pt\n",
      "1h00m Epoch: 238/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.42% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h00m Epoch: 239/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 84.38% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h00m Epoch: 240/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.34% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_240_20240510125042.pt\n",
      "1h01m Epoch: 241/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.77884615384616_epoch_241_20240510125057.pt\n",
      "1h01m Epoch: 242/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.74038461538461_epoch_242_20240510125112.pt\n",
      "1h01m Epoch: 243/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.74% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h01m Epoch: 244/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h02m Epoch: 245/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h02m Epoch: 246/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h02m Epoch: 247/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h02m Epoch: 248/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 84.86% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h03m Epoch: 249/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.34% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h03m Epoch: 250/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.22% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc87.5_epoch_250_20240510125312.pt\n",
      "1h03m Epoch: 251/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h03m Epoch: 252/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 85.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.29807692307693_epoch_252_20240510125342.pt\n",
      "1h04m Epoch: 253/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 86.30% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h04m Epoch: 254/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.22% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h04m Epoch: 255/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 84.74% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.86057692307693_epoch_255_20240510125427.pt\n",
      "1h04m Epoch: 256/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 87.86% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.5_epoch_256_20240510125442.pt\n",
      "1h05m Epoch: 257/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.50% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.84249114990234_tracc86.29807692307693_epoch_257_20240510125457.pt\n",
      "1h05m Epoch: 258/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.17788461538461_epoch_258_20240510125513.pt\n",
      "1h05m Epoch: 259/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 86.18% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.0576923076923_epoch_259_20240510125528.pt\n",
      "1h05m Epoch: 260/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 86.06% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.29807692307693_epoch_260_20240510125543.pt\n",
      "1h06m Epoch: 261/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.01923076923077_epoch_261_20240510125558.pt\n",
      "1h06m Epoch: 262/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 87.02% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.17788461538461_epoch_262_20240510125613.pt\n",
      "1h06m Epoch: 263/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.29807692307693_epoch_263_20240510125629.pt\n",
      "1h06m Epoch: 264/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.09  Acc 86.30% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc88.10096153846155_epoch_264_20240510125644.pt\n",
      "1h07m Epoch: 265/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 88.10% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.3798076923077_epoch_265_20240510125659.pt\n",
      "1h07m Epoch: 266/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.38% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h07m Epoch: 267/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.58% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc89.78365384615384_epoch_267_20240510125729.pt\n",
      "1h07m Epoch: 268/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 89.78% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h08m Epoch: 269/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 84.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc87.74038461538461_epoch_269_20240510125759.pt\n",
      "1h08m Epoch: 270/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 87.74% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h08m Epoch: 271/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h08m Epoch: 272/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.70% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.29807692307693_epoch_272_20240510125844.pt\n",
      "1h09m Epoch: 273/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.13942307692307_epoch_273_20240510125900.pt\n",
      "1h09m Epoch: 274/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 87.14% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h09m Epoch: 275/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.34% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.89903846153845_epoch_275_20240510125930.pt\n",
      "1h09m Epoch: 276/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 86.90% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.13942307692307_epoch_276_20240510125946.pt\n",
      "1h10m Epoch: 277/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.14% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h10m Epoch: 278/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.94% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.01923076923077_epoch_278_20240510130016.pt\n",
      "1h10m Epoch: 279/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 87.02% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.25961538461539_epoch_279_20240510130031.pt\n",
      "1h10m Epoch: 280/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.26% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h11m Epoch: 281/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 84.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.84249114990234_tracc86.89903846153845_epoch_281_20240510130102.pt\n",
      "1h11m Epoch: 282/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "1h11m Epoch: 283/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h11m Epoch: 284/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.46% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.86057692307693_epoch_284_20240510130147.pt\n",
      "1h12m Epoch: 285/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.86% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.41826923076923_epoch_285_20240510130203.pt\n",
      "1h12m Epoch: 286/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.42% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h12m Epoch: 287/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h12m Epoch: 288/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.89903846153845_epoch_288_20240510130248.pt\n",
      "1h13m Epoch: 289/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 86.90% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc88.70192307692307_epoch_289_20240510130303.pt\n",
      "1h13m Epoch: 290/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 88.70% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h13m Epoch: 291/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 85.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.10096153846155_epoch_291_20240510130334.pt\n",
      "1h13m Epoch: 292/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 88.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.65865384615384_epoch_292_20240510130349.pt\n",
      "1h14m Epoch: 293/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_293_20240510130404.pt\n",
      "1h14m Epoch: 294/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.74038461538461_epoch_294_20240510130419.pt\n",
      "1h14m Epoch: 295/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.74% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.3798076923077_epoch_295_20240510130434.pt\n",
      "1h14m Epoch: 296/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.11  Acc 87.38% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.89903846153845_epoch_296_20240510130449.pt\n",
      "1h15m Epoch: 297/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 86.90% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h15m Epoch: 298/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 85.70% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.3798076923077_epoch_298_20240510130519.pt\n",
      "1h15m Epoch: 299/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.10  Acc 87.38% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h15m Epoch: 300/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 0.00010000000000000003  Loss 0.12  Acc 85.70% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h16m Epoch: 301/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 83.89% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.65865384615384_epoch_301_20240510130606.pt\n",
      "1h16m Epoch: 302/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 86.66% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h16m Epoch: 303/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h16m Epoch: 304/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_304_20240510130652.pt\n",
      "1h17m Epoch: 305/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.58173076923077_epoch_305_20240510130707.pt\n",
      "1h17m Epoch: 306/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h17m Epoch: 307/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.22% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h17m Epoch: 308/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.34% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h18m Epoch: 309/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.70% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.29807692307693_epoch_309_20240510130809.pt\n",
      "1h18m Epoch: 310/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 86.30% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.89903846153845_epoch_310_20240510130824.pt\n",
      "1h18m Epoch: 311/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 86.90% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.64469146728516_tracc86.89903846153845_epoch_311_20240510130839.pt\n",
      "1h18m Epoch: 312/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.3798076923077_epoch_312_20240510130855.pt\n",
      "1h19m Epoch: 313/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.38% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.29807692307693_epoch_313_20240510130910.pt\n",
      "1h19m Epoch: 314/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.29807692307693_epoch_314_20240510130925.pt\n",
      "1h19m Epoch: 315/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.65865384615384_epoch_315_20240510130941.pt\n",
      "1h20m Epoch: 316/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h20m Epoch: 317/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 84.74% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.89903846153845_epoch_317_20240510131011.pt\n",
      "1h20m Epoch: 318/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.53846153846155_epoch_318_20240510131026.pt\n",
      "1h20m Epoch: 319/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.54% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h21m Epoch: 320/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.74038461538461_epoch_320_20240510131057.pt\n",
      "1h21m Epoch: 321/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.74% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc87.01923076923077_epoch_321_20240510131112.pt\n",
      "1h21m Epoch: 322/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.02% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h21m Epoch: 323/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 85.22% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.89903846153845_epoch_323_20240510131143.pt\n",
      "1h22m Epoch: 324/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.01923076923077_epoch_324_20240510131158.pt\n",
      "1h22m Epoch: 325/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.02% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h22m Epoch: 326/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.94% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.17788461538461_epoch_326_20240510131229.pt\n",
      "1h22m Epoch: 327/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 86.18% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h23m Epoch: 328/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.34% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.25961538461539_epoch_328_20240510131259.pt\n",
      "1h23m Epoch: 329/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.26% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.53846153846155_epoch_329_20240510131314.pt\n",
      "1h23m Epoch: 330/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.53846153846155_epoch_330_20240510131330.pt\n",
      "1h23m Epoch: 331/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 86.54% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h24m Epoch: 332/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.98076923076923_epoch_332_20240510131400.pt\n",
      "1h24m Epoch: 333/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.6201923076923_epoch_333_20240510131416.pt\n",
      "1h24m Epoch: 334/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.25961538461539_epoch_334_20240510131431.pt\n",
      "1h24m Epoch: 335/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.26% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h25m Epoch: 336/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.82% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_336_20240510131502.pt\n",
      "1h25m Epoch: 337/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.17788461538461_epoch_337_20240510131517.pt\n",
      "1h25m Epoch: 338/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.53846153846155_epoch_338_20240510131532.pt\n",
      "1h25m Epoch: 339/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.54% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h26m Epoch: 340/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc89.0625_epoch_340_20240510131603.pt\n",
      "1h26m Epoch: 341/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.6201923076923_epoch_341_20240510131619.pt\n",
      "1h26m Epoch: 342/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h26m Epoch: 343/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 84.74% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h27m Epoch: 344/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.82% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.29807692307693_epoch_344_20240510131704.pt\n",
      "1h27m Epoch: 345/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.0576923076923_epoch_345_20240510131720.pt\n",
      "1h27m Epoch: 346/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h27m Epoch: 347/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 85.22% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.65865384615384_epoch_347_20240510131751.pt\n",
      "1h28m Epoch: 348/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.6201923076923_epoch_348_20240510131806.pt\n",
      "1h28m Epoch: 349/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.53846153846155_epoch_349_20240510131821.pt\n",
      "1h28m Epoch: 350/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 86.54% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.58173076923077_epoch_350_20240510131837.pt\n",
      "1h28m Epoch: 351/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h29m Epoch: 352/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 84.74% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h29m Epoch: 353/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 84.98% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h29m Epoch: 354/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.22% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_354_20240510131938.pt\n",
      "1h29m Epoch: 355/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_355_20240510131954.pt\n",
      "1h30m Epoch: 356/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.5_epoch_356_20240510132009.pt\n",
      "1h30m Epoch: 357/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h30m Epoch: 358/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h30m Epoch: 359/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.22% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.86057692307693_epoch_359_20240510132055.pt\n",
      "1h31m Epoch: 360/500 | Time: 0m15s (Train 0m13s  Val 0m02s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.86% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.89903846153845_epoch_360_20240510132110.pt\n",
      "1h31m Epoch: 361/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.6201923076923_epoch_361_20240510132126.pt\n",
      "1h31m Epoch: 362/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h32m Epoch: 363/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 83.65% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc88.58173076923077_epoch_363_20240510132156.pt\n",
      "1h32m Epoch: 364/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.58% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc89.42307692307693_epoch_364_20240510132212.pt\n",
      "1h32m Epoch: 365/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 89.42% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_365_20240510132227.pt\n",
      "1h32m Epoch: 366/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h33m Epoch: 367/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 84.13% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.22115384615384_epoch_367_20240510132257.pt\n",
      "1h33m Epoch: 368/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 88.22% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.89903846153845_epoch_368_20240510132312.pt\n",
      "1h33m Epoch: 369/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.12  Acc 86.90% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h33m Epoch: 370/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 85.82% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_370_20240510132343.pt\n",
      "1h34m Epoch: 371/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.98076923076923_epoch_371_20240510132358.pt\n",
      "1h34m Epoch: 372/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 87.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_372_20240510132414.pt\n",
      "1h34m Epoch: 373/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h34m Epoch: 374/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 85.82% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.29807692307693_epoch_374_20240510132444.pt\n",
      "1h35m Epoch: 375/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 86.30% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.53846153846155_epoch_375_20240510132459.pt\n",
      "1h35m Epoch: 376/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.54% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_376_20240510132515.pt\n",
      "1h35m Epoch: 377/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.74038461538461_epoch_377_20240510132530.pt\n",
      "1h35m Epoch: 378/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.74% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.17788461538461_epoch_378_20240510132545.pt\n",
      "1h36m Epoch: 379/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.18% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_379_20240510132601.pt\n",
      "1h36m Epoch: 380/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.25961538461539_epoch_380_20240510132616.pt\n",
      "1h36m Epoch: 381/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.26% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.25961538461539_epoch_381_20240510132631.pt\n",
      "1h36m Epoch: 382/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.26% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h37m Epoch: 383/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 84.74% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.29807692307693_epoch_383_20240510132702.pt\n",
      "1h37m Epoch: 384/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.30% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.5_epoch_384_20240510132717.pt\n",
      "1h37m Epoch: 385/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_385_20240510132733.pt\n",
      "1h37m Epoch: 386/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc88.10096153846155_epoch_386_20240510132748.pt\n",
      "1h38m Epoch: 387/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.09  Acc 88.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.64469146728516_tracc86.0576923076923_epoch_387_20240510132803.pt\n",
      "1h38m Epoch: 388/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.06% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "1h38m Epoch: 389/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 85.58% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h38m Epoch: 390/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 83.89% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.6201923076923_epoch_390_20240510132849.pt\n",
      "1h39m Epoch: 391/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.01923076923077_epoch_391_20240510132905.pt\n",
      "1h39m Epoch: 392/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.02% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.74038461538461_epoch_392_20240510132920.pt\n",
      "1h39m Epoch: 393/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.74% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.27838897705078_tracc86.41826923076923_epoch_393_20240510132935.pt\n",
      "1h39m Epoch: 394/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 86.42% | Val: Loss nan  Acc(top1) 88.28% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.65865384615384_epoch_394_20240510132950.pt\n",
      "1h40m Epoch: 395/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.5_epoch_395_20240510133006.pt\n",
      "1h40m Epoch: 396/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h40m Epoch: 397/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 84.38% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.29807692307693_epoch_397_20240510133036.pt\n",
      "1h40m Epoch: 398/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc88.64469146728516_tracc87.13942307692307_epoch_398_20240510133051.pt\n",
      "1h41m Epoch: 399/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.14% | Val: Loss nan  Acc(top1) 88.64% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.65865384615384_epoch_399_20240510133107.pt\n",
      "1h41m Epoch: 400/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.89903846153845_epoch_400_20240510133122.pt\n",
      "1h41m Epoch: 401/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 86.90% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.6201923076923_epoch_401_20240510133137.pt\n",
      "1h41m Epoch: 402/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_402_20240510133152.pt\n",
      "1h42m Epoch: 403/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.09  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.77884615384616_epoch_403_20240510133208.pt\n",
      "1h42m Epoch: 404/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.78% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc88.10096153846155_epoch_404_20240510133223.pt\n",
      "1h42m Epoch: 405/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 88.10% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h42m Epoch: 406/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 84.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc89.0625_epoch_406_20240510133253.pt\n",
      "1h43m Epoch: 407/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_407_20240510133309.pt\n",
      "1h43m Epoch: 408/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.53846153846155_epoch_408_20240510133324.pt\n",
      "1h43m Epoch: 409/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.54% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.5_epoch_409_20240510133339.pt\n",
      "1h43m Epoch: 410/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_410_20240510133354.pt\n",
      "1h44m Epoch: 411/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc89.0625_epoch_411_20240510133409.pt\n",
      "1h44m Epoch: 412/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.3798076923077_epoch_412_20240510133425.pt\n",
      "1h44m Epoch: 413/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.09  Acc 87.38% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h44m Epoch: 414/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 84.62% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h45m Epoch: 415/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 85.82% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h45m Epoch: 416/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 83.53% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h45m Epoch: 417/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.13942307692307_epoch_417_20240510133541.pt\n",
      "1h46m Epoch: 418/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.14% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.25961538461539_epoch_418_20240510133556.pt\n",
      "1h46m Epoch: 419/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.26% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_419_20240510133612.pt\n",
      "1h46m Epoch: 420/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h46m Epoch: 421/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.70% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h47m Epoch: 422/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.22% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.89903846153845_epoch_422_20240510133657.pt\n",
      "1h47m Epoch: 423/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.74038461538461_epoch_423_20240510133713.pt\n",
      "1h47m Epoch: 424/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.74% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h47m Epoch: 425/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h48m Epoch: 426/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 85.46% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h48m Epoch: 427/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.65865384615384_epoch_427_20240510133814.pt\n",
      "1h48m Epoch: 428/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.66% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.5_epoch_428_20240510133829.pt\n",
      "1h48m Epoch: 429/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.50% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.89903846153845_epoch_429_20240510133844.pt\n",
      "1h49m Epoch: 430/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.90% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h49m Epoch: 431/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 84.01% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc88.34134615384616_epoch_431_20240510133915.pt\n",
      "1h49m Epoch: 432/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 88.34% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.41826923076923_epoch_432_20240510133930.pt\n",
      "1h49m Epoch: 433/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.42% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.65865384615384_epoch_433_20240510133945.pt\n",
      "1h50m Epoch: 434/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.17788461538461_epoch_434_20240510134000.pt\n",
      "1h50m Epoch: 435/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_435_20240510134016.pt\n",
      "1h50m Epoch: 436/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h50m Epoch: 437/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 83.29% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.41826923076923_epoch_437_20240510134046.pt\n",
      "1h51m Epoch: 438/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.42% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.01923076923077_epoch_438_20240510134101.pt\n",
      "1h51m Epoch: 439/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.02% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.3798076923077_epoch_439_20240510134116.pt\n",
      "1h51m Epoch: 440/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.38% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc88.22115384615384_epoch_440_20240510134132.pt\n",
      "1h51m Epoch: 441/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 88.22% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.25961538461539_epoch_441_20240510134147.pt\n",
      "1h52m Epoch: 442/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.09  Acc 87.26% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.65865384615384_epoch_442_20240510134202.pt\n",
      "1h52m Epoch: 443/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.66% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.41826923076923_epoch_443_20240510134217.pt\n",
      "1h52m Epoch: 444/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.42% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.77884615384616_epoch_444_20240510134232.pt\n",
      "1h52m Epoch: 445/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.010986328125_tracc86.65865384615384_epoch_445_20240510134248.pt\n",
      "1h53m Epoch: 446/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.66% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.17788461538461_epoch_446_20240510134303.pt\n",
      "1h53m Epoch: 447/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "1h53m Epoch: 448/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 84.98% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.98076923076923_epoch_448_20240510134333.pt\n",
      "1h53m Epoch: 449/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.98% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h54m Epoch: 450/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 84.13% | Val: Loss nan  Acc(top1) 90.84% | HA 93.04@3\n",
      "1h54m Epoch: 451/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 89.01% | HA 93.04@3\n",
      "1h54m Epoch: 452/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.89903846153845_epoch_452_20240510134434.pt\n",
      "1h54m Epoch: 453/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.90% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.25961538461539_epoch_453_20240510134449.pt\n",
      "1h55m Epoch: 454/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.26% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.65865384615384_epoch_454_20240510134504.pt\n",
      "1h55m Epoch: 455/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.66% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.29807692307693_epoch_455_20240510134519.pt\n",
      "1h55m Epoch: 456/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.30% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h55m Epoch: 457/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.10096153846155_epoch_457_20240510134550.pt\n",
      "1h56m Epoch: 458/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 88.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_458_20240510134605.pt\n",
      "1h56m Epoch: 459/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h56m Epoch: 460/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 84.13% | Val: Loss nan  Acc(top1) 86.45% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.89903846153845_epoch_460_20240510134635.pt\n",
      "1h56m Epoch: 461/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 86.90% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h57m Epoch: 462/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.34% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc86.29807692307693_epoch_462_20240510134704.pt\n",
      "1h57m Epoch: 463/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.30% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "1h57m Epoch: 464/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.5_epoch_464_20240510134734.pt\n",
      "1h57m Epoch: 465/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.41826923076923_epoch_465_20240510134749.pt\n",
      "1h58m Epoch: 466/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.42% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.13942307692307_epoch_466_20240510134804.pt\n",
      "1h58m Epoch: 467/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.14% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.3798076923077_epoch_467_20240510134819.pt\n",
      "1h58m Epoch: 468/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.38% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc87.74038461538461_epoch_468_20240510134834.pt\n",
      "1h58m Epoch: 469/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.74% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "1h59m Epoch: 470/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 85.46% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.5_epoch_470_20240510134904.pt\n",
      "1h59m Epoch: 471/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "1h59m Epoch: 472/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 82.81% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.9423076923077_epoch_472_20240510134934.pt\n",
      "1h59m Epoch: 473/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 88.94% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "2h00m Epoch: 474/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 87.91% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.17788461538461_epoch_474_20240510135004.pt\n",
      "2h00m Epoch: 475/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.18% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.25961538461539_epoch_475_20240510135019.pt\n",
      "2h00m Epoch: 476/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 87.26% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.58173076923077_epoch_476_20240510135034.pt\n",
      "2h00m Epoch: 477/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 88.58% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "2h01m Epoch: 478/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 84.38% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.25961538461539_epoch_478_20240510135104.pt\n",
      "2h01m Epoch: 479/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.26% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc88.10096153846155_epoch_479_20240510135119.pt\n",
      "2h01m Epoch: 480/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 88.10% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc87.25961538461539_epoch_480_20240510135134.pt\n",
      "2h01m Epoch: 481/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.26% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.98076923076923_epoch_481_20240510135149.pt\n",
      "2h02m Epoch: 482/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.09  Acc 87.98% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_482_20240510135204.pt\n",
      "2h02m Epoch: 483/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.86057692307693_epoch_483_20240510135219.pt\n",
      "2h02m Epoch: 484/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.86% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.65865384615384_epoch_484_20240510135234.pt\n",
      "2h02m Epoch: 485/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.66% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_485_20240510135249.pt\n",
      "2h03m Epoch: 486/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.41826923076923_epoch_486_20240510135304.pt\n",
      "2h03m Epoch: 487/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.42% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.47618865966797_tracc87.74038461538461_epoch_487_20240510135319.pt\n",
      "2h03m Epoch: 488/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.74% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "2h03m Epoch: 489/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 85.22% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "2h04m Epoch: 490/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 85.70% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.77884615384616_epoch_490_20240510135403.pt\n",
      "2h04m Epoch: 491/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.78% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "2h04m Epoch: 492/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 87.55% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.13942307692307_epoch_492_20240510135433.pt\n",
      "2h04m Epoch: 493/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 87.14% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.41826923076923_epoch_493_20240510135448.pt\n",
      "2h05m Epoch: 494/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.42% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "2h05m Epoch: 495/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 84.74% | Val: Loss nan  Acc(top1) 90.48% | HA 93.04@3\n",
      "2h05m Epoch: 496/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.12  Acc 85.22% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc87.6201923076923_epoch_496_20240510135533.pt\n",
      "2h05m Epoch: 497/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 87.62% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.74359130859375_tracc86.77884615384616_epoch_497_20240510135548.pt\n",
      "2h06m Epoch: 498/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.78% | Val: Loss nan  Acc(top1) 89.74% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc90.10989379882812_tracc86.0576923076923_epoch_498_20240510135603.pt\n",
      "2h06m Epoch: 499/500 | Time: 0m15s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.10  Acc 86.06% | Val: Loss nan  Acc(top1) 90.11% | HA 93.04@3\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/pruning_time_2024051011_prunratio80.0/sp_ai_model_first_stage_prun_haacc_93.04029083251953_valacc89.37728881835938_tracc86.17788461538461_epoch_499_20240510135618.pt\n",
      "2h06m Epoch: 500/500 | Time: 0m14s (Train 0m13s  Val 0m01s) | Train: LR 1.0000000000000004e-06  Loss 0.11  Acc 86.18% | Val: Loss nan  Acc(top1) 89.38% | HA 93.04@3\n",
      "Execution finished in: 2h06m\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae31fa-cf50-449c-88b3-fdca70c66228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee28e6-b711-4642-8ac9-7e506be8ad4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5acd0-ef85-4301-aedb-5cc561681ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
