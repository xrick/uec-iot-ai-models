{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b923ec-5857-4d0b-9909-1e0236ba4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import torch;\n",
    "import numpy as np;\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;\n",
    "from operator import itemgetter;\n",
    "from heapq import nsmallest;\n",
    "import time;\n",
    "import glob;\n",
    "import math;\n",
    "import random;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a88dd74-1ca3-4b31-b50b-84b82739d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../src/\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f540e4f9-0b63-4f73-bca1-51557fb87033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.opts as opt;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "# import th.resources.train_generator as train_generator;\n",
    "from th.resources.pruning_tools import filter_pruning, filter_pruner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7215cbb7-d9ce-4b64-9f81-8abc3fada11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import common.tlopts as tlopts\n",
    "from SharedLibs.datestring import genDataTimeStr, getDateStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74b3364-cd68-464d-9426-3a4f85ad7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducibility\n",
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69211f9d-1d78-4eae-8540-2d1b04cd13c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d34d33d-7683-4dd8-ab36-9b9b9d95be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([('52',1),('56',2),('99',3)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess_setup_withoutt_normalize(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength)\n",
    "                 ]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b648ed6-051a-49d7-ad2a-b051c05e88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    dataset = np.load(opt.trainSet, allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    trainGen.preprocess_setup();\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6266cad-9de1-47c0-87dc-c8ade965a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/forOneClassModel_alarm/train_test_npz/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086db016-0acf-4029-9634-e79d30842ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customed_ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(Customed_ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs =  n_class #self.ch_config[-1];\n",
    "        ch_confing_10 = 512 #8 * 64\n",
    "        ch_n_class = n_class\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(ch_confing_10, ch_n_class, (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, ch_n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (2,4)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetCustomedACDNetModel(input_len=30225, nclass=3, sr=20000, channel_config=None):\n",
    "    net = Customed_ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cae6f7-e8e2-438f-a003-f4b111ccae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        self.opt.channels_to_prune_per_iteration = 1;\n",
    "        self.opt.finetune_epoch_per_iteration = 2;\n",
    "        self.opt.lr=0.001;\n",
    "        self.opt.schedule = [0.5, 0.8];\n",
    "        self.opt.prune_type = 2 #determine the prunning algo, 1: Magnitude Pruning ;2: tylor-pruning\n",
    "        # torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"); #in office use \n",
    "        self.opt.device = 'cuda:0'#at home use apple m2\n",
    "        self.pruner = None;\n",
    "        self.iterations = 0;\n",
    "        self.cur_acc = 0.0;\n",
    "        self.cur_iter = 1;\n",
    "        self.cur_lr = self.opt.lr;\n",
    "        self.net = None;\n",
    "        self.criterion = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "        self.trainGen = getTrainGen(opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.load_test_data();\n",
    "\n",
    "    def PruneAndTrain(self):\n",
    "        dir = os.getcwd();\n",
    "        self.net = GetCustomedACDNetModel();\n",
    "        trained_model = \"../../../trained_models/step_2_first_stage_pruning/pruning_time_2024050214_prunratio80.0/current_best/sp_ai_model_first_stage_prun_acc95.34_trAcc_90_epoch_562_20240502161851.pt\"\n",
    "        self.net.load_state_dict(torch.load(trained_model, map_location=\"cuda:0\")['weight']);\n",
    "        self.net = self.net.to('cuda:0');#at home use apple m2\n",
    "        self.pruner = filter_pruning.Magnitude(self.net, self.opt) if self.opt.prune_type == 1 else filter_pruning.Taylor(self.net, self.opt);\n",
    "        print(f\"pruning algorithm is {self.pruner}\");\n",
    "        self.validate();\n",
    "        calc.summary(self.net, (1, 1, self.opt.inputLength), brief=False); # shape of one sample for inferenceing\n",
    "        # exit();\n",
    "        #Make sure all the layers are trainable\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.iterations = self.estimate_pruning_iterations();\n",
    "        # exit();\n",
    "        for i in range(1, self.iterations):\n",
    "            self.cur_iter = i;\n",
    "            iter_start = time.time();\n",
    "            print(\"\\nIteration {} of {} starts..\".format(i, self.iterations-1), flush=True);\n",
    "            print(\"Ranking channels.. \", flush=True);\n",
    "            prune_targets = self.get_candidates_to_prune(self.opt.channels_to_prune_per_iteration);\n",
    "            # prune_targets = [(40,3)];\n",
    "            print(\"Pruning channels: {}\".format(prune_targets), flush=True);\n",
    "            self.net = filter_pruner.prune_layers(self.net, prune_targets, self.opt.prune_all, self.opt.device);\n",
    "            calc.summary(self.net, (1, 1, self.opt.inputLength), brief=True); # shape of one sample for inferenceing\n",
    "            self.validate();\n",
    "            print(\"Fine tuning {} epochs to recover from prunning iteration.\".format(self.opt.finetune_epoch_per_iteration), flush=True);\n",
    "\n",
    "            if self.cur_iter in list(map(int, np.array(self.iterations)*self.opt.schedule)):\n",
    "                self.cur_lr *= 0.1;\n",
    "            optimizer = optim.SGD(self.net.parameters(), lr=self.cur_lr, momentum=0.9);\n",
    "            self.train(optimizer, epoches = self.opt.finetune_epoch_per_iteration);\n",
    "            print(\"Iteration {}/{} finished in {}\".format(self.cur_iter, self.iterations+1, U.to_hms(time.time()-iter_start)), flush=True);\n",
    "            print(\"Total channels prunned so far: {}\".format(i*self.opt.channels_to_prune_per_iteration), flush=True);\n",
    "            self.__save_model(self.net);\n",
    "\n",
    "        calc.summary(self.net, (1, 1, self.opt.inputLength)); # shape of one sample for inferenceing\n",
    "        self.__save_model(self.net);\n",
    "\n",
    "    def get_candidates_to_prune(self, num_filters_to_prune):\n",
    "        self.pruner.reset();\n",
    "        if self.opt.prune_type == 1:\n",
    "            self.pruner.compute_filter_magnitude();\n",
    "        else:\n",
    "            self.train_epoch(rank_filters = True);\n",
    "            self.pruner.normalize_ranks_per_layer();\n",
    "\n",
    "        return self.pruner.get_prunning_plan(num_filters_to_prune);\n",
    "\n",
    "    def estimate_pruning_iterations(self):\n",
    "        # get total number of variables from all conv2d featuremaps\n",
    "        prunable_count = sum(self.get_channel_list(self.opt.prune_all));\n",
    "        total_count= sum(self.get_channel_list());\n",
    "        #iterations_reqired = int((prunable_count * self.opt.prune_ratio) / self.opt.channels_to_prune_per_iteration);\n",
    "        #prune_ratio works with the total number of channels, not only with the prunable channels. i.e. 80% or total will be pruned from total or from only features\n",
    "        iterations_reqired = int((total_count * self.opt.prune_ratio) / self.opt.channels_to_prune_per_iteration);\n",
    "        print('Total Channels: {}, Prunable: {}, Non-Prunable: {}'.format(total_count, prunable_count, total_count - prunable_count), flush=True);\n",
    "        print('No. of Channels to prune per iteration: {}'.format(self.opt.channels_to_prune_per_iteration), flush=True);\n",
    "        print('Total Channels to prune ({}%): {}'.format(int(self.opt.prune_ratio*100), int(total_count * self.opt.prune_ratio)-1), flush=True);\n",
    "        print('Total iterations required: {}'.format(iterations_reqired-1), flush=True);\n",
    "        return iterations_reqired;\n",
    "\n",
    "    def get_channel_list(self, prune_all=True):\n",
    "        ch_conf = [];\n",
    "        if prune_all:\n",
    "            for name, module in enumerate(self.net.sfeb):\n",
    "                if issubclass(type(module), torch.nn.Conv2d):\n",
    "                    ch_conf.append(module.out_channels);\n",
    "\n",
    "        for name, module in enumerate(self.net.tfeb):\n",
    "            if issubclass(type(module), torch.nn.Conv2d):\n",
    "                ch_conf.append(module.out_channels);\n",
    "\n",
    "        return ch_conf;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load(self.opt.valSet, allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            # self.testX = torch.tensor(dataX).cuda();\n",
    "            # self.testY = torch.tensor(data['y']).cuda();\n",
    "            self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "            # self.testY = torch.tensor(data['y']).to(self.opt.device);#in office, use cuda(better) or cpu\n",
    "            self.testY = torch.FloatTensor(data['y']).to(self.opt.device);#at home use apple m2\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find the indices that has highest average value for each sample\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            # if self.opt.device == \"mps\":\n",
    "            #     y_target = y_target.cpu() #use apple m2, in office use cuda\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = self.criterion(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def train(self, optimizer = None, epoches=10):\n",
    "        for i in range(epoches):\n",
    "            # print(\"Epoch: \", i);\n",
    "            self.train_epoch(optimizer);\n",
    "            self.validate();\n",
    "        print(\"Finished fine tuning.\", flush=True);\n",
    "\n",
    "    def train_batch(self, optimizer, batch, label, rank_filters):\n",
    "        self.net.zero_grad()\n",
    "        if rank_filters:\n",
    "            output = self.pruner.forward(batch);\n",
    "            if self.opt.device == \"mps\":\n",
    "                label = label.cpu() #use apple m2, in office use cuda\n",
    "                output = output.cpu() #use apple m2, in office use cuda\n",
    "            self.criterion(output.log(), label).backward();\n",
    "        else:\n",
    "            self.criterion(self.net(batch), label).backward();\n",
    "            optimizer.step();\n",
    "\n",
    "    def train_epoch(self, optimizer = None, rank_filters = False):\n",
    "        if rank_filters is False and optimizer is None:\n",
    "            print('Please provide optimizer to train_epoch', flush=True);\n",
    "            exit();\n",
    "        n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "        for b_idx in range(n_batches):\n",
    "            x,y = self.trainGen.__getitem__(b_idx)\n",
    "            x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "            y = torch.tensor(y).to(self.opt.device);\n",
    "            self.train_batch(optimizer, x, y, rank_filters);\n",
    "\n",
    "    def validate(self):\n",
    "        self.net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                # if self.opt.device == \"mps\":\n",
    "                #     x = torch.tensor(x)\n",
    "                #     x = x.type(torch.FloatTensor) # use apple mp2\n",
    "                scores = self.net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "            acc, loss = self.compute_accuracy(y_pred, self.testY);\n",
    "        print('Current Testing Performance - Val: Loss {:.3f}  Acc(top1) {:.3f}%'.format(loss, acc), flush=True);\n",
    "        self.cur_acc = acc;\n",
    "        self.net.train();\n",
    "        return acc, loss;\n",
    "\n",
    "    def __save_model(self, net):\n",
    "        net.ch_config = self.get_channel_list();\n",
    "        dir = os.getcwd();\n",
    "        fname = self.opt.model_name;\n",
    "        if os.path.isfile(fname):\n",
    "            os.remove(fname);\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, fname);\n",
    "        \n",
    "    # def __save_model(self, acc, train_acc, epochIdx, net):\n",
    "    #     if acc > self.bestAcc:\n",
    "    #         self.bestAcc = acc;\n",
    "    #         self.bestAccEpoch = epochIdx +1;\n",
    "    #         __do_save_model(self, acc, train_acc, self.bestAccEpoch, net);\n",
    "    #     else:\n",
    "    #         if acc > 94.0 or train_acc > 85.0: \n",
    "    #             __do_save_model(self, acc, train_acc, epochIdx, net);\n",
    "    #         else:\n",
    "    #             pass\n",
    "\n",
    "    # def __do_save_model(self, acc, tr_acc, bestAccIdx, net):\n",
    "    #     save_model_name = self.opt.model_name.format(self.bestAcc, acc, train_acc, epochIdx, genDataTimeStr());\n",
    "    #     save_model_fullpath = self.opt.save_dir + save_model_name;\n",
    "    #     print(f\"save model to {save_model_fullpath}\")\n",
    "    #     torch.save({'weight':net.state_dict(), 'config':net.ch_config}, save_model_fullpath);\n",
    "    #     logObj.write(f\"save model:{model_name}, bestAcc:{self.bestAcc}@{self.}, currentAcc:{acc}@{epochIdx}\");\n",
    "    #     logObj.write(\"\\n\");\n",
    "    #     logObj.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d935fefb-1073-4894-aae9-9317b88dfd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save and record the training hyperparameters and results\\npruning algo: tylor-pruning\\npruning ration : 0.85\\nfinal accuracy : \\nepoch: \\nself.opt.LR = 0.01;\\nopt.momentum = 0.009;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'tylor-pruning';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"save and record the training hyperparameters and results\n",
    "pruning algo: tylor-pruning\n",
    "pruning ration : 0.85\n",
    "final accuracy : \n",
    "epoch: \n",
    "self.opt.LR = 0.01;\n",
    "opt.momentum = 0.009;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'tylor-pruning';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3888935-7f32-4d8e-bdc3-48761aa4e13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b05ced26-21b7-4292-b531-276490ea99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts()\n",
    "    #Learning settings\n",
    "    opt.batchSize = 32;\n",
    "    # opt.LR = 0.01;\n",
    "    # opt.momentum = 0.09;\n",
    "    # opt.weightDecay = 5e-3;\n",
    "    # opt.nEpochs = 1000;#2000;\n",
    "    # opt.schedule = [0.03, 0.06, 0.09]\n",
    "    # opt.warmup = 10;\n",
    "    #set train and validation sets\n",
    "    opt.trainSet = \"../../../datasets/CurrentUse/generated_datasets/train/version4/single_fold_train_20240502114607.npz\"\n",
    "    opt.valSet = \"../../../datasets/CurrentUse/generated_datasets/val/version4/final_single_val_20240502120516.npz\"\n",
    "    #Basic Net Settings\n",
    "    opt.prune_ratio = 0.85\n",
    "    opt.prune_all = True;\n",
    "    opt.nClasses = 3\n",
    "    opt.nFolds = 1;\n",
    "    opt.split = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    opt.trainer = None\n",
    "    \n",
    "    # import torch;\n",
    "    # opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"); #in office use cuda or cpu\n",
    "    opt.device = 'cuda:0' #at home use apple m2\n",
    "    # tlopts.display_info(opt)\n",
    "    save_dir = \"../../../trained_models/step_4_second_stage_pruning/valacc_95.34_tracc90_pruning_time_{}_prunratio{}/\".format(getDateStr(),opt.prune_ratio*100)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    model_name = \"model_second_stage_prun_ratio0.85_{}.pt\".format(genDataTimeStr());\n",
    "    opt.model_name = save_dir + model_name;\n",
    "    # valid_path = False;\n",
    "    print(\"Initializing PruneAndTrain Object.....\")\n",
    "    trainer = PruningTrainer(opt=opt)#TLTrainer(opt)\n",
    "    print(\"Start to pruning.....\")\n",
    "    trainer.PruneAndTrain();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c17937-4b7a-49f2-bb4c-cf323d891beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PruneAndTrain Object.....\n",
      "length of samples:651\n",
      "Start to pruning.....\n",
      "pruning algorithm is <th.resources.pruning_tools.filter_pruning.Taylor object at 0x7f7d24c1f430>\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.339%\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (8, 1, 15109)         72    1,087,848\n",
      "  BatchNorm2d-2     (8, 1, 15109)     (8, 1, 15109)         16            0\n",
      "         ReLu-3     (8, 1, 15109)     (8, 1, 15109)          0      120,872\n",
      "       Conv2d-4     (8, 1, 15109)     (64, 1, 7553)      2,560   19,335,680\n",
      "  BatchNorm2d-5     (64, 1, 7553)     (64, 1, 7553)        128            0\n",
      "         ReLu-6     (64, 1, 7553)     (64, 1, 7553)          0      483,392\n",
      "    MaxPool2d-7     (64, 1, 7553)      (64, 1, 151)          0      483,200\n",
      "      Permute-8      (64, 1, 151)      (1, 64, 151)          0            0\n",
      "       Conv2d-9      (1, 64, 151)     (32, 64, 151)        288    2,783,232\n",
      " BatchNorm2d-10     (32, 64, 151)     (32, 64, 151)         64            0\n",
      "        ReLu-11     (32, 64, 151)     (32, 64, 151)          0      309,248\n",
      "   MaxPool2d-12     (32, 64, 151)      (32, 32, 75)          0      307,200\n",
      "      Conv2d-13      (32, 32, 75)      (64, 32, 75)     18,432   44,236,800\n",
      " BatchNorm2d-14      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-15      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "      Conv2d-16      (64, 32, 75)      (64, 32, 75)     36,864   88,473,600\n",
      " BatchNorm2d-17      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-18      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "   MaxPool2d-19      (64, 32, 75)      (64, 16, 37)          0      151,552\n",
      "      Conv2d-20      (64, 16, 37)     (128, 16, 37)     73,728   43,646,976\n",
      " BatchNorm2d-21     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-22     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "      Conv2d-23     (128, 16, 37)     (128, 16, 37)    147,456   87,293,952\n",
      " BatchNorm2d-24     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-25     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "   MaxPool2d-26     (128, 16, 37)      (128, 8, 18)          0       73,728\n",
      "      Conv2d-27      (128, 8, 18)      (256, 8, 18)    294,912   42,467,328\n",
      " BatchNorm2d-28      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-29      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "      Conv2d-30      (256, 8, 18)      (256, 8, 18)    589,824   84,934,656\n",
      " BatchNorm2d-31      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-32      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "   MaxPool2d-33      (256, 8, 18)       (256, 4, 9)          0       36,864\n",
      "      Conv2d-34       (256, 4, 9)       (512, 4, 9)  1,179,648   42,467,328\n",
      " BatchNorm2d-35       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-36       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "      Conv2d-37       (512, 4, 9)       (512, 4, 9)  2,359,296   84,934,656\n",
      " BatchNorm2d-38       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-39       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "   MaxPool2d-40       (512, 4, 9)       (512, 2, 4)          0       16,384\n",
      "      Conv2d-41       (512, 2, 4)         (3, 2, 4)      1,536       12,288\n",
      " BatchNorm2d-42         (3, 2, 4)         (3, 2, 4)          6            0\n",
      "        ReLu-43         (3, 2, 4)         (3, 2, 4)          0           24\n",
      "   AvgPool2d-44         (3, 2, 4)         (3, 1, 1)          0           24\n",
      "     Flatten-45         (3, 1, 1)            (1, 3)          0            0\n",
      "      Linear-46            (1, 3)            (1, 3)         12           12\n",
      "     Softmax-47            (1, 3)            (1, 3)          0            3\n",
      "==============================================================================\n",
      "Total Params: 4,708,682\n",
      "Total FLOPs : 544,226,191\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 17.96\n",
      "Total size (MB) : 18.08\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Total Channels: 2027, Prunable: 2027, Non-Prunable: 0\n",
      "No. of Channels to prune per iteration: 1\n",
      "Total Channels to prune (85%): 1721\n",
      "Total iterations required: 1721\n",
      "\n",
      "Iteration 1 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 6)]\n",
      "Input: 0.115 MB, Params: 4,708,640 (17.962 MB), Total: 18.08 MB, FLOPs: 483,664,528\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 1/1723 finished in 0m16s\n",
      "Total channels prunned so far: 1\n",
      "\n",
      "Iteration 2 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 8)]\n",
      "Input: 0.115 MB, Params: 4,708,598 (17.962 MB), Total: 18.08 MB, FLOPs: 483,298,985\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.441%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 2/1723 finished in 0m15s\n",
      "Total channels prunned so far: 2\n",
      "\n",
      "Iteration 3 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 13)]\n",
      "Input: 0.115 MB, Params: 4,708,556 (17.962 MB), Total: 18.08 MB, FLOPs: 478,767,042\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 3/1723 finished in 0m15s\n",
      "Total channels prunned so far: 3\n",
      "\n",
      "Iteration 4 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 16)]\n",
      "Input: 0.115 MB, Params: 4,708,514 (17.962 MB), Total: 18.08 MB, FLOPs: 478,401,499\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 4/1723 finished in 0m15s\n",
      "Total channels prunned so far: 4\n",
      "\n",
      "Iteration 5 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 16)]\n",
      "Input: 0.115 MB, Params: 4,708,472 (17.961 MB), Total: 18.08 MB, FLOPs: 465,666,804\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 5/1723 finished in 0m15s\n",
      "Total channels prunned so far: 5\n",
      "\n",
      "Iteration 6 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 21)]\n",
      "Input: 0.115 MB, Params: 4,708,430 (17.961 MB), Total: 18.08 MB, FLOPs: 465,301,261\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 6/1723 finished in 0m16s\n",
      "Total channels prunned so far: 6\n",
      "\n",
      "Iteration 7 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 28)]\n",
      "Input: 0.115 MB, Params: 4,708,388 (17.961 MB), Total: 18.08 MB, FLOPs: 460,769,318\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 7/1723 finished in 0m15s\n",
      "Total channels prunned so far: 7\n",
      "\n",
      "Iteration 8 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 28)]\n",
      "Input: 0.115 MB, Params: 4,708,346 (17.961 MB), Total: 18.08 MB, FLOPs: 460,403,775\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 8/1723 finished in 0m15s\n",
      "Total channels prunned so far: 8\n",
      "\n",
      "Iteration 9 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 29)]\n",
      "Input: 0.115 MB, Params: 4,708,304 (17.961 MB), Total: 18.08 MB, FLOPs: 431,725,400\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 9/1723 finished in 0m15s\n",
      "Total channels prunned so far: 9\n",
      "\n",
      "Iteration 10 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 40)]\n",
      "Input: 0.115 MB, Params: 4,708,262 (17.961 MB), Total: 18.08 MB, FLOPs: 431,359,857\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 10/1723 finished in 0m15s\n",
      "Total channels prunned so far: 10\n",
      "\n",
      "Iteration 11 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 46)]\n",
      "Input: 0.115 MB, Params: 4,708,220 (17.960 MB), Total: 18.08 MB, FLOPs: 426,827,914\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 11/1723 finished in 0m15s\n",
      "Total channels prunned so far: 11\n",
      "\n",
      "Iteration 12 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(7, 26)]\n",
      "Input: 0.115 MB, Params: 4,707,633 (17.958 MB), Total: 18.07 MB, FLOPs: 425,616,884\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 12/1723 finished in 0m15s\n",
      "Total channels prunned so far: 12\n",
      "\n",
      "Iteration 13 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 95)]\n",
      "Input: 0.115 MB, Params: 4,704,175 (17.945 MB), Total: 18.06 MB, FLOPs: 424,813,027\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 13/1723 finished in 0m15s\n",
      "Total channels prunned so far: 13\n",
      "\n",
      "Iteration 14 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 34)]\n",
      "Input: 0.115 MB, Params: 4,697,261 (17.919 MB), Total: 18.03 MB, FLOPs: 424,439,563\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 14/1723 finished in 0m15s\n",
      "Total channels prunned so far: 14\n",
      "\n",
      "Iteration 15 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 361)]\n",
      "Input: 0.115 MB, Params: 4,690,356 (17.892 MB), Total: 18.01 MB, FLOPs: 424,253,155\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 15/1723 finished in 0m15s\n",
      "Total channels prunned so far: 15\n",
      "\n",
      "Iteration 16 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 41)]\n",
      "Input: 0.115 MB, Params: 4,685,752 (17.875 MB), Total: 17.99 MB, FLOPs: 424,128,927\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 16/1723 finished in 0m15s\n",
      "Total channels prunned so far: 16\n",
      "\n",
      "Iteration 17 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 81)]\n",
      "Input: 0.115 MB, Params: 4,681,148 (17.857 MB), Total: 17.97 MB, FLOPs: 424,004,699\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 17/1723 finished in 0m15s\n",
      "Total channels prunned so far: 17\n",
      "\n",
      "Iteration 18 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 116)]\n",
      "Input: 0.115 MB, Params: 4,676,544 (17.840 MB), Total: 17.95 MB, FLOPs: 423,880,471\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 18/1723 finished in 0m15s\n",
      "Total channels prunned so far: 18\n",
      "\n",
      "Iteration 19 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 143)]\n",
      "Input: 0.115 MB, Params: 4,671,940 (17.822 MB), Total: 17.94 MB, FLOPs: 423,756,243\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 19/1723 finished in 0m15s\n",
      "Total channels prunned so far: 19\n",
      "\n",
      "Iteration 20 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 345)]\n",
      "Input: 0.115 MB, Params: 4,667,336 (17.804 MB), Total: 17.92 MB, FLOPs: 423,632,015\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 20/1723 finished in 0m15s\n",
      "Total channels prunned so far: 20\n",
      "\n",
      "Iteration 21 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 343)]\n",
      "Input: 0.115 MB, Params: 4,662,732 (17.787 MB), Total: 17.90 MB, FLOPs: 423,507,787\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 21/1723 finished in 0m15s\n",
      "Total channels prunned so far: 21\n",
      "\n",
      "Iteration 22 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 273)]\n",
      "Input: 0.115 MB, Params: 4,658,128 (17.769 MB), Total: 17.88 MB, FLOPs: 423,383,559\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 22/1723 finished in 0m15s\n",
      "Total channels prunned so far: 22\n",
      "\n",
      "Iteration 23 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 321)]\n",
      "Input: 0.115 MB, Params: 4,653,524 (17.752 MB), Total: 17.87 MB, FLOPs: 423,259,331\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 23/1723 finished in 0m15s\n",
      "Total channels prunned so far: 23\n",
      "\n",
      "Iteration 24 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 2)]\n",
      "Input: 0.115 MB, Params: 4,646,691 (17.726 MB), Total: 17.84 MB, FLOPs: 423,074,867\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 24/1723 finished in 0m15s\n",
      "Total channels prunned so far: 24\n",
      "\n",
      "Iteration 25 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 423)]\n",
      "Input: 0.115 MB, Params: 4,642,096 (17.708 MB), Total: 17.82 MB, FLOPs: 422,950,882\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 25/1723 finished in 0m15s\n",
      "Total channels prunned so far: 25\n",
      "\n",
      "Iteration 26 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 415)]\n",
      "Input: 0.115 MB, Params: 4,637,501 (17.691 MB), Total: 17.81 MB, FLOPs: 422,826,897\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 26/1723 finished in 0m15s\n",
      "Total channels prunned so far: 26\n",
      "\n",
      "Iteration 27 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 468)]\n",
      "Input: 0.115 MB, Params: 4,632,906 (17.673 MB), Total: 17.79 MB, FLOPs: 422,702,912\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 27/1723 finished in 0m15s\n",
      "Total channels prunned so far: 27\n",
      "\n",
      "Iteration 28 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 470)]\n",
      "Input: 0.115 MB, Params: 4,628,311 (17.656 MB), Total: 17.77 MB, FLOPs: 422,578,927\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 28/1723 finished in 0m15s\n",
      "Total channels prunned so far: 28\n",
      "\n",
      "Iteration 29 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 496)]\n",
      "Input: 0.115 MB, Params: 4,623,716 (17.638 MB), Total: 17.75 MB, FLOPs: 422,454,942\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 29/1723 finished in 0m15s\n",
      "Total channels prunned so far: 29\n",
      "\n",
      "Iteration 30 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 0)]\n",
      "Input: 0.115 MB, Params: 4,616,820 (17.612 MB), Total: 17.73 MB, FLOPs: 422,081,964\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 30/1723 finished in 0m15s\n",
      "Total channels prunned so far: 30\n",
      "\n",
      "Iteration 31 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 119)]\n",
      "Input: 0.115 MB, Params: 4,610,041 (17.586 MB), Total: 17.70 MB, FLOPs: 421,898,958\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 31/1723 finished in 0m15s\n",
      "Total channels prunned so far: 31\n",
      "\n",
      "Iteration 32 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 142)]\n",
      "Input: 0.115 MB, Params: 4,603,262 (17.560 MB), Total: 17.68 MB, FLOPs: 421,715,952\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 32/1723 finished in 0m15s\n",
      "Total channels prunned so far: 32\n",
      "\n",
      "Iteration 33 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 292)]\n",
      "Input: 0.115 MB, Params: 4,598,685 (17.543 MB), Total: 17.66 MB, FLOPs: 421,592,453\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 33/1723 finished in 0m15s\n",
      "Total channels prunned so far: 33\n",
      "\n",
      "Iteration 34 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 50)]\n",
      "Input: 0.115 MB, Params: 4,596,955 (17.536 MB), Total: 17.65 MB, FLOPs: 419,911,267\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 34/1723 finished in 0m15s\n",
      "Total channels prunned so far: 34\n",
      "\n",
      "Iteration 35 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 41)]\n",
      "Input: 0.115 MB, Params: 4,596,107 (17.533 MB), Total: 17.65 MB, FLOPs: 418,259,617\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 35/1723 finished in 0m15s\n",
      "Total channels prunned so far: 35\n",
      "\n",
      "Iteration 36 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 393)]\n",
      "Input: 0.115 MB, Params: 4,589,337 (17.507 MB), Total: 17.62 MB, FLOPs: 418,076,854\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 36/1723 finished in 0m15s\n",
      "Total channels prunned so far: 36\n",
      "\n",
      "Iteration 37 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 363)]\n",
      "Input: 0.115 MB, Params: 4,584,769 (17.490 MB), Total: 17.60 MB, FLOPs: 417,953,598\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 37/1723 finished in 0m15s\n",
      "Total channels prunned so far: 37\n",
      "\n",
      "Iteration 38 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 183)]\n",
      "Input: 0.115 MB, Params: 4,580,201 (17.472 MB), Total: 17.59 MB, FLOPs: 417,830,342\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 38/1723 finished in 0m15s\n",
      "Total channels prunned so far: 38\n",
      "\n",
      "Iteration 39 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 68)]\n",
      "Input: 0.115 MB, Params: 4,576,770 (17.459 MB), Total: 17.57 MB, FLOPs: 417,459,902\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 39/1723 finished in 0m15s\n",
      "Total channels prunned so far: 39\n",
      "\n",
      "Iteration 40 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 196)]\n",
      "Input: 0.115 MB, Params: 4,573,339 (17.446 MB), Total: 17.56 MB, FLOPs: 417,089,462\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 40/1723 finished in 0m15s\n",
      "Total channels prunned so far: 40\n",
      "\n",
      "Iteration 41 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 467)]\n",
      "Input: 0.115 MB, Params: 4,566,587 (17.420 MB), Total: 17.54 MB, FLOPs: 416,907,185\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 41/1723 finished in 0m15s\n",
      "Total channels prunned so far: 41\n",
      "\n",
      "Iteration 42 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 93)]\n",
      "Input: 0.115 MB, Params: 4,562,028 (17.403 MB), Total: 17.52 MB, FLOPs: 416,784,172\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 42/1723 finished in 0m15s\n",
      "Total channels prunned so far: 42\n",
      "\n",
      "Iteration 43 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 19)]\n",
      "Input: 0.115 MB, Params: 4,555,186 (17.377 MB), Total: 17.49 MB, FLOPs: 416,414,110\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 43/1723 finished in 0m15s\n",
      "Total channels prunned so far: 43\n",
      "\n",
      "Iteration 44 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 118)]\n",
      "Input: 0.115 MB, Params: 4,550,627 (17.359 MB), Total: 17.47 MB, FLOPs: 416,291,097\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 44/1723 finished in 0m15s\n",
      "Total channels prunned so far: 44\n",
      "\n",
      "Iteration 45 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 456)]\n",
      "Input: 0.115 MB, Params: 4,543,902 (17.334 MB), Total: 17.45 MB, FLOPs: 416,109,549\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 45/1723 finished in 0m15s\n",
      "Total channels prunned so far: 45\n",
      "\n",
      "Iteration 46 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 402)]\n",
      "Input: 0.115 MB, Params: 4,539,352 (17.316 MB), Total: 17.43 MB, FLOPs: 415,986,779\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 46/1723 finished in 0m15s\n",
      "Total channels prunned so far: 46\n",
      "\n",
      "Iteration 47 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 71)]\n",
      "Input: 0.115 MB, Params: 4,532,519 (17.290 MB), Total: 17.41 MB, FLOPs: 415,616,960\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 47/1723 finished in 0m15s\n",
      "Total channels prunned so far: 47\n",
      "\n",
      "Iteration 48 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 223)]\n",
      "Input: 0.115 MB, Params: 4,525,812 (17.265 MB), Total: 17.38 MB, FLOPs: 415,435,898\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 48/1723 finished in 0m15s\n",
      "Total channels prunned so far: 48\n",
      "\n",
      "Iteration 49 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 83)]\n",
      "Input: 0.115 MB, Params: 4,519,105 (17.239 MB), Total: 17.35 MB, FLOPs: 415,254,836\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 49/1723 finished in 0m15s\n",
      "Total channels prunned so far: 49\n",
      "\n",
      "Iteration 50 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 248)]\n",
      "Input: 0.115 MB, Params: 4,514,573 (17.222 MB), Total: 17.34 MB, FLOPs: 415,132,552\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 50/1723 finished in 0m15s\n",
      "Total channels prunned so far: 50\n",
      "\n",
      "Iteration 51 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 198)]\n",
      "Input: 0.115 MB, Params: 4,507,875 (17.196 MB), Total: 17.31 MB, FLOPs: 414,951,733\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 51/1723 finished in 0m15s\n",
      "Total channels prunned so far: 51\n",
      "\n",
      "Iteration 52 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 102)]\n",
      "Input: 0.115 MB, Params: 4,503,352 (17.179 MB), Total: 17.29 MB, FLOPs: 414,829,692\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 52/1723 finished in 0m15s\n",
      "Total channels prunned so far: 52\n",
      "\n",
      "Iteration 53 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 277)]\n",
      "Input: 0.115 MB, Params: 4,498,829 (17.162 MB), Total: 17.28 MB, FLOPs: 414,707,651\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 53/1723 finished in 0m15s\n",
      "Total channels prunned so far: 53\n",
      "\n",
      "Iteration 54 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 102)]\n",
      "Input: 0.115 MB, Params: 4,492,149 (17.136 MB), Total: 17.25 MB, FLOPs: 414,527,318\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 54/1723 finished in 0m15s\n",
      "Total channels prunned so far: 54\n",
      "\n",
      "Iteration 55 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 317)]\n",
      "Input: 0.115 MB, Params: 4,487,635 (17.119 MB), Total: 17.23 MB, FLOPs: 414,405,520\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 55/1723 finished in 0m15s\n",
      "Total channels prunned so far: 55\n",
      "\n",
      "Iteration 56 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 217)]\n",
      "Input: 0.115 MB, Params: 4,480,964 (17.094 MB), Total: 17.21 MB, FLOPs: 414,225,430\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 56/1723 finished in 0m15s\n",
      "Total channels prunned so far: 56\n",
      "\n",
      "Iteration 57 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 196)]\n",
      "Input: 0.115 MB, Params: 4,477,551 (17.081 MB), Total: 17.20 MB, FLOPs: 413,856,934\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 57/1723 finished in 0m15s\n",
      "Total channels prunned so far: 57\n",
      "\n",
      "Iteration 58 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 86)]\n",
      "Input: 0.115 MB, Params: 4,474,138 (17.067 MB), Total: 17.18 MB, FLOPs: 413,488,438\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 58/1723 finished in 0m15s\n",
      "Total channels prunned so far: 58\n",
      "\n",
      "Iteration 59 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 345)]\n",
      "Input: 0.115 MB, Params: 4,469,633 (17.050 MB), Total: 17.17 MB, FLOPs: 413,366,883\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 59/1723 finished in 0m15s\n",
      "Total channels prunned so far: 59\n",
      "\n",
      "Iteration 60 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 144)]\n",
      "Input: 0.115 MB, Params: 4,465,128 (17.033 MB), Total: 17.15 MB, FLOPs: 413,245,328\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 60/1723 finished in 0m15s\n",
      "Total channels prunned so far: 60\n",
      "\n",
      "Iteration 61 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 326)]\n",
      "Input: 0.115 MB, Params: 4,458,475 (17.008 MB), Total: 17.12 MB, FLOPs: 413,065,724\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 61/1723 finished in 0m15s\n",
      "Total channels prunned so far: 61\n",
      "\n",
      "Iteration 62 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 160)]\n",
      "Input: 0.115 MB, Params: 4,451,714 (16.982 MB), Total: 17.10 MB, FLOPs: 412,699,307\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 62/1723 finished in 0m15s\n",
      "Total channels prunned so far: 62\n",
      "\n",
      "Iteration 63 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 452)]\n",
      "Input: 0.115 MB, Params: 4,447,218 (16.965 MB), Total: 17.08 MB, FLOPs: 412,577,995\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 63/1723 finished in 0m15s\n",
      "Total channels prunned so far: 63\n",
      "\n",
      "Iteration 64 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 167)]\n",
      "Input: 0.115 MB, Params: 4,443,814 (16.952 MB), Total: 17.07 MB, FLOPs: 412,210,471\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 64/1723 finished in 0m15s\n",
      "Total channels prunned so far: 64\n",
      "\n",
      "Iteration 65 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 6)]\n",
      "Input: 0.115 MB, Params: 4,442,966 (16.949 MB), Total: 17.06 MB, FLOPs: 410,558,821\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 65/1723 finished in 0m15s\n",
      "Total channels prunned so far: 65\n",
      "\n",
      "Iteration 66 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 401)]\n",
      "Input: 0.115 MB, Params: 4,436,331 (16.923 MB), Total: 17.04 MB, FLOPs: 410,379,703\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 66/1723 finished in 0m15s\n",
      "Total channels prunned so far: 66\n",
      "\n",
      "Iteration 67 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 180)]\n",
      "Input: 0.115 MB, Params: 4,429,696 (16.898 MB), Total: 17.01 MB, FLOPs: 410,200,585\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 67/1723 finished in 0m15s\n",
      "Total channels prunned so far: 67\n",
      "\n",
      "Iteration 68 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 65)]\n",
      "Input: 0.115 MB, Params: 4,423,061 (16.873 MB), Total: 16.99 MB, FLOPs: 410,021,467\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 68/1723 finished in 0m15s\n",
      "Total channels prunned so far: 68\n",
      "\n",
      "Iteration 69 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 41)]\n",
      "Input: 0.115 MB, Params: 4,416,336 (16.847 MB), Total: 16.96 MB, FLOPs: 409,656,751\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 69/1723 finished in 0m15s\n",
      "Total channels prunned so far: 69\n",
      "\n",
      "Iteration 70 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 328)]\n",
      "Input: 0.115 MB, Params: 4,409,710 (16.822 MB), Total: 16.94 MB, FLOPs: 409,477,876\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 70/1723 finished in 0m15s\n",
      "Total channels prunned so far: 70\n",
      "\n",
      "Iteration 71 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 353)]\n",
      "Input: 0.115 MB, Params: 4,403,084 (16.796 MB), Total: 16.91 MB, FLOPs: 409,299,001\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 71/1723 finished in 0m15s\n",
      "Total channels prunned so far: 71\n",
      "\n",
      "Iteration 72 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 63)]\n",
      "Input: 0.115 MB, Params: 4,396,377 (16.771 MB), Total: 16.89 MB, FLOPs: 408,934,771\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 72/1723 finished in 0m15s\n",
      "Total channels prunned so far: 72\n",
      "\n",
      "Iteration 73 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 265)]\n",
      "Input: 0.115 MB, Params: 4,389,760 (16.746 MB), Total: 16.86 MB, FLOPs: 408,756,139\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 73/1723 finished in 0m15s\n",
      "Total channels prunned so far: 73\n",
      "\n",
      "Iteration 74 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 310)]\n",
      "Input: 0.115 MB, Params: 4,383,143 (16.720 MB), Total: 16.84 MB, FLOPs: 408,577,507\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 74/1723 finished in 0m15s\n",
      "Total channels prunned so far: 74\n",
      "\n",
      "Iteration 75 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 177)]\n",
      "Input: 0.115 MB, Params: 4,378,710 (16.703 MB), Total: 16.82 MB, FLOPs: 408,457,896\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 75/1723 finished in 0m15s\n",
      "Total channels prunned so far: 75\n",
      "\n",
      "Iteration 76 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 465)]\n",
      "Input: 0.115 MB, Params: 4,372,102 (16.678 MB), Total: 16.79 MB, FLOPs: 408,279,507\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 76/1723 finished in 0m15s\n",
      "Total channels prunned so far: 76\n",
      "\n",
      "Iteration 77 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 234)]\n",
      "Input: 0.115 MB, Params: 4,365,494 (16.653 MB), Total: 16.77 MB, FLOPs: 408,101,118\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 77/1723 finished in 0m15s\n",
      "Total channels prunned so far: 77\n",
      "\n",
      "Iteration 78 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 107)]\n",
      "Input: 0.115 MB, Params: 4,358,823 (16.628 MB), Total: 16.74 MB, FLOPs: 407,737,860\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 78/1723 finished in 0m15s\n",
      "Total channels prunned so far: 78\n",
      "\n",
      "Iteration 79 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 365)]\n",
      "Input: 0.115 MB, Params: 4,354,408 (16.611 MB), Total: 16.73 MB, FLOPs: 407,618,735\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 79/1723 finished in 0m15s\n",
      "Total channels prunned so far: 79\n",
      "\n",
      "Iteration 80 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 279)]\n",
      "Input: 0.115 MB, Params: 4,349,993 (16.594 MB), Total: 16.71 MB, FLOPs: 407,499,610\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 80/1723 finished in 0m15s\n",
      "Total channels prunned so far: 80\n",
      "\n",
      "Iteration 81 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 182)]\n",
      "Input: 0.115 MB, Params: 4,345,578 (16.577 MB), Total: 16.69 MB, FLOPs: 407,380,485\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 81/1723 finished in 0m15s\n",
      "Total channels prunned so far: 81\n",
      "\n",
      "Iteration 82 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 342)]\n",
      "Input: 0.115 MB, Params: 4,341,163 (16.560 MB), Total: 16.68 MB, FLOPs: 407,261,360\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 82/1723 finished in 0m15s\n",
      "Total channels prunned so far: 82\n",
      "\n",
      "Iteration 83 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 125)]\n",
      "Input: 0.115 MB, Params: 4,336,748 (16.543 MB), Total: 16.66 MB, FLOPs: 407,142,235\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 83/1723 finished in 0m15s\n",
      "Total channels prunned so far: 83\n",
      "\n",
      "Iteration 84 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 296)]\n",
      "Input: 0.115 MB, Params: 4,332,333 (16.527 MB), Total: 16.64 MB, FLOPs: 407,023,110\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 84/1723 finished in 0m15s\n",
      "Total channels prunned so far: 84\n",
      "\n",
      "Iteration 85 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 283)]\n",
      "Input: 0.115 MB, Params: 4,325,788 (16.502 MB), Total: 16.62 MB, FLOPs: 406,846,422\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 85/1723 finished in 0m15s\n",
      "Total channels prunned so far: 85\n",
      "\n",
      "Iteration 86 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 207)]\n",
      "Input: 0.115 MB, Params: 4,321,382 (16.485 MB), Total: 16.60 MB, FLOPs: 406,727,540\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 86/1723 finished in 0m15s\n",
      "Total channels prunned so far: 86\n",
      "\n",
      "Iteration 87 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 458)]\n",
      "Input: 0.115 MB, Params: 4,316,976 (16.468 MB), Total: 16.58 MB, FLOPs: 406,608,658\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 87/1723 finished in 0m15s\n",
      "Total channels prunned so far: 87\n",
      "\n",
      "Iteration 88 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 64)]\n",
      "Input: 0.115 MB, Params: 4,310,314 (16.443 MB), Total: 16.56 MB, FLOPs: 406,245,643\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 88/1723 finished in 0m15s\n",
      "Total channels prunned so far: 88\n",
      "\n",
      "Iteration 89 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 163)]\n",
      "Input: 0.115 MB, Params: 4,305,908 (16.426 MB), Total: 16.54 MB, FLOPs: 406,126,761\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 89/1723 finished in 0m15s\n",
      "Total channels prunned so far: 89\n",
      "\n",
      "Iteration 90 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 152)]\n",
      "Input: 0.115 MB, Params: 4,299,399 (16.401 MB), Total: 16.52 MB, FLOPs: 405,951,045\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 90/1723 finished in 0m15s\n",
      "Total channels prunned so far: 90\n",
      "\n",
      "Iteration 91 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 471)]\n",
      "Input: 0.115 MB, Params: 4,295,002 (16.384 MB), Total: 16.50 MB, FLOPs: 405,832,406\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 91/1723 finished in 0m15s\n",
      "Total channels prunned so far: 91\n",
      "\n",
      "Iteration 92 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 26)]\n",
      "Input: 0.115 MB, Params: 4,288,502 (16.359 MB), Total: 16.47 MB, FLOPs: 405,656,933\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 92/1723 finished in 0m15s\n",
      "Total channels prunned so far: 92\n",
      "\n",
      "Iteration 93 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 37)]\n",
      "Input: 0.115 MB, Params: 4,286,790 (16.353 MB), Total: 16.47 MB, FLOPs: 404,833,942\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 93/1723 finished in 0m15s\n",
      "Total channels prunned so far: 93\n",
      "\n",
      "Iteration 94 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 198)]\n",
      "Input: 0.115 MB, Params: 4,280,290 (16.328 MB), Total: 16.44 MB, FLOPs: 404,658,469\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 94/1723 finished in 0m15s\n",
      "Total channels prunned so far: 94\n",
      "\n",
      "Iteration 95 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 15)]\n",
      "Input: 0.115 MB, Params: 4,278,578 (16.321 MB), Total: 16.44 MB, FLOPs: 403,835,478\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 95/1723 finished in 0m15s\n",
      "Total channels prunned so far: 95\n",
      "\n",
      "Iteration 96 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 2)]\n",
      "Input: 0.115 MB, Params: 4,277,730 (16.318 MB), Total: 16.43 MB, FLOPs: 402,183,828\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 96/1723 finished in 0m15s\n",
      "Total channels prunned so far: 96\n",
      "\n",
      "Iteration 97 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 80)]\n",
      "Input: 0.115 MB, Params: 4,271,095 (16.293 MB), Total: 16.41 MB, FLOPs: 401,821,542\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 97/1723 finished in 0m15s\n",
      "Total channels prunned so far: 97\n",
      "\n",
      "Iteration 98 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 102)]\n",
      "Input: 0.115 MB, Params: 4,266,716 (16.276 MB), Total: 16.39 MB, FLOPs: 401,703,389\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 98/1723 finished in 0m15s\n",
      "Total channels prunned so far: 98\n",
      "\n",
      "Iteration 99 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 57)]\n",
      "Input: 0.115 MB, Params: 4,262,337 (16.260 MB), Total: 16.37 MB, FLOPs: 401,585,236\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 99/1723 finished in 0m15s\n",
      "Total channels prunned so far: 99\n",
      "\n",
      "Iteration 100 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 222)]\n",
      "Input: 0.115 MB, Params: 4,255,702 (16.234 MB), Total: 16.35 MB, FLOPs: 401,222,950\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 100/1723 finished in 0m15s\n",
      "Total channels prunned so far: 100\n",
      "\n",
      "Iteration 101 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 51)]\n",
      "Input: 0.115 MB, Params: 4,254,854 (16.231 MB), Total: 16.35 MB, FLOPs: 399,571,300\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 101/1723 finished in 0m15s\n",
      "Total channels prunned so far: 101\n",
      "\n",
      "Iteration 102 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 456)]\n",
      "Input: 0.115 MB, Params: 4,250,475 (16.214 MB), Total: 16.33 MB, FLOPs: 399,453,147\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 102/1723 finished in 0m15s\n",
      "Total channels prunned so far: 102\n",
      "\n",
      "Iteration 103 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 127)]\n",
      "Input: 0.115 MB, Params: 4,244,020 (16.190 MB), Total: 16.30 MB, FLOPs: 399,278,889\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 103/1723 finished in 0m15s\n",
      "Total channels prunned so far: 103\n",
      "\n",
      "Iteration 104 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 262)]\n",
      "Input: 0.115 MB, Params: 4,239,650 (16.173 MB), Total: 16.29 MB, FLOPs: 399,160,979\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 104/1723 finished in 0m15s\n",
      "Total channels prunned so far: 104\n",
      "\n",
      "Iteration 105 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 96)]\n",
      "Input: 0.115 MB, Params: 4,237,938 (16.166 MB), Total: 16.28 MB, FLOPs: 398,337,988\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.831%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 105/1723 finished in 0m15s\n",
      "Total channels prunned so far: 105\n",
      "\n",
      "Iteration 106 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 295)]\n",
      "Input: 0.115 MB, Params: 4,233,568 (16.150 MB), Total: 16.27 MB, FLOPs: 398,220,078\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 106/1723 finished in 0m15s\n",
      "Total channels prunned so far: 106\n",
      "\n",
      "Iteration 107 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 225)]\n",
      "Input: 0.115 MB, Params: 4,227,131 (16.125 MB), Total: 16.24 MB, FLOPs: 398,046,306\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 107/1723 finished in 0m15s\n",
      "Total channels prunned so far: 107\n",
      "\n",
      "Iteration 108 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 7)]\n",
      "Input: 0.115 MB, Params: 4,223,781 (16.112 MB), Total: 16.23 MB, FLOPs: 397,684,614\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 108/1723 finished in 0m15s\n",
      "Total channels prunned so far: 108\n",
      "\n",
      "Iteration 109 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 80)]\n",
      "Input: 0.115 MB, Params: 4,220,431 (16.100 MB), Total: 16.21 MB, FLOPs: 397,322,922\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 109/1723 finished in 0m15s\n",
      "Total channels prunned so far: 109\n",
      "\n",
      "Iteration 110 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 194)]\n",
      "Input: 0.115 MB, Params: 4,216,070 (16.083 MB), Total: 16.20 MB, FLOPs: 397,205,255\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Finished fine tuning.\n",
      "Iteration 110/1723 finished in 0m15s\n",
      "Total channels prunned so far: 110\n",
      "\n",
      "Iteration 111 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 206)]\n",
      "Input: 0.115 MB, Params: 4,212,720 (16.070 MB), Total: 16.19 MB, FLOPs: 396,843,563\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 111/1723 finished in 0m15s\n",
      "Total channels prunned so far: 111\n",
      "\n",
      "Iteration 112 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 169)]\n",
      "Input: 0.115 MB, Params: 4,206,292 (16.046 MB), Total: 16.16 MB, FLOPs: 396,670,034\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 112/1723 finished in 0m15s\n",
      "Total channels prunned so far: 112\n",
      "\n",
      "Iteration 113 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 403)]\n",
      "Input: 0.115 MB, Params: 4,199,864 (16.021 MB), Total: 16.14 MB, FLOPs: 396,496,505\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 113/1723 finished in 0m15s\n",
      "Total channels prunned so far: 113\n",
      "\n",
      "Iteration 114 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 402)]\n",
      "Input: 0.115 MB, Params: 4,193,436 (15.997 MB), Total: 16.11 MB, FLOPs: 396,322,976\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 114/1723 finished in 0m15s\n",
      "Total channels prunned so far: 114\n",
      "\n",
      "Iteration 115 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 18)]\n",
      "Input: 0.115 MB, Params: 4,191,769 (15.990 MB), Total: 16.11 MB, FLOPs: 394,724,977\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 115/1723 finished in 0m15s\n",
      "Total channels prunned so far: 115\n",
      "\n",
      "Iteration 116 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 33)]\n",
      "Input: 0.115 MB, Params: 4,185,206 (15.965 MB), Total: 16.08 MB, FLOPs: 394,366,822\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 116/1723 finished in 0m15s\n",
      "Total channels prunned so far: 116\n",
      "\n",
      "Iteration 117 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 252)]\n",
      "Input: 0.115 MB, Params: 4,178,787 (15.941 MB), Total: 16.06 MB, FLOPs: 394,193,536\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 117/1723 finished in 0m15s\n",
      "Total channels prunned so far: 117\n",
      "\n",
      "Iteration 118 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 27)]\n",
      "Input: 0.115 MB, Params: 4,172,368 (15.916 MB), Total: 16.03 MB, FLOPs: 394,020,250\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 118/1723 finished in 0m15s\n",
      "Total channels prunned so far: 118\n",
      "\n",
      "Iteration 119 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 208)]\n",
      "Input: 0.115 MB, Params: 4,168,052 (15.900 MB), Total: 16.02 MB, FLOPs: 393,903,798\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 119/1723 finished in 0m15s\n",
      "Total channels prunned so far: 119\n",
      "\n",
      "Iteration 120 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 206)]\n",
      "Input: 0.115 MB, Params: 4,164,711 (15.887 MB), Total: 16.00 MB, FLOPs: 393,543,078\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 120/1723 finished in 0m15s\n",
      "Total channels prunned so far: 120\n",
      "\n",
      "Iteration 121 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 398)]\n",
      "Input: 0.115 MB, Params: 4,160,395 (15.871 MB), Total: 15.99 MB, FLOPs: 393,426,626\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 121/1723 finished in 0m15s\n",
      "Total channels prunned so far: 121\n",
      "\n",
      "Iteration 122 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 324)]\n",
      "Input: 0.115 MB, Params: 4,156,079 (15.854 MB), Total: 15.97 MB, FLOPs: 393,310,174\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 122/1723 finished in 0m15s\n",
      "Total channels prunned so far: 122\n",
      "\n",
      "Iteration 123 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 418)]\n",
      "Input: 0.115 MB, Params: 4,151,763 (15.838 MB), Total: 15.95 MB, FLOPs: 393,193,722\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 123/1723 finished in 0m15s\n",
      "Total channels prunned so far: 123\n",
      "\n",
      "Iteration 124 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 416)]\n",
      "Input: 0.115 MB, Params: 4,147,447 (15.821 MB), Total: 15.94 MB, FLOPs: 393,077,270\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 124/1723 finished in 0m15s\n",
      "Total channels prunned so far: 124\n",
      "\n",
      "Iteration 125 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 297)]\n",
      "Input: 0.115 MB, Params: 4,143,131 (15.805 MB), Total: 15.92 MB, FLOPs: 392,960,818\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 125/1723 finished in 0m15s\n",
      "Total channels prunned so far: 125\n",
      "\n",
      "Iteration 126 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 363)]\n",
      "Input: 0.115 MB, Params: 4,138,815 (15.788 MB), Total: 15.90 MB, FLOPs: 392,844,366\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 126/1723 finished in 0m15s\n",
      "Total channels prunned so far: 126\n",
      "\n",
      "Iteration 127 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 68)]\n",
      "Input: 0.115 MB, Params: 4,132,459 (15.764 MB), Total: 15.88 MB, FLOPs: 392,672,781\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 127/1723 finished in 0m15s\n",
      "Total channels prunned so far: 127\n",
      "\n",
      "Iteration 128 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 79)]\n",
      "Input: 0.115 MB, Params: 4,126,103 (15.740 MB), Total: 15.86 MB, FLOPs: 392,501,196\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 128/1723 finished in 0m15s\n",
      "Total channels prunned so far: 128\n",
      "\n",
      "Iteration 129 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 126)]\n",
      "Input: 0.115 MB, Params: 4,121,805 (15.723 MB), Total: 15.84 MB, FLOPs: 392,385,230\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 129/1723 finished in 0m15s\n",
      "Total channels prunned so far: 129\n",
      "\n",
      "Iteration 130 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 205)]\n",
      "Input: 0.115 MB, Params: 4,115,287 (15.699 MB), Total: 15.81 MB, FLOPs: 392,029,019\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 130/1723 finished in 0m15s\n",
      "Total channels prunned so far: 130\n",
      "\n",
      "Iteration 131 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 47)]\n",
      "Input: 0.115 MB, Params: 4,110,989 (15.682 MB), Total: 15.80 MB, FLOPs: 391,913,053\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 131/1723 finished in 0m15s\n",
      "Total channels prunned so far: 131\n",
      "\n",
      "Iteration 132 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 327)]\n",
      "Input: 0.115 MB, Params: 4,106,691 (15.666 MB), Total: 15.78 MB, FLOPs: 391,797,087\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 132/1723 finished in 0m15s\n",
      "Total channels prunned so far: 132\n",
      "\n",
      "Iteration 133 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 456)]\n",
      "Input: 0.115 MB, Params: 4,100,371 (15.642 MB), Total: 15.76 MB, FLOPs: 391,626,474\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 133/1723 finished in 0m15s\n",
      "Total channels prunned so far: 133\n",
      "\n",
      "Iteration 134 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 90)]\n",
      "Input: 0.115 MB, Params: 4,094,051 (15.618 MB), Total: 15.73 MB, FLOPs: 391,455,861\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 134/1723 finished in 0m15s\n",
      "Total channels prunned so far: 134\n",
      "\n",
      "Iteration 135 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 3)]\n",
      "Input: 0.115 MB, Params: 4,087,551 (15.593 MB), Total: 15.71 MB, FLOPs: 391,100,136\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 135/1723 finished in 0m15s\n",
      "Total channels prunned so far: 135\n",
      "\n",
      "Iteration 136 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 67)]\n",
      "Input: 0.115 MB, Params: 4,081,240 (15.569 MB), Total: 15.68 MB, FLOPs: 390,929,766\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 136/1723 finished in 0m15s\n",
      "Total channels prunned so far: 136\n",
      "\n",
      "Iteration 137 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 143)]\n",
      "Input: 0.115 MB, Params: 4,077,917 (15.556 MB), Total: 15.67 MB, FLOPs: 390,570,990\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 137/1723 finished in 0m15s\n",
      "Total channels prunned so far: 137\n",
      "\n",
      "Iteration 138 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 111)]\n",
      "Input: 0.115 MB, Params: 4,076,214 (15.550 MB), Total: 15.66 MB, FLOPs: 389,752,328\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 138/1723 finished in 0m15s\n",
      "Total channels prunned so far: 138\n",
      "\n",
      "Iteration 139 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 344)]\n",
      "Input: 0.115 MB, Params: 4,069,903 (15.525 MB), Total: 15.64 MB, FLOPs: 389,581,958\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 139/1723 finished in 0m15s\n",
      "Total channels prunned so far: 139\n",
      "\n",
      "Iteration 140 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 45)]\n",
      "Input: 0.115 MB, Params: 4,069,861 (15.525 MB), Total: 15.64 MB, FLOPs: 389,217,925\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 140/1723 finished in 0m15s\n",
      "Total channels prunned so far: 140\n",
      "\n",
      "Iteration 141 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 4)]\n",
      "Input: 0.115 MB, Params: 4,063,388 (15.501 MB), Total: 15.62 MB, FLOPs: 388,863,658\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 141/1723 finished in 0m15s\n",
      "Total channels prunned so far: 141\n",
      "\n",
      "Iteration 142 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 95)]\n",
      "Input: 0.115 MB, Params: 4,056,915 (15.476 MB), Total: 15.59 MB, FLOPs: 388,509,391\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 142/1723 finished in 0m15s\n",
      "Total channels prunned so far: 142\n",
      "\n",
      "Iteration 143 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 374)]\n",
      "Input: 0.115 MB, Params: 4,050,622 (15.452 MB), Total: 15.57 MB, FLOPs: 388,339,507\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 143/1723 finished in 0m15s\n",
      "Total channels prunned so far: 143\n",
      "\n",
      "Iteration 144 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 353)]\n",
      "Input: 0.115 MB, Params: 4,046,369 (15.436 MB), Total: 15.55 MB, FLOPs: 388,224,756\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 144/1723 finished in 0m15s\n",
      "Total channels prunned so far: 144\n",
      "\n",
      "Iteration 145 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 142)]\n",
      "Input: 0.115 MB, Params: 4,043,064 (15.423 MB), Total: 15.54 MB, FLOPs: 387,867,924\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 145/1723 finished in 0m15s\n",
      "Total channels prunned so far: 145\n",
      "\n",
      "Iteration 146 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 371)]\n",
      "Input: 0.115 MB, Params: 4,036,780 (15.399 MB), Total: 15.51 MB, FLOPs: 387,698,283\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 146/1723 finished in 0m15s\n",
      "Total channels prunned so far: 146\n",
      "\n",
      "Iteration 147 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 22)]\n",
      "Input: 0.115 MB, Params: 4,033,475 (15.386 MB), Total: 15.50 MB, FLOPs: 387,341,451\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 147/1723 finished in 0m15s\n",
      "Total channels prunned so far: 147\n",
      "\n",
      "Iteration 148 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 138)]\n",
      "Input: 0.115 MB, Params: 4,027,191 (15.363 MB), Total: 15.48 MB, FLOPs: 387,171,810\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 148/1723 finished in 0m15s\n",
      "Total channels prunned so far: 148\n",
      "\n",
      "Iteration 149 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 95)]\n",
      "Input: 0.115 MB, Params: 4,025,488 (15.356 MB), Total: 15.47 MB, FLOPs: 386,353,148\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 149/1723 finished in 0m15s\n",
      "Total channels prunned so far: 149\n",
      "\n",
      "Iteration 150 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 120)]\n",
      "Input: 0.115 MB, Params: 4,023,785 (15.350 MB), Total: 15.46 MB, FLOPs: 385,534,486\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 150/1723 finished in 0m15s\n",
      "Total channels prunned so far: 150\n",
      "\n",
      "Iteration 151 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 375)]\n",
      "Input: 0.115 MB, Params: 4,017,501 (15.326 MB), Total: 15.44 MB, FLOPs: 385,364,845\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 151/1723 finished in 0m15s\n",
      "Total channels prunned so far: 151\n",
      "\n",
      "Iteration 152 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 244)]\n",
      "Input: 0.115 MB, Params: 4,011,217 (15.302 MB), Total: 15.42 MB, FLOPs: 385,195,204\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 152/1723 finished in 0m15s\n",
      "Total channels prunned so far: 152\n",
      "\n",
      "Iteration 153 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 115)]\n",
      "Input: 0.115 MB, Params: 4,007,000 (15.285 MB), Total: 15.40 MB, FLOPs: 385,081,425\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 153/1723 finished in 0m15s\n",
      "Total channels prunned so far: 153\n",
      "\n",
      "Iteration 154 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 264)]\n",
      "Input: 0.115 MB, Params: 4,000,725 (15.262 MB), Total: 15.38 MB, FLOPs: 384,912,027\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 154/1723 finished in 0m15s\n",
      "Total channels prunned so far: 154\n",
      "\n",
      "Iteration 155 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 47)]\n",
      "Input: 0.115 MB, Params: 3,996,517 (15.246 MB), Total: 15.36 MB, FLOPs: 384,798,491\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 155/1723 finished in 0m15s\n",
      "Total channels prunned so far: 155\n",
      "\n",
      "Iteration 156 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 181)]\n",
      "Input: 0.115 MB, Params: 3,990,251 (15.222 MB), Total: 15.34 MB, FLOPs: 384,629,336\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 156/1723 finished in 0m15s\n",
      "Total channels prunned so far: 156\n",
      "\n",
      "Iteration 157 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 63)]\n",
      "Input: 0.115 MB, Params: 3,988,548 (15.215 MB), Total: 15.33 MB, FLOPs: 383,810,674\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 157/1723 finished in 0m15s\n",
      "Total channels prunned so far: 157\n",
      "\n",
      "Iteration 158 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 150)]\n",
      "Input: 0.115 MB, Params: 3,982,282 (15.191 MB), Total: 15.31 MB, FLOPs: 383,641,519\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 158/1723 finished in 0m15s\n",
      "Total channels prunned so far: 158\n",
      "\n",
      "Iteration 159 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 103)]\n",
      "Input: 0.115 MB, Params: 3,976,016 (15.167 MB), Total: 15.28 MB, FLOPs: 383,472,364\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 159/1723 finished in 0m15s\n",
      "Total channels prunned so far: 159\n",
      "\n",
      "Iteration 160 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 93)]\n",
      "Input: 0.115 MB, Params: 3,972,711 (15.155 MB), Total: 15.27 MB, FLOPs: 383,115,532\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 160/1723 finished in 0m15s\n",
      "Total channels prunned so far: 160\n",
      "\n",
      "Iteration 161 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 219)]\n",
      "Input: 0.115 MB, Params: 3,966,346 (15.130 MB), Total: 15.25 MB, FLOPs: 382,766,368\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 161/1723 finished in 0m15s\n",
      "Total channels prunned so far: 161\n",
      "\n",
      "Iteration 162 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 22)]\n",
      "Input: 0.115 MB, Params: 3,966,304 (15.130 MB), Total: 15.25 MB, FLOPs: 370,983,656\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 162/1723 finished in 0m15s\n",
      "Total channels prunned so far: 162\n",
      "\n",
      "Iteration 163 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 103)]\n",
      "Input: 0.115 MB, Params: 3,963,008 (15.118 MB), Total: 15.23 MB, FLOPs: 370,627,796\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 163/1723 finished in 0m15s\n",
      "Total channels prunned so far: 163\n",
      "\n",
      "Iteration 164 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 146)]\n",
      "Input: 0.115 MB, Params: 3,956,751 (15.094 MB), Total: 15.21 MB, FLOPs: 370,458,884\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 164/1723 finished in 0m15s\n",
      "Total channels prunned so far: 164\n",
      "\n",
      "Iteration 165 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 438)]\n",
      "Input: 0.115 MB, Params: 3,950,494 (15.070 MB), Total: 15.19 MB, FLOPs: 370,289,972\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 165/1723 finished in 0m15s\n",
      "Total channels prunned so far: 165\n",
      "\n",
      "Iteration 166 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 13)]\n",
      "Input: 0.115 MB, Params: 3,944,237 (15.046 MB), Total: 15.16 MB, FLOPs: 370,121,060\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 166/1723 finished in 0m15s\n",
      "Total channels prunned so far: 166\n",
      "\n",
      "Iteration 167 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 223)]\n",
      "Input: 0.115 MB, Params: 3,940,083 (15.030 MB), Total: 15.15 MB, FLOPs: 370,008,982\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 167/1723 finished in 0m15s\n",
      "Total channels prunned so far: 167\n",
      "\n",
      "Iteration 168 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 153)]\n",
      "Input: 0.115 MB, Params: 3,936,787 (15.018 MB), Total: 15.13 MB, FLOPs: 369,653,122\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 168/1723 finished in 0m17s\n",
      "Total channels prunned so far: 168\n",
      "\n",
      "Iteration 169 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 126)]\n",
      "Input: 0.115 MB, Params: 3,930,539 (14.994 MB), Total: 15.11 MB, FLOPs: 369,484,453\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 169/1723 finished in 0m20s\n",
      "Total channels prunned so far: 169\n",
      "\n",
      "Iteration 170 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 361)]\n",
      "Input: 0.115 MB, Params: 3,924,291 (14.970 MB), Total: 15.09 MB, FLOPs: 369,315,784\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 170/1723 finished in 0m20s\n",
      "Total channels prunned so far: 170\n",
      "\n",
      "Iteration 171 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 451)]\n",
      "Input: 0.115 MB, Params: 3,920,155 (14.954 MB), Total: 15.07 MB, FLOPs: 369,204,192\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 171/1723 finished in 0m19s\n",
      "Total channels prunned so far: 171\n",
      "\n",
      "Iteration 172 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 207)]\n",
      "Input: 0.115 MB, Params: 3,913,916 (14.930 MB), Total: 15.05 MB, FLOPs: 369,035,766\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 172/1723 finished in 0m19s\n",
      "Total channels prunned so far: 172\n",
      "\n",
      "Iteration 173 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 254)]\n",
      "Input: 0.115 MB, Params: 3,909,789 (14.915 MB), Total: 15.03 MB, FLOPs: 368,924,417\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 173/1723 finished in 0m20s\n",
      "Total channels prunned so far: 173\n",
      "\n",
      "Iteration 174 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 49)]\n",
      "Input: 0.115 MB, Params: 3,908,950 (14.911 MB), Total: 15.03 MB, FLOPs: 367,353,167\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 174/1723 finished in 0m20s\n",
      "Total channels prunned so far: 174\n",
      "\n",
      "Iteration 175 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 26)]\n",
      "Input: 0.115 MB, Params: 3,907,328 (14.905 MB), Total: 15.02 MB, FLOPs: 365,870,375\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 175/1723 finished in 0m16s\n",
      "Total channels prunned so far: 175\n",
      "\n",
      "Iteration 176 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 64)]\n",
      "Input: 0.115 MB, Params: 3,901,098 (14.882 MB), Total: 15.00 MB, FLOPs: 365,702,192\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Finished fine tuning.\n",
      "Iteration 176/1723 finished in 0m15s\n",
      "Total channels prunned so far: 176\n",
      "\n",
      "Iteration 177 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 14)]\n",
      "Input: 0.115 MB, Params: 3,899,476 (14.875 MB), Total: 14.99 MB, FLOPs: 364,219,400\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 177/1723 finished in 0m15s\n",
      "Total channels prunned so far: 177\n",
      "\n",
      "Iteration 178 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 303)]\n",
      "Input: 0.115 MB, Params: 3,895,358 (14.860 MB), Total: 14.97 MB, FLOPs: 364,108,294\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 178/1723 finished in 0m15s\n",
      "Total channels prunned so far: 178\n",
      "\n",
      "Iteration 179 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 72)]\n",
      "Input: 0.115 MB, Params: 3,889,074 (14.836 MB), Total: 14.95 MB, FLOPs: 363,762,775\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 179/1723 finished in 0m15s\n",
      "Total channels prunned so far: 179\n",
      "\n",
      "Iteration 180 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 182)]\n",
      "Input: 0.115 MB, Params: 3,882,862 (14.812 MB), Total: 14.93 MB, FLOPs: 363,595,078\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Finished fine tuning.\n",
      "Iteration 180/1723 finished in 0m15s\n",
      "Total channels prunned so far: 180\n",
      "\n",
      "Iteration 181 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 199)]\n",
      "Input: 0.115 MB, Params: 3,876,650 (14.788 MB), Total: 14.90 MB, FLOPs: 363,427,381\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 181/1723 finished in 0m15s\n",
      "Total channels prunned so far: 181\n",
      "\n",
      "Iteration 182 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 1)]\n",
      "Input: 0.115 MB, Params: 3,870,384 (14.764 MB), Total: 14.88 MB, FLOPs: 363,082,348\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 182/1723 finished in 0m15s\n",
      "Total channels prunned so far: 182\n",
      "\n",
      "Iteration 183 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 420)]\n",
      "Input: 0.115 MB, Params: 3,864,181 (14.741 MB), Total: 14.86 MB, FLOPs: 362,914,894\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 183/1723 finished in 0m19s\n",
      "Total channels prunned so far: 183\n",
      "\n",
      "Iteration 184 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 241)]\n",
      "Input: 0.115 MB, Params: 3,860,090 (14.725 MB), Total: 14.84 MB, FLOPs: 362,804,517\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 184/1723 finished in 0m19s\n",
      "Total channels prunned so far: 184\n",
      "\n",
      "Iteration 185 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 119)]\n",
      "Input: 0.115 MB, Params: 3,853,896 (14.701 MB), Total: 14.82 MB, FLOPs: 362,637,306\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 185/1723 finished in 0m20s\n",
      "Total channels prunned so far: 185\n",
      "\n",
      "Iteration 186 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 29)]\n",
      "Input: 0.115 MB, Params: 3,853,075 (14.698 MB), Total: 14.81 MB, FLOPs: 361,099,806\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 186/1723 finished in 0m20s\n",
      "Total channels prunned so far: 186\n",
      "\n",
      "Iteration 187 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 193)]\n",
      "Input: 0.115 MB, Params: 3,849,797 (14.686 MB), Total: 14.80 MB, FLOPs: 360,745,890\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 187/1723 finished in 0m20s\n",
      "Total channels prunned so far: 187\n",
      "\n",
      "Iteration 188 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 73)]\n",
      "Input: 0.115 MB, Params: 3,845,715 (14.670 MB), Total: 14.79 MB, FLOPs: 360,635,756\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 188/1723 finished in 0m19s\n",
      "Total channels prunned so far: 188\n",
      "\n",
      "Iteration 189 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 119)]\n",
      "Input: 0.115 MB, Params: 3,841,633 (14.655 MB), Total: 14.77 MB, FLOPs: 360,525,622\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 189/1723 finished in 0m18s\n",
      "Total channels prunned so far: 189\n",
      "\n",
      "Iteration 190 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 35)]\n",
      "Input: 0.115 MB, Params: 3,835,457 (14.631 MB), Total: 14.75 MB, FLOPs: 360,358,897\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 96.186%\n",
      "Finished fine tuning.\n",
      "Iteration 190/1723 finished in 0m16s\n",
      "Total channels prunned so far: 190\n",
      "\n",
      "Iteration 191 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 271)]\n",
      "Input: 0.115 MB, Params: 3,831,384 (14.616 MB), Total: 14.73 MB, FLOPs: 360,249,006\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.915%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 191/1723 finished in 0m15s\n",
      "Total channels prunned so far: 191\n",
      "\n",
      "Iteration 192 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 286)]\n",
      "Input: 0.115 MB, Params: 3,827,311 (14.600 MB), Total: 14.72 MB, FLOPs: 360,139,115\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 192/1723 finished in 0m15s\n",
      "Total channels prunned so far: 192\n",
      "\n",
      "Iteration 193 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 70)]\n",
      "Input: 0.115 MB, Params: 3,821,081 (14.576 MB), Total: 14.69 MB, FLOPs: 359,795,783\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 193/1723 finished in 0m15s\n",
      "Total channels prunned so far: 193\n",
      "\n",
      "Iteration 194 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 60)]\n",
      "Input: 0.115 MB, Params: 3,814,851 (14.553 MB), Total: 14.67 MB, FLOPs: 359,452,451\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 194/1723 finished in 0m15s\n",
      "Total channels prunned so far: 194\n",
      "\n",
      "Iteration 195 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 80)]\n",
      "Input: 0.115 MB, Params: 3,811,591 (14.540 MB), Total: 14.66 MB, FLOPs: 359,100,479\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 195/1723 finished in 0m15s\n",
      "Total channels prunned so far: 195\n",
      "\n",
      "Iteration 196 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 5)]\n",
      "Input: 0.115 MB, Params: 3,810,770 (14.537 MB), Total: 14.65 MB, FLOPs: 357,562,979\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 196/1723 finished in 0m15s\n",
      "Total channels prunned so far: 196\n",
      "\n",
      "Iteration 197 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 39)]\n",
      "Input: 0.115 MB, Params: 3,804,549 (14.513 MB), Total: 14.63 MB, FLOPs: 357,220,619\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.254%\n",
      "Finished fine tuning.\n",
      "Iteration 197/1723 finished in 0m15s\n",
      "Total channels prunned so far: 197\n",
      "\n",
      "Iteration 198 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 386)]\n",
      "Input: 0.115 MB, Params: 3,800,476 (14.498 MB), Total: 14.61 MB, FLOPs: 357,110,728\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 198/1723 finished in 0m15s\n",
      "Total channels prunned so far: 198\n",
      "\n",
      "Iteration 199 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 423)]\n",
      "Input: 0.115 MB, Params: 3,794,354 (14.474 MB), Total: 14.59 MB, FLOPs: 356,945,461\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 199/1723 finished in 0m15s\n",
      "Total channels prunned so far: 199\n",
      "\n",
      "Iteration 200 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 10)]\n",
      "Input: 0.115 MB, Params: 3,788,232 (14.451 MB), Total: 14.57 MB, FLOPs: 356,780,194\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 200/1723 finished in 0m15s\n",
      "Total channels prunned so far: 200\n",
      "\n",
      "Iteration 201 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 178)]\n",
      "Input: 0.115 MB, Params: 3,784,981 (14.439 MB), Total: 14.55 MB, FLOPs: 356,429,194\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 201/1723 finished in 0m15s\n",
      "Total channels prunned so far: 201\n",
      "\n",
      "Iteration 202 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 365)]\n",
      "Input: 0.115 MB, Params: 3,780,926 (14.423 MB), Total: 14.54 MB, FLOPs: 356,319,789\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 202/1723 finished in 0m15s\n",
      "Total channels prunned so far: 202\n",
      "\n",
      "Iteration 203 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 346)]\n",
      "Input: 0.115 MB, Params: 3,776,871 (14.408 MB), Total: 14.52 MB, FLOPs: 356,210,384\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 203/1723 finished in 0m15s\n",
      "Total channels prunned so far: 203\n",
      "\n",
      "Iteration 204 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 70)]\n",
      "Input: 0.115 MB, Params: 3,773,638 (14.395 MB), Total: 14.51 MB, FLOPs: 355,494,656\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.915%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.339%\n",
      "Finished fine tuning.\n",
      "Iteration 204/1723 finished in 0m15s\n",
      "Total channels prunned so far: 204\n",
      "\n",
      "Iteration 205 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 302)]\n",
      "Input: 0.115 MB, Params: 3,769,583 (14.380 MB), Total: 14.50 MB, FLOPs: 355,385,251\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 205/1723 finished in 0m15s\n",
      "Total channels prunned so far: 205\n",
      "\n",
      "Iteration 206 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 41)]\n",
      "Input: 0.115 MB, Params: 3,763,488 (14.357 MB), Total: 14.47 MB, FLOPs: 355,220,713\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 206/1723 finished in 0m15s\n",
      "Total channels prunned so far: 206\n",
      "\n",
      "Iteration 207 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 281)]\n",
      "Input: 0.115 MB, Params: 3,759,442 (14.341 MB), Total: 14.46 MB, FLOPs: 355,111,551\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 207/1723 finished in 0m15s\n",
      "Total channels prunned so far: 207\n",
      "\n",
      "Iteration 208 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 13)]\n",
      "Input: 0.115 MB, Params: 3,753,356 (14.318 MB), Total: 14.43 MB, FLOPs: 354,947,256\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 208/1723 finished in 0m15s\n",
      "Total channels prunned so far: 208\n",
      "\n",
      "Iteration 209 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 435)]\n",
      "Input: 0.115 MB, Params: 3,749,319 (14.303 MB), Total: 14.42 MB, FLOPs: 354,838,337\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 209/1723 finished in 0m15s\n",
      "Total channels prunned so far: 209\n",
      "\n",
      "Iteration 210 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 257)]\n",
      "Input: 0.115 MB, Params: 3,745,282 (14.287 MB), Total: 14.40 MB, FLOPs: 354,729,418\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 210/1723 finished in 0m15s\n",
      "Total channels prunned so far: 210\n",
      "\n",
      "Iteration 211 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 184)]\n",
      "Input: 0.115 MB, Params: 3,741,245 (14.272 MB), Total: 14.39 MB, FLOPs: 354,620,499\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 211/1723 finished in 0m15s\n",
      "Total channels prunned so far: 211\n",
      "\n",
      "Iteration 212 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 13)]\n",
      "Input: 0.115 MB, Params: 3,735,069 (14.248 MB), Total: 14.36 MB, FLOPs: 354,280,083\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 212/1723 finished in 0m15s\n",
      "Total channels prunned so far: 212\n",
      "\n",
      "Iteration 213 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 315)]\n",
      "Input: 0.115 MB, Params: 3,731,032 (14.233 MB), Total: 14.35 MB, FLOPs: 354,171,164\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 213/1723 finished in 0m15s\n",
      "Total channels prunned so far: 213\n",
      "\n",
      "Iteration 214 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 148)]\n",
      "Input: 0.115 MB, Params: 3,724,856 (14.209 MB), Total: 14.32 MB, FLOPs: 353,830,748\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 214/1723 finished in 0m15s\n",
      "Total channels prunned so far: 214\n",
      "\n",
      "Iteration 215 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 31)]\n",
      "Input: 0.115 MB, Params: 3,718,680 (14.186 MB), Total: 14.30 MB, FLOPs: 353,490,332\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 215/1723 finished in 0m15s\n",
      "Total channels prunned so far: 215\n",
      "\n",
      "Iteration 216 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 349)]\n",
      "Input: 0.115 MB, Params: 3,712,657 (14.163 MB), Total: 14.28 MB, FLOPs: 353,327,738\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Finished fine tuning.\n",
      "Iteration 216/1723 finished in 0m15s\n",
      "Total channels prunned so far: 216\n",
      "\n",
      "Iteration 217 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 231)]\n",
      "Input: 0.115 MB, Params: 3,708,629 (14.147 MB), Total: 14.26 MB, FLOPs: 353,219,062\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.102%\n",
      "Finished fine tuning.\n",
      "Iteration 217/1723 finished in 0m15s\n",
      "Total channels prunned so far: 217\n",
      "\n",
      "Iteration 218 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 228)]\n",
      "Input: 0.115 MB, Params: 3,702,615 (14.124 MB), Total: 14.24 MB, FLOPs: 353,056,711\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 218/1723 finished in 0m15s\n",
      "Total channels prunned so far: 218\n",
      "\n",
      "Iteration 219 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 168)]\n",
      "Input: 0.115 MB, Params: 3,699,400 (14.112 MB), Total: 14.23 MB, FLOPs: 352,709,599\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 219/1723 finished in 0m15s\n",
      "Total channels prunned so far: 219\n",
      "\n",
      "Iteration 220 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 24)]\n",
      "Input: 0.115 MB, Params: 3,693,386 (14.089 MB), Total: 14.20 MB, FLOPs: 352,547,248\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Finished fine tuning.\n",
      "Iteration 220/1723 finished in 0m15s\n",
      "Total channels prunned so far: 220\n",
      "\n",
      "Iteration 221 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 121)]\n",
      "Input: 0.115 MB, Params: 3,690,171 (14.077 MB), Total: 14.19 MB, FLOPs: 352,200,136\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 221/1723 finished in 0m15s\n",
      "Total channels prunned so far: 221\n",
      "\n",
      "Iteration 222 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 369)]\n",
      "Input: 0.115 MB, Params: 3,686,161 (14.062 MB), Total: 14.18 MB, FLOPs: 352,091,946\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Finished fine tuning.\n",
      "Iteration 222/1723 finished in 0m15s\n",
      "Total channels prunned so far: 222\n",
      "\n",
      "Iteration 223 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 219)]\n",
      "Input: 0.115 MB, Params: 3,682,946 (14.049 MB), Total: 14.16 MB, FLOPs: 351,744,834\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.797%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 223/1723 finished in 0m15s\n",
      "Total channels prunned so far: 223\n",
      "\n",
      "Iteration 224 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 90)]\n",
      "Input: 0.115 MB, Params: 3,679,740 (14.037 MB), Total: 14.15 MB, FLOPs: 351,032,022\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.339%\n",
      "Finished fine tuning.\n",
      "Iteration 224/1723 finished in 0m15s\n",
      "Total channels prunned so far: 224\n",
      "\n",
      "Iteration 225 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 83)]\n",
      "Input: 0.115 MB, Params: 3,675,730 (14.022 MB), Total: 14.14 MB, FLOPs: 350,923,832\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Finished fine tuning.\n",
      "Iteration 225/1723 finished in 0m15s\n",
      "Total channels prunned so far: 225\n",
      "\n",
      "Iteration 226 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 128)]\n",
      "Input: 0.115 MB, Params: 3,669,734 (13.999 MB), Total: 14.11 MB, FLOPs: 350,761,967\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.915%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 226/1723 finished in 0m15s\n",
      "Total channels prunned so far: 226\n",
      "\n",
      "Iteration 227 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 380)]\n",
      "Input: 0.115 MB, Params: 3,663,738 (13.976 MB), Total: 14.09 MB, FLOPs: 350,600,102\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 227/1723 finished in 0m15s\n",
      "Total channels prunned so far: 227\n",
      "\n",
      "Iteration 228 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 7)]\n",
      "Input: 0.115 MB, Params: 3,657,742 (13.953 MB), Total: 14.07 MB, FLOPs: 350,438,237\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 228/1723 finished in 0m20s\n",
      "Total channels prunned so far: 228\n",
      "\n",
      "Iteration 229 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 52)]\n",
      "Input: 0.115 MB, Params: 3,653,759 (13.938 MB), Total: 14.05 MB, FLOPs: 350,330,776\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Finished fine tuning.\n",
      "Iteration 229/1723 finished in 0m19s\n",
      "Total channels prunned so far: 229\n",
      "\n",
      "Iteration 230 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 330)]\n",
      "Input: 0.115 MB, Params: 3,649,776 (13.923 MB), Total: 14.04 MB, FLOPs: 350,223,315\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 230/1723 finished in 0m20s\n",
      "Total channels prunned so far: 230\n",
      "\n",
      "Iteration 231 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 106)]\n",
      "Input: 0.115 MB, Params: 3,646,570 (13.911 MB), Total: 14.03 MB, FLOPs: 349,510,503\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.915%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Finished fine tuning.\n",
      "Iteration 231/1723 finished in 0m20s\n",
      "Total channels prunned so far: 231\n",
      "\n",
      "Iteration 232 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 158)]\n",
      "Input: 0.115 MB, Params: 3,642,587 (13.895 MB), Total: 14.01 MB, FLOPs: 349,403,042\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Finished fine tuning.\n",
      "Iteration 232/1723 finished in 0m20s\n",
      "Total channels prunned so far: 232\n",
      "\n",
      "Iteration 233 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 204)]\n",
      "Input: 0.115 MB, Params: 3,636,492 (13.872 MB), Total: 13.99 MB, FLOPs: 349,067,000\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 233/1723 finished in 0m20s\n",
      "Total channels prunned so far: 233\n",
      "\n",
      "Iteration 234 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 11)]\n",
      "Input: 0.115 MB, Params: 3,634,834 (13.866 MB), Total: 13.98 MB, FLOPs: 348,331,292\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.644%\n",
      "Finished fine tuning.\n",
      "Iteration 234/1723 finished in 0m20s\n",
      "Total channels prunned so far: 234\n",
      "\n",
      "Iteration 235 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 320)]\n",
      "Input: 0.115 MB, Params: 3,630,851 (13.851 MB), Total: 13.97 MB, FLOPs: 348,223,831\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.492%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.373%\n",
      "Finished fine tuning.\n",
      "Iteration 235/1723 finished in 0m15s\n",
      "Total channels prunned so far: 235\n",
      "\n",
      "Iteration 236 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 272)]\n",
      "Input: 0.115 MB, Params: 3,624,900 (13.828 MB), Total: 13.94 MB, FLOPs: 348,063,181\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Finished fine tuning.\n",
      "Iteration 236/1723 finished in 0m15s\n",
      "Total channels prunned so far: 236\n",
      "\n",
      "Iteration 237 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 288)]\n",
      "Input: 0.115 MB, Params: 3,618,949 (13.805 MB), Total: 13.92 MB, FLOPs: 347,902,531\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.068%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.949%\n",
      "Finished fine tuning.\n",
      "Iteration 237/1723 finished in 0m15s\n",
      "Total channels prunned so far: 237\n",
      "\n",
      "Iteration 238 of 1721 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 189)]\n",
      "Input: 0.115 MB, Params: 3,615,761 (13.793 MB), Total: 13.91 MB, FLOPs: 347,558,335\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.220%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbe351-bc11-4e98-89ce-6c0531393ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c660e0-e1c1-4f30-be4f-d8d322c93eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
