{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c23266-1a02-4456-ba2d-f77af2b52a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inherented from th/Torch_TLTrainer_for_Alarm_refine_structure_name.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cc9cb1-7673-4e90-b4c6-a32aba2e4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import numpy as np;\n",
    "import glob;\n",
    "import random;\n",
    "import time;\n",
    "import torch;\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;\n",
    "import json\n",
    "sys.path.append(os.getcwd());\n",
    "sys.path.append('../')\n",
    "sys.path.append(os.path.abspath('../../'));\n",
    "# sys.path.append(os.path.join(os.getcwd(), 'torch/resources'));\n",
    "import common.utils as U;\n",
    "import common.opts as opts;\n",
    "# import resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "import common.tlopts as tlopts\n",
    "# import resources.train_generator as train_generator;\n",
    "import argparse\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0815030-17d2-4406-b087-0409a9863de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SharedLibs.config_utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62f53e33-c3ee-4c51-a448-5ebfc6da731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducibility\n",
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d743ea-3384-413a-9f9e-a05dd4af34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_acdnet = '../th/resources/pretrained_models/acdnet_20khz_trained_model_fold4_91.00.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037be559-e258-4780-9416-dd969489d029",
   "metadata": {},
   "source": [
    "## define TLTraining Generator Class\n",
    "The Class is an python iterator class for generating data for trainer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9850c9d-6a7d-41ed-9244-a802f25b316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples=None, labels=None, options=None, classes_dict=None):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = classes_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            # print(f\"nClasses:{self.opt.nClasses}, type of mapdict:{type(self.mapdict)}, type of label1:{type(label1)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        print(f\"batchIndex is {batchIndex}, total sounds is {len(sounds)}\")\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1262f5-e327-4ba4-b8ea-f8570463bcaf",
   "metadata": {},
   "source": [
    "## ACDNetV2 define the acdnet model structure.\n",
    "定義原本的ACDNetV2，for載入pretrained acdnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5099b20-938b-4841-85d7-fa6ee30ce834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs = self.ch_config[-1];\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetACDNetModel(input_len=30225, nclass=50, sr=20000, channel_config=None):\n",
    "    net = ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47820f3a-2b2a-4eae-b56f-590433fef0ab",
   "metadata": {},
   "source": [
    "## load pretrained acdnet weights of 20khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbb28af-b51f-409c-ad38-ca8da8879874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdnet_model = GetACDNetModel()\n",
    "# pretrain_weight= torch.load('./resources/pretrained_models/acdnet_20khz_trained_model_fold4_91.00.pt', map_location=torch.device('cpu'))['weight']\n",
    "\n",
    "# model_state = acdnet_model.state_dict()\n",
    "# model_state.update(pretrain_weight)\n",
    "# acdnet_model.load_state_dict(pretrain_weight, strict=False)\n",
    "\n",
    "# for k, v in pretrain_weight['weight'].items():\n",
    "#     print(\"name:\", k)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# remove the unexpected keys: weight and config\n",
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint.items():\n",
    "#     name = k.replace(\"weight\", \"\") # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "#     name = k.replace(\"config\", \"\") # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "# model_state = acdnet_model.state_dict()\n",
    "# model_state.update(new_state_dict)\n",
    "# acdnet_model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# print(\"acdnet_model state_dict:\\n\",acdnet_model.state_dict())\n",
    "# print(\"pretrain_weight: \\n\",pretrain_weight)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc665597-d662-466d-99d7-b32fa9a5afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_38_of_tfeb = list(acdnet_model.tfeb.children())[38]\n",
    "\n",
    "# print(layer_38_of_tfeb)\n",
    "# print(nn.Sequential(*list(acdnet_model.tfeb.children())[:-6]))\n",
    "# print(nn.Sequential(*list(acdnet_model.tfeb.children())))\n",
    "# print(acdnet_model)\n",
    "# for item_v in nn.Sequential(*list(acdnet_model.tfeb.children())):\n",
    "#     for internal_k, internal_v in item_v.named_parameters():\n",
    "#         print(internal_v.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa8d3d7-8658-4b73-a643-e7d68ddeab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(acdnet_model.fc)\n",
    "#acdnet 包含三部份：sfeb, tfeb and output\n",
    "# print(nn.Sequential(*list(acdnet_model.children())))\n",
    "# print(nn.Sequential(*list(acdnet_model.children())[:-1]))\n",
    "# for k, v in acdnet_model.named_parameters():\n",
    "#     print(\"key:\", k)\n",
    "#     v.requires_grad = False\n",
    "\n",
    "# acdnet_model.fcn = nn.Linear(num_ftrs, 10)\n",
    "# print(acdnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e74198-72bd-48f2-90a9-8f4f29b9f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='ACDNet_TL_Model_Extend',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args();\n",
    "    \n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 64;\n",
    "    opt.LR = 0.1;\n",
    "    opt.weightDecay = 5e-2#9e-3;#5e-3;#5e-2;#1e-2;#5e-4;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 1000;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "    if torch.backends.mps.is_available():\n",
    "        opt.device=\"mps\"; #for apple m2 gpu\n",
    "    elif torch.cuda.is_available():\n",
    "        opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    else:\n",
    "        opt.device=\"cpu\"\n",
    "    print(f\"***Use device:{opt.device}\");\n",
    "    # opt.device = torch.device(\"cuda:0\" if  else \"cpu\");\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.TLAcdnetConfig = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt\n",
    "    # opt = parser.parse_args();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82574608-e5c1-47ea-817f-c1bfa8ffba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771ea414-dd74-4897-9f3d-f7566ea543fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_confing_10 = 8 * 64\n",
    "ch_n_class = 2\n",
    "fcn_no_of_inputs = 2\n",
    "# conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "conv12, bn12 = make_layers(in_channels = ch_confing_10, out_channels = ch_n_class, kernel_size = (1, 1));\n",
    "fcn = nn.Linear(fcn_no_of_inputs, ch_n_class);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec5a27ea-d330-4d04-b996-b38dde0b6a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[8,64,32,64,64,128,128,256,256,512,512, 50]\\n[1, 2, 3, 4, 5, 6,  7,  8,  9   10, 11, 12]\\n[8,64,32,64,64,128,128,256,256,512,2,2]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[8,64,32,64,64,128,128,256,256,512,512, 50]\n",
    "[1, 2, 3, 4, 5, 6,  7,  8,  9   10, 11, 12]\n",
    "[8,64,32,64,64,128,128,256,256,512,2,2]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f157eff-03da-4e48-9701-606b472663e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDNet_TL_Model_Extend(nn.Module):\n",
    "    def __init__(self, PretrainedWeights='./resources/pretrained_models/acdnet_20khz_trained_model_fold4_91.00.pt',opt=None):\n",
    "        super(ACDNet_TL_Model_Extend, self).__init__()\n",
    "        acdnet_model = GetACDNetModel(); # load original acdnet model first\n",
    "        # device = opt#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.opt = opt;\n",
    "        self.ch_config = None;\n",
    "        print(f\"device is {self.opt.device}\")\n",
    "        pretrain_weight= torch.load(PretrainedWeights, map_location=torch.device(self.opt.device))['weight']\n",
    "        model_state = acdnet_model.state_dict()\n",
    "        model_state.update(pretrain_weight)\n",
    "        acdnet_model.load_state_dict(pretrain_weight, strict=False)\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = opt.TLAcdnetConfig;\n",
    "            #[channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # print(type(acdnet_model))\n",
    "        # count = 0;\n",
    "        for k, v in acdnet_model.named_parameters():\n",
    "            # count += 1;\n",
    "            # print(f\"set {k} required_grade to False\");\n",
    "            v.requires_grad = False\n",
    "        # print(f\"count is {count}\");\n",
    "        self.sfeb = nn.Sequential(*list(acdnet_model.children())[0])\n",
    "        tfeb_modules = []\n",
    "        tfeb_modules.extend([*list(acdnet_model.tfeb.children())[:-6]])\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        tfeb_modules.append(nn.AvgPool2d(kernel_size = (2,4)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "        # self.retrained_layers = nn.Sequential(*list(acdnet_model.tfeb.children())[:-1])\n",
    "        # fcn_no_of_inputs = 50, n_class=10\n",
    "        # n_class=6\n",
    "        # fc = nn.Linear(50, n_class);\n",
    "        # fc.requires_grad = True\n",
    "        # tfeb_modules.extend([fc])\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules)\n",
    "        self.output = nn.Sequential(\n",
    "        nn.Softmax(dim=1));\n",
    "        # print(f\"type of self.tfeb is {type(self.tfeb)}\")\n",
    "        # for k2, v2 in self.tfeb:\n",
    "        #     print(f\"k:{k}'s requires_grad is {v2.requires_grad}\");\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"sfeb:\\n{list(self.sfeb.children())}\");\n",
    "        print(f\"input x shape:{x.size()}\")\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d89596f-04bf-4222-b23f-31ed75cd1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTLACDNet(opt=None):\n",
    "    model = ACDNet_TL_Model_Extend(PretrainedWeights=pretrained_acdnet, opt=opt);#ACDNet_TL_Model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efbc0b0a-28fb-440f-97e3-c59ecb9fa47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = GetTLACDNet()\n",
    "# calc.summary(test_model, (1,1,30225))\n",
    "# print(test_model)\n",
    "# print(test_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e62e010-cd9a-42cb-ba31-66bb71ae1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3ca4f4-2d45-488d-ba0f-3466bd397cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c56d94-f01d-4b0b-8c90-85f6e29421f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n報錯訊息：\\nTypeError: can't convert np.ndarray of type numpy.object_. \\nThe only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\\n1\\n2\\n問題描述：\\n當把np轉換成torch tensor時，\\n\\ntrainx = torch.from_numpy(np.reshape(train_x, newshape=(-1,25)))\\n1\\n解決方法：\\n由於讀入的numpy陣列裡的元素是object類型，無法將此型別轉換成tensor。\\n\\n所以，將numpy數組進行強制型別轉換成float型別（或任何pytorch支援的型別：float64, float32, float16, int64, int32, int16, int8, uint8, and bool）即可。\\n\\ntrainx = trainx.astype(float)  # numpy強制轉型\\n————————————————\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "報錯訊息：\n",
    "TypeError: can't convert np.ndarray of type numpy.object_. \n",
    "The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n",
    "1\n",
    "2\n",
    "問題描述：\n",
    "當把np轉換成torch tensor時，\n",
    "\n",
    "trainx = torch.from_numpy(np.reshape(train_x, newshape=(-1,25)))\n",
    "1\n",
    "解決方法：\n",
    "由於讀入的numpy陣列裡的元素是object類型，無法將此型別轉換成tensor。\n",
    "\n",
    "所以，將numpy數組進行強制型別轉換成float型別（或任何pytorch支援的型別：float64, float32, float16, int64, int32, int16, int8, uint8, and bool）即可。\n",
    "\n",
    "trainx = trainx.astype(float)  # numpy強制轉型\n",
    "————————————————\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d36f12e-f8a5-41de-bc65-95bb84e23a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n解決RuntimeError: Input type and weight type should be the same\\n\\n\\n根據報錯資訊的意思可以推斷，這個錯誤是由輸入和權重的資料類型不一致引起的。因此解決方法很簡單，就是將輸入的資料和模型參數的資料類型統一即可。在這個例子中，有以下幾個解決方法。\\n\\n1.將輸入資料（torch.tensor 形式）轉換成FloatTensor形式，如下：\\n\\n# net_in是torch.tensor形式的输入数据\\nnet_in = net_in.float();\\n1\\n2\\n2.如果輸入資料在轉變為torch.tensor前是以numpy數組的形式儲存的，我們可以將資料提前轉變為float32形式，具體如下：\\n\\n# train_set是numpy.array形式的输入数据\\nimport numpy as np\\nX = train_set.astype(np.float32);\\n1\\n2\\n3\\n3.將模型參數類型轉換為與輸入張量（tensor）一致的型別。在這個例子裡，模型參數需轉換為DoubleTensor，如下所示：\\n\\nmodel.double()\\n1\\n可選擇以上任一方法解決這個問題。但在實際應用上需要注意，第三種解決方法會增加顯存的需求量。更多關於torch中張量（tensor）資料類型的介紹，可參考這個網頁Link。\\n————————————————\\n版权声明：本文为CSDN博主「Henry积少成多」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/qq_34612816/article/details/123372456\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "解決RuntimeError: Input type and weight type should be the same\n",
    "\n",
    "\n",
    "根據報錯資訊的意思可以推斷，這個錯誤是由輸入和權重的資料類型不一致引起的。因此解決方法很簡單，就是將輸入的資料和模型參數的資料類型統一即可。在這個例子中，有以下幾個解決方法。\n",
    "\n",
    "1.將輸入資料（torch.tensor 形式）轉換成FloatTensor形式，如下：\n",
    "\n",
    "# net_in是torch.tensor形式的输入数据\n",
    "net_in = net_in.float();\n",
    "1\n",
    "2\n",
    "2.如果輸入資料在轉變為torch.tensor前是以numpy數組的形式儲存的，我們可以將資料提前轉變為float32形式，具體如下：\n",
    "\n",
    "# train_set是numpy.array形式的输入数据\n",
    "import numpy as np\n",
    "X = train_set.astype(np.float32);\n",
    "1\n",
    "2\n",
    "3\n",
    "3.將模型參數類型轉換為與輸入張量（tensor）一致的型別。在這個例子裡，模型參數需轉換為DoubleTensor，如下所示：\n",
    "\n",
    "model.double()\n",
    "1\n",
    "可選擇以上任一方法解決這個問題。但在實際應用上需要注意，第三種解決方法會增加顯存的需求量。更多關於torch中張量（tensor）資料類型的介紹，可參考這個網頁Link。\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Henry积少成多」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/qq_34612816/article/details/123372456\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "463a7a09-5a89-4097-b4d2-1f8e50f8b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLTrainer:\n",
    "    def __init__(self, opt=None, classes_dict=None):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "        self.trainGen = getTrainGen(opt,classes_dict=classes_dict)#train_generator.setup(opt, split);\n",
    "        # self.opt.trainer = self;\n",
    "        # self.trainGen = getTrainGen(self.opt, self.opt.splits)#train_generator.setup(self.opt, self.opt.split);\n",
    "        # self.pretrainedmodelpath = \"./resources/pretrained_models/acdnet20_20khz_fold4.h5\"\n",
    "\n",
    "    def Train(self):\n",
    "        train_start_time = time.time();\n",
    "        net = GetTLACDNet(self.opt).to(self.opt.device)#models.GetACDNetModel().to(self.opt.device);\n",
    "        #print networks parameters' require_grade value\n",
    "        for k_, v_ in net.named_parameters():\n",
    "            print(f\"{k_}:{v_.requires_grad}\")\n",
    "        print('ACDNet model has been prepared for training');\n",
    "\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "\n",
    "        # training_text = \"Re-Training\" if self.opt.retrain else \"Training from Scratch\";\n",
    "        # print(\"{} has been started. You will see update after finishing every training epoch and validation\".format(training_text));\n",
    "\n",
    "        lossFunc = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True);\n",
    "\n",
    "        # self.opt.nEpochs = 1957 if self.opt.split == 4 else 2000;\n",
    "        for epochIdx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epochIdx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            for batchIdx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batchIdx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "                y = torch.tensor(y).to(self.opt.device);\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(x);\n",
    "                running_acc += (((outputs.data.argmax(dim=1) == y.argmax(dim=1))*1).float().mean()).item();\n",
    "                loss = lossFunc(outputs.log(), y);\n",
    "                loss.backward();\n",
    "                optimizer.step();\n",
    "\n",
    "                running_loss += loss.item();\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, lossFunc);\n",
    "            #Save best model\n",
    "            self.__save_model(val_acc, epochIdx, net);\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epochIdx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - train_start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "    def load_test_data(self):\n",
    "        # data = np.load(os.path.join(self.opt.data, self.opt.dataset, 'test_data_{}khz/fold{}_test4000.npz'.format(self.opt.sr//1000, self.opt.split)), allow_pickle=True);\n",
    "        data = np.load(self.opt.testData, allow_pickle=True);\n",
    "        print(f\"device is :{self.opt.device}\")\n",
    "        print(f\"len of Y:{len(data['y'])}\")\n",
    "        # self.testX = torch.tensor(np.moveaxis(data['x'], 3, 1)).to(self.opt.device);\n",
    "        dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "        self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "        self.testY = torch.tensor(data['y']).type(torch.float32).to(self.opt.device);\n",
    "\n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __get_batch(self, index):\n",
    "        x = self.trainX[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        y = self.trainY[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        return x.to(self.opt.device), y.to(self.opt.device);\n",
    "\n",
    "    def __validate(self, net, lossFunc):\n",
    "        if self.testX is None:\n",
    "            self.load_test_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);#(self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "#             for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "#             for idx in range(len(self.testX)):\n",
    "#             x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "            x = self.testX[:];\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "#         with torch.no_grad():\n",
    "#             y_pred = None;\n",
    "#             batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "#             for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "#                 x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "#                 scores = net(x);\n",
    "#                 y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "#             acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "        net.train();\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        print(f\"shape of y_pred:{y_pred.shape}\");\n",
    "        print(f\"shape of y_target:{y_target.shape}\");\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            if self.opt.nCrops == 1:\n",
    "                y_pred = y_pred.argmax(dim=1);\n",
    "                y_target = y_target.argmax(dim=1);\n",
    "            else:\n",
    "                y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                print(f\"after: len of y_pred:{len(y_pred)}, len of y_target:{len(y_target)}\")\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def __on_epoch_end(self, start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        line = 'SP-{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            self.opt.splits, epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def __save_model(self, acc, epochIdx, net):\n",
    "        print(\"__save_model is called\")\n",
    "        print(f\"current best Acc is {self.bestAcc}\")\n",
    "        print(f\"pass in acc is {acc}\")\n",
    "        if acc > self.bestAcc:\n",
    "            dir = os.getcwd();\n",
    "            save_path = \"./trained_models/{}\".format(self.opt.model_name.format(genDataTimeStr(),acc,epochIdx));\n",
    "            # fname = \"{}/torch/trained_models/{}_fold{}.pt\";\n",
    "            # fname = \"{}/trained_models/acdnet_torch_20231218.pt\";\n",
    "            # old_model = fname.format(dir, self.opt.model_name.lower(), self.opt.splits);\n",
    "            # if os.path.isfile(old_model):\n",
    "            #     os.remove(old_model);\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            # torch.save({'weight':net.state_dict(), 'config':net.ch_config}, fname.format(dir, self.opt.model_name.lower(), self.opt.split));\n",
    "            # torch.save({'weight':net.state_dict()}, fname.format(dir, self.opt.model_name.lower(), self.opt.splits));\n",
    "            torch.save({'weight':net.state_dict(), 'config':net.ch_config}, save_path);\n",
    "            print(f\"model saved....., acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb03c91-9d87-44bf-86d5-793e2df7fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, fold=None, classes_dict=None):\n",
    "    dataset = np.load(opt.trainData, allow_pickle=True);\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(fold)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(fold)].item()['labels']\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt, classes_dict=classes_dict);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5215371b-d1a4-4980-a7a3-0cdce4c37515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_training_data = \"../../../acdnet_training_data/for_alarm_help_two_classes/train/multi_folds_train_20240318160830.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916fd66a-d86f-4481-b740-473daf4b5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = np.load(cv_training_data, allow_pickle=True);\n",
    "# train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "# train_labels = dataset['fold{}'.format(1)].item()['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76169924-a64c-4e44-9e5f-9fe638cd6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"len(train_sounds):{len(train_sounds)}, len(train_labels):{len(train_labels)}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fc9169-9c28-4b0b-8120-9de854f1a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaea1e6d-a35e-4f5b-9222-39f87e82069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Settings={\n",
    "    \"last_checkpoint\":\"../trained_models/\",\n",
    "    \"last_best_acc\":90,\n",
    "    \"last_acc_epochs\":100,\n",
    "    \"last_acc_lr\":0.005,\n",
    "    \"last_acc_weight_decay\":[0.3,0.6,0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e748bbe-3e6b-4bef-9457-c995cac69a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "    opt = opts.parse();\n",
    "    opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    opt.trainer = None\n",
    "    opt.trainData=\"../../../acdnet_training_data/for_alarm_help_two_classes/train/multi_folds_train_20240318160830.npz\";\n",
    "    opt.testData=\"../../datasets/CurrentUse/generated_datasets/validation/fold1_val_20240318165621.npz\";\n",
    "    opt.config = \"./cfg/config.json\";\n",
    "    cfg_settings = readAssumeConfig(opt.config)\n",
    "    opt.checkpoint = \n",
    "    valid_path = False;\n",
    "    while not valid_path:\n",
    "        model_path = input(\"Enter your pruned model path OR keep it blank to train the base ACDNet model\\n:\");\n",
    "        opt.model_path = \"ACDNet\" if model_path == '' else model_path;\n",
    "        if model_path == '':\n",
    "            opt.model_path = \"ACDNet\";\n",
    "            print('ACDNet base model will be trained.');\n",
    "            valid_path = True;\n",
    "\n",
    "        else:\n",
    "            file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\n",
    "            if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "                state = torch.load(file_paths[0], map_location=opt.device);\n",
    "                opt.model_path = file_paths[0];\n",
    "                print('Model has been found at: {}'.format(opt.model_path));\n",
    "                valid_path = True;\n",
    "\n",
    "    valid_model_name = False;\n",
    "    while not valid_model_name:\n",
    "        model_name = input('Enter a name that will be used to save the trained model: ');\n",
    "        if model_name != '':\n",
    "            opt.model_name = model_name;\n",
    "            valid_model_name = True;\n",
    "\n",
    "    valid_fold = False;\n",
    "    split = None;\n",
    "    while not valid_fold:\n",
    "        fold = input(\"Which fold do you want your model to be Validated:\\n 0. 5-Fold Cross Validation\\n 1. Fold-1\\n 2. Fold-2\\n 3. Fold-3\\n 4. Fold-4\\n 5. Fold-5\\n :\")\n",
    "        if fold in ['0','1','2','3','4','5']:\n",
    "            split = int(fold);\n",
    "            valid_fold = True;\n",
    "\n",
    "    if split == 0:\n",
    "        # -- Run for all splits\n",
    "        for split in opt.splits:\n",
    "            opt.split = split;\n",
    "            Train(opt);\n",
    "    else:\n",
    "        opt.split = split;\n",
    "        Train(opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cdef4-593f-490d-a98f-79a3eca5f5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c24dcd-bb04-433f-9294-b95709fa8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdnet original torch Train\n",
    "# def Train(opt):\n",
    "#     print('Starting {} model Training for Fold-{}'.format(opt.model_name.upper(), opt.split));\n",
    "#     opts.display_info(opt);\n",
    "#     trainer = Trainer(opt);\n",
    "#     trainer.Train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2213cc7a-28bd-42e0-ab67-cf8969c5a718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \\'__main__\\':\\n    opt = opts.parse();\\n    opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\\n    valid_training_type = False;\\n    while not valid_training_type:\\n        train_type = input(\\'Enter an option: \\n1. Re-Training\\n2. Training from Scratch\\n:\\');\\n        if train_type in [\\'1\\', \\'2\\']:\\n            opt.retrain = True if train_type == \\'1\\' else False;\\n            valid_training_type = True;\\n\\n    valid_path = False;\\n    while not valid_path:\\n        model_path = input(\"Enter your pruned model path OR keep it blank to train the base ACDNet model\\n:\");\\n        opt.model_path = \"ACDNet\" if model_path == \\'\\' else model_path;\\n        if model_path == \\'\\':\\n            opt.model_path = \"ACDNet\";\\n            print(\\'ACDNet base model will be trained.\\');\\n            valid_path = True;\\n\\n        else:\\n            file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\\n            if len(file_paths)>0 and os.path.isfile(file_paths[0]):\\n                state = torch.load(file_paths[0], map_location=opt.device);\\n                opt.model_path = file_paths[0];\\n                print(\\'Model has been found at: {}\\'.format(opt.model_path));\\n                valid_path = True;\\n\\n    valid_model_name = False;\\n    while not valid_model_name:\\n        model_name = input(\\'Enter a name that will be used to save the trained model: \\');\\n        if model_name != \\'\\':\\n            opt.model_name = model_name;\\n            valid_model_name = True;\\n\\n    valid_fold = False;\\n    split = None;\\n    while not valid_fold:\\n        fold = input(\"Which fold do you want your model to be Validated:\\n 0. 5-Fold Cross Validation\\n 1. Fold-1\\n 2. Fold-2\\n 3. Fold-3\\n 4. Fold-4\\n 5. Fold-5\\n :\")\\n        if fold in [\\'0\\',\\'1\\',\\'2\\',\\'3\\',\\'4\\',\\'5\\']:\\n            split = int(fold);\\n            valid_fold = True;\\n\\n    if split == 0:\\n        # -- Run for all splits\\n        for split in opt.splits:\\n            opt.split = split;\\n            Train(opt);\\n    else:\\n        opt.split = split;\\n        Train(opt);\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    opt = opts.parse();\n",
    "    opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "    valid_training_type = False;\n",
    "    while not valid_training_type:\n",
    "        train_type = input('Enter an option: \\n1. Re-Training\\n2. Training from Scratch\\n:');\n",
    "        if train_type in ['1', '2']:\n",
    "            opt.retrain = True if train_type == '1' else False;\n",
    "            valid_training_type = True;\n",
    "\n",
    "    valid_path = False;\n",
    "    while not valid_path:\n",
    "        model_path = input(\"Enter your pruned model path OR keep it blank to train the base ACDNet model\\n:\");\n",
    "        opt.model_path = \"ACDNet\" if model_path == '' else model_path;\n",
    "        if model_path == '':\n",
    "            opt.model_path = \"ACDNet\";\n",
    "            print('ACDNet base model will be trained.');\n",
    "            valid_path = True;\n",
    "\n",
    "        else:\n",
    "            file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\n",
    "            if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "                state = torch.load(file_paths[0], map_location=opt.device);\n",
    "                opt.model_path = file_paths[0];\n",
    "                print('Model has been found at: {}'.format(opt.model_path));\n",
    "                valid_path = True;\n",
    "\n",
    "    valid_model_name = False;\n",
    "    while not valid_model_name:\n",
    "        model_name = input('Enter a name that will be used to save the trained model: ');\n",
    "        if model_name != '':\n",
    "            opt.model_name = model_name;\n",
    "            valid_model_name = True;\n",
    "\n",
    "    valid_fold = False;\n",
    "    split = None;\n",
    "    while not valid_fold:\n",
    "        fold = input(\"Which fold do you want your model to be Validated:\\n 0. 5-Fold Cross Validation\\n 1. Fold-1\\n 2. Fold-2\\n 3. Fold-3\\n 4. Fold-4\\n 5. Fold-5\\n :\")\n",
    "        if fold in ['0','1','2','3','4','5']:\n",
    "            split = int(fold);\n",
    "            valid_fold = True;\n",
    "\n",
    "    if split == 0:\n",
    "        # -- Run for all splits\n",
    "        for split in opt.splits:\n",
    "            opt.split = split;\n",
    "            Train(opt);\n",
    "    else:\n",
    "        opt.split = split;\n",
    "        Train(opt);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b75e3-fc9c-4480-9a5f-c0d4b4ecd77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c404f54-3575-41ef-b92e-9c5743e3ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"pt_name\":\"\",\n",
    "    \"accuracy\":90,\n",
    "    \"epochs\":100,\n",
    "    \"lr\":0.005,\n",
    "    \"weight_decay\":[0.3,0.6,0.9]\n",
    "}\n",
    "json_file = \"./test/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef58fdff-c158-4534-b71e-d8d80a622582",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAssumeConfig(cfg_file=json_file, settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5354503f-e6e1-4c6b-ba6b-79421bc725c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pt_name': '', 'accuracy': 90, 'epochs': 100, 'lr': 0.005, 'weight_decay': [0.3, 0.6, 0.9]}\n"
     ]
    }
   ],
   "source": [
    "read_ret = readAssumeConfig(cfg_file=json_file)\n",
    "print(read_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fa572c1-095e-4586-b2aa-8e64aa31cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"pt_name\":\"here\",\n",
    "    \"accuracy\":95.5,\n",
    "    \"epochs\":100,\n",
    "    \"lr\":0.0005,\n",
    "    \"weight_decay\":[0.3,0.6,0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feee0192-08f6-42f0-ad6c-1c722e60ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAssumeConfig(cfg_file=json_file, settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "577ffba1-0478-4d0a-9a6f-1ec8d3573b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pt_name': 'here', 'accuracy': 95.5, 'epochs': 100, 'lr': 0.0005, 'weight_decay': [0.3, 0.6, 0.9]}\n"
     ]
    }
   ],
   "source": [
    "read_ret = readAssumeConfig(cfg_file=json_file)\n",
    "print(read_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd0a02-5ede-47ab-873a-bae1f69f291c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
