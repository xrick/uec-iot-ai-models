{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369d21d4-2c2f-4ebf-b987-d28aef031497",
   "metadata": {},
   "source": [
    "## QAT過程問題集\n",
    "- can't convert float NaN (actually 0.00000) to int:\n",
    "  - 與weight-decay設定值可能有關，設定太大倒導致錯誤。\n",
    "  - 可能是同時開啟三個訓練程式造成記憶體不足造成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d582be4b-1b35-4dfc-a20d-b68077cb88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import random;\n",
    "import torch;\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d3cae9-1208-4e00-bbcf-b7b5c2f91304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737226ae-820b-4739-ab00-755bfe323ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b701027a-c7e0-4908-8d3b-6c8cdf91746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.opts as opts;\n",
    "import common.utils as U;\n",
    "# import th.resources.models as models;\n",
    "import th.resources.no_softmax_quant_model as models;\n",
    "import th.resources.calculator as calc;\n",
    "from SharedLibs.datestring import getDateStr, genDataTimeStr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55848bf9-571e-4995-bba6-c85fd888664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d9d813-5a9d-4595-a25b-b0fcef8f351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from tinynn.converter import TFLiteConverter\n",
    "from tinynn.graph.quantization.quantizer import PostQuantizer\n",
    "from tinynn.graph.tracer import model_tracer\n",
    "from tinynn.util.train_util import DLContext, get_device\n",
    "from tinynn.graph.quantization.algorithm.cross_layer_equalization import cross_layer_equalize\n",
    "from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cce7e38-1de4-4782-9a3e-6a1fc3cad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref site:https://discuss.pytorch.org/t/how-to-generate-a-fully-quantized-model/175185\n",
    "# from torch.ao.quantization.backend_config import BackendConfig, BackendPatternConfig, DTypeConfig, ObservationType\n",
    "# from torch.quantization import quantize_fx\n",
    "# from torch.ao.quantization import QConfigMapping\n",
    "# from torch.ao.quantization.fx.custom_config import PrepareCustomConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa078a1-193d-419e-a154-eb69dd598cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeb4481-8d39-4ed5-b432-d04802e4ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask8 = 0x4000 # >> 8 : 16384\n",
    "mask7 = 0x2000 # >> 7 :  8192\n",
    "mask6 = 0x1000 # >> 6 :  4096\n",
    "mask5 = 0x0800 # >> 5 :  2048\n",
    "mask4 = 0x0400 # >> 4 :  1024\n",
    "mask3 = 0x0200 # >> 3 :   512\n",
    "mask2 = 0x0100 # >> 2 :   256\n",
    "mask1 = 0x0080 # >> 1 :   128\n",
    "mask0 = 0x0040 # >> 0 :    64 below the value, drop the value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2252d3f-c228-4753-80a9-da7226e66380",
   "metadata": {},
   "source": [
    "## 將訓練資料轉換成int8格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70785c76-3485-48fd-b052-f33ea13876c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskOP(x):\n",
    "    x = np.int16(x)\n",
    "    # print(f\"begin:x:{x}\")\n",
    "    if (mask8&x):\n",
    "        return x >> 8\n",
    "    elif (mask7&x):\n",
    "        return x >> 7\n",
    "    elif (mask6&x):\n",
    "        return x >> 6\n",
    "    elif (mask5&x):\n",
    "        return x >> 5\n",
    "    elif (mask4&x):\n",
    "        return x >> 4\n",
    "    elif (mask3&x):\n",
    "        return x >> 3\n",
    "    elif (mask2&x):\n",
    "        return x >> 2\n",
    "    elif (mask1&x):\n",
    "        return x >> 1\n",
    "    elif (mask0&x):\n",
    "        return x\n",
    "    else:\n",
    "        return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d015bc36-f536-4736-8d15-d86bfddc9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8(x, axis):\n",
    "    len_of_x = len(x[0][0][0])\n",
    "    print(f\"len_of_x:{len_of_x}\")\n",
    "    for i in range(len_of_x):\n",
    "        nflag = 2; #positive\n",
    "        print(\"{}:{}\".format(i,x[0][0][0][i]))\n",
    "        tmp_x = x[0][0][0][i]\n",
    "        if tmp_x < 0:\n",
    "            tmp_x = np.abs(tmp_x)\n",
    "            nflag = 1\n",
    "        tmp_x = maskOP(tmp_x)\n",
    "        if(nflag==1):\n",
    "            tmp_x = -1 * (tmp_x)\n",
    "        print(\"{}:{}\".format(i,x[0][0][0][i]))\n",
    "        print(\"*********************************\")\n",
    "        x[0][0][0][i] = tmp_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f24261-75d3-4535-a6ba-205eb485c507",
   "metadata": {},
   "source": [
    "## 訓練程式產生程式-Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85b1c0f-4596-4d69-bc5d-94af3243dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([('52',1),('56',2),('99',3)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess_setup_without_normalization(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength)\n",
    "                  ]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82f89d02-367f-4058-addf-3b9befa7988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    dataset = np.load(\"../../../datasets/CurrentUse/generated_datasets/train/version4/single_fold_train_20240502114607.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de21f9-4fcd-4f11-8110-a78aedfeea20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6ab99b-1937-49d1-a1e9-7cf45adfa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 128;\n",
    "    opt.weightDecay = 5e-3;\n",
    "    opt.LR = 0.1;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 800;#2000;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 3#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    # opt.ch_config = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207352a5-4f18-4809-9215-7eb4fbd16c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_info(opt):\n",
    "    print('+------------------------------+');\n",
    "    print('| {} Sound classification'.format(opt.netType));\n",
    "    print('+------------------------------+');\n",
    "    print('| dataset  : {}'.format(opt.dataset));\n",
    "    print('| nEpochs  : {}'.format(opt.nEpochs));\n",
    "    print('| LRInit   : {}'.format(opt.LR));\n",
    "    print('| weightDecay   : {}'.format(opt.weightDecay));\n",
    "    print('| momentum   : {}'.format(opt.momentum));\n",
    "    print('| schedule : {}'.format(opt.schedule));\n",
    "    print('| warmup   : {}'.format(opt.warmup));\n",
    "    print('| batchSize: {}'.format(opt.batchSize));\n",
    "    print('| nFolds: {}'.format(opt.nFolds));\n",
    "    print('| Splits: {}'.format(opt.splits));\n",
    "    print('| Device: {}'.format(opt.device));\n",
    "    print('| Model Path: {}'.format(opt.model_path));\n",
    "    print('| Model Name: {}'.format(opt.model_name));\n",
    "    print('+------------------------------+');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce696bd8-42c8-4408-9d39-8fe42665011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATTrainer:\n",
    "    def __init__(self, opt=None, split=0):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        # self.opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.opt.device = torch.device(\"cpu\")\n",
    "        self.trainGen = getTrainGen(self.opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.qunt_nClass = opt.nClasses;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "\n",
    "    def load_train_data(self):\n",
    "        print('Preparing calibration dataset..');\n",
    "        x,y = self.trainGen.__getitem__(0);\n",
    "        self.trainX = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "        \"\"\"\n",
    "        trainX size:torch.Size([1, 1, 30225]), but must be [1,1,1,30225]\n",
    "        Due to the reason: raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        \"\"\"\n",
    "        # print(f\"trainX[0] shape:{self.trainX[0].shape}\")\n",
    "        self.trainY = torch.tensor(y).to(self.opt.device);\n",
    "        print('Calibration dataset is ready');\n",
    "        # self.opt.batchSize = 32;\n",
    "\n",
    "    # def load_test_data(self):\n",
    "    #     if(self.testX is None):\n",
    "    #         data = np.load('../../datasets/CurrentUse/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz', allow_pickle=True);\n",
    "    #         dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "    #         self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "    #         self.testY = torch.tensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def load_test_data(self):\n",
    "        testData = '../../../datasets/CurrentUse/generated_datasets/val/version4/final_single_val_20240502120516.npz'\n",
    "        data = np.load(testData, allow_pickle=True);\n",
    "        print(f\"device is :{self.opt.device}\")\n",
    "        print(f\"len of Y:{len(data['y'])}\")\n",
    "        # self.testX = torch.tensor(np.moveaxis(data['x'], 3, 1)).to(self.opt.device);\n",
    "        dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "        self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "        self.testY = torch.tensor(data['y']).type(torch.float32).to(self.opt.device);\n",
    "\n",
    "    def __validate_test(self, net, qat_done, testX, testY):\n",
    "        net.eval();\n",
    "        # if qat_done:\n",
    "        #     testX.to('cpu');\n",
    "        #     testY.to('cpu');\n",
    "        # else:\n",
    "        #     testX.to('cuda:0');\n",
    "        #     testY.to('cuda:0');\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);\n",
    "            x = self.testX[:];\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc = self.__compute_accuracy_2(y_pred, self.testY);\n",
    "        return acc;\n",
    "\n",
    "    \n",
    "    def __validate(self, net, lossFunc):\n",
    "        if self.testX is None:\n",
    "            self.load_test_data();\n",
    "        net.eval();\n",
    "        acc=0.0; \n",
    "        loss = 0.0;\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);#(self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            x = self.testX[:];\n",
    "            try:\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "                acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "            except ValueError:\n",
    "                print(f\"error data:{x}\")\n",
    "        net.train();\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy_2(self, y_pred, y_target):\n",
    "        print(y_pred.shape);\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1);\n",
    "\n",
    "            y_pred = y_pred.argmax(dim=1);\n",
    "            y_target = y_target.argmax(dim=1);\n",
    "\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "        return acc;\n",
    "        \n",
    "\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        print(f\"shape of y_pred:{y_pred.shape}\");\n",
    "        print(f\"shape of y_target:{y_target.shape}\");\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            if self.opt.nCrops == 1:\n",
    "                y_pred = y_pred.argmax(dim=1);\n",
    "                y_target = y_target.argmax(dim=1);\n",
    "            else:\n",
    "                y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                print(f\"after: len of y_pred:{len(y_pred)}, len of y_target:{len(y_target)}\")\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "        \n",
    "\n",
    "    def __load_model(self, quant=True):\n",
    "        state = torch.load(self.opt.model_path, map_location=self.opt.device);\n",
    "        print(state['config']);\n",
    "        net = None;\n",
    "        net = models.GetACDNetQuantModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        net.load_state_dict(state['weight']);\n",
    "        return net;\n",
    "\n",
    "    \n",
    "    def __train(self, net):\n",
    "        self.load_train_data();\n",
    "        # net.eval();\n",
    "        # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        lossFunc = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True);\n",
    "        train_start_time = time.time();\n",
    "        for epochIdx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epochIdx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            for batchIdx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batchIdx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "                y = torch.tensor(y).to(self.opt.device);\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                try:\n",
    "                    outputs = torch.softmax(input=net(x),dim=1); #need to check float NaN value?\n",
    "                    running_acc += (((outputs.data.argmax(dim=1) == y.argmax(dim=1))*1).float().mean()).item();\n",
    "                    loss = lossFunc(outputs.log(), y);\n",
    "                    loss.backward();\n",
    "                    optimizer.step();\n",
    "                    running_loss += loss.item();\n",
    "                except ValueError:\n",
    "                    print(f\"error label:{y}\")\n",
    "                    print(f\"error data:{x}\")\n",
    "                    continue\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, lossFunc);\n",
    "            #Save best model\n",
    "            self.__chk_bestAcc(val_acc, epochIdx, net);\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epochIdx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - train_start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "\n",
    "    def __chk_bestAcc(self, acc, epochIdx, net):\n",
    "        print(\"__chk_bestAcc is called\")\n",
    "        print(f\"current best Acc is {self.bestAcc}\")\n",
    "        print(f\"pass in acc is {acc}\")\n",
    "        if acc > self.bestAcc:\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            print(f\"model saved....., acc: {acc}\")\n",
    "            \n",
    "    def __on_epoch_end(self, start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        line = 'SP-{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            self.opt.splits, epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "        \n",
    "    \n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __get_batch(self, index):\n",
    "        x = self.trainX[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        y = self.trainY[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        return x.to(self.opt.device), y.to(self.opt.device);\n",
    "    \n",
    "    \n",
    "    def __calibrate(self, net):\n",
    "        self.load_train_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            for i in range(1,2):\n",
    "                x_pred = None;\n",
    "                for idx in range(math.ceil(len(self.trainX)/self.opt.batchSize)):\n",
    "                    x = self.trainX[idx*self.opt.batchSize : (idx+1)*self.opt.batchSize];\n",
    "                    scores = net(x);\n",
    "                    x_pred = scores.data if x_pred is None else torch.cat((x_pred, scores.data));\n",
    "                x_pred = x_pred.argmax(dim=1);\n",
    "                x_target = self.trainY.argmax(dim=1);\n",
    "                acc = (((x_pred==x_target)*1).float().mean()*100).item();\n",
    "                print('calibrate accuracy is: {:.2f}'.format(acc));\n",
    "        return acc;\n",
    "\n",
    "    def QuantizeModel(self):\n",
    "        net = self.__load_model(True);\n",
    "        # net = self.__load_model(False);\n",
    "        config = net.ch_config;\n",
    "        net.eval();\n",
    "        \n",
    "        #Fuse modules to\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['7','8','9'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['11','12','13'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['14','15','16'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['18','19','20'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['21','22','23'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['25','26','27'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['28','29','30'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['33','34','35'], inplace=True);\n",
    "\n",
    "        net.train();\n",
    "        net.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "        torch.backends.quantized.engine = 'qnnpack';\n",
    "        print(f\"net.qconfig : {net.qconfig}\");\n",
    "        torch.quantization.prepare_qat(net, inplace=True);\n",
    "        \n",
    "        # Calibrate with the training data\n",
    "        # self.__calibrate(net);\n",
    "        self.__train(net);\n",
    "\n",
    "        #place trained model to cpu\n",
    "        net.to('cpu');\n",
    "        # Convert to quantized model\n",
    "        torch.quantization.convert(net, inplace=True);\n",
    "        print('Post Training Quantization: Convert done');\n",
    "\n",
    "        print(\"Size of model after quantization\");\n",
    "        torch.save(net.state_dict(), \"temp.p\")\n",
    "        print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "        os.remove('temp.p')\n",
    "\n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate_test(net, True, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "        net.to('cpu');\n",
    "        # torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name.format()));\n",
    "        torch.jit.save(torch.jit.script(net), '../../../trained_models/step_6_QAT_and_Convert2TFLite/{}.pt'.format(self.opt.model_name));\n",
    "        # torch.save({'weight':net.state_dict(), 'config':net.ch_config}, '../has_qat_models/{}.pt'.format(self.opt.full_weight_and_config_model_name));\n",
    "        \n",
    "        # **************convert to tflite**********\n",
    "        with torch.no_grad():\n",
    "            # dummy_input = torch.randn(1, 1, 30225, 1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            # dummy_input = torch.FloatTensor(quantize_int8(torch.randn(1, 1, 1, 30225).numpy(),3)); #correct,workable\n",
    "            dummy_input = torch.FloatTensor(maskOP(torch.randn(1, 1, 1, 30225).numpy(),3)); #correct,workable\n",
    "            # dummy_input = torch.randn(30225,1,1,1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            # dummy_input = torch.randn(1,30225,1,1); wrong:RuntimeError: Input channel size of weight and bias must match.\n",
    "            #the followng setting for TFLiteConverter, especially quantize_input_output_type='int8',fuse_quant_dequant=True,\n",
    "            #we need to remove softmax layer from ACDQuantModel to satisfy the output is int8 type\n",
    "            converter = TFLiteConverter(net,\n",
    "                                        dummy_input,\n",
    "                                        quantize_input_output_type='int8',#設定此欄，輸入會強制為int8\n",
    "                                        fuse_quant_dequant=True,\n",
    "                                        quantize_target_type='int8',\n",
    "                                        hybrid_conv=False,\n",
    "                                        float16_quantization=True,\n",
    "                                        optimize=5,\n",
    "                                        tflite_path=\"../../../trained_models/step_6_QAT_and_Convert2TFLite/{}.tflite\".format(self.opt.model_name))\n",
    "            converter.convert()\n",
    "\n",
    "        \n",
    "    def TestModel(self, quant=False):\n",
    "        if quant:\n",
    "            print(f\"the model name:{self.opt.model_name}\");\n",
    "            net = torch.jit.load('../../../trained_models/step_6_QAT_and_Convert2TFLite/{}.pt'.format(self.opt.model_name))\n",
    "        else:\n",
    "            print(\"has not quanted, load unquanted model...\");\n",
    "            net = self.__load_model();\n",
    "            # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        self.load_test_data();\n",
    "        net.eval();\n",
    "        val_acc = self.__validate_test(net, False, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "    def GetModelSize(self):\n",
    "        orig_net_path = self.opt.model_path;\n",
    "        print('Full precision model size (KB):', os.path.getsize(orig_net_path)/(1024));\n",
    "        save_onnx_name = \"../../../trained_models/step_6_QAT_and_Convert2TFLite/{}.onnx\".format(self.opt.model_name);\n",
    "        quant_net_path = \"../has_qat_models/onnx_models/\"+save_onnx_name;\n",
    "        print('Quantized model size (KB):', os.path.getsize(quant_net_path)/(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1d22c1a-75b7-49c1-855f-7021a5ad68b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npruning algo: tylor-pruning\\npruning ration : 0.85\\nfinal accuracy : 87.46\\nepoch: \\nself.opt.LR = 0.1;\\nopt.momentum = 0.09;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'tylor-pruning';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n===================================\\n#this is bad settings\\npruning ration : 0.85\\nfinal accuracy : \\nepoch: \\nopt.batchSize = 128;\\nopt.weightDecay = 5e-4;\\nopt.LR = 0.5;\\nopt.momentum = 0.9;\\nopt.nEpochs = 1000;#2000;\\nopt.schedule = [0.6, 0.8, 0.9];\\nopt.warmup = 10;\\n===================================\\npruning ration : 0.85\\nfinal accuracy : \\nepoch: \\nopt.batchSize = 128;\\nopt.weightDecay = 5e-3;\\nopt.LR = 0.1;\\nopt.momentum = 0.9;\\nopt.nEpochs = 600;#2000;\\nopt.schedule = [0.6, 0.8, 0.9];\\nopt.warmup = 10;\\n############################################Training DataSet Version 4##########################\\n============================\\nopt.batchSize = 64;\\nopt.weightDecay = 5e-4;\\nopt.LR = 0.05;\\nopt.momentum = 0.9;\\nopt.nEpochs = 800;#2000;\\nopt.schedule = [0.1, 0.8, 0.9];\\nopt.warmup = 10;\\n===========================\\nno use\\nopt.batchSize = 128;\\nopt.weightDecay = 5e-4;\\nopt.LR = 0.5;\\nopt.momentum = 0.5;\\nopt.nEpochs = 800;#2000;\\nopt.schedule = [0.3, 0.6\\n==========================\\npruning ration : 0.85\\nfinal accuracy : \\nepoch:\\ntest acc:\\nopt.batchSize = 128;\\nopt.weightDecay = 5e-3;\\nopt.LR = 0.1;\\nopt.momentum = 0.09;\\nopt.nEpochs = 800;#2000;\\nopt.schedule = [0.3, 0.6, 0.9];\\nopt.warmup = 10;\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pruning algo: tylor-pruning\n",
    "pruning ration : 0.85\n",
    "final accuracy : 87.46\n",
    "epoch: \n",
    "self.opt.LR = 0.1;\n",
    "opt.momentum = 0.09;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'tylor-pruning';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "===================================\n",
    "#this is bad settings\n",
    "pruning ration : 0.85\n",
    "final accuracy : \n",
    "epoch: \n",
    "opt.batchSize = 128;\n",
    "opt.weightDecay = 5e-4;\n",
    "opt.LR = 0.5;\n",
    "opt.momentum = 0.9;\n",
    "opt.nEpochs = 1000;#2000;\n",
    "opt.schedule = [0.6, 0.8, 0.9];\n",
    "opt.warmup = 10;\n",
    "===================================\n",
    "pruning ration : 0.85\n",
    "final accuracy : \n",
    "epoch: \n",
    "opt.batchSize = 128;\n",
    "opt.weightDecay = 5e-3;\n",
    "opt.LR = 0.1;\n",
    "opt.momentum = 0.9;\n",
    "opt.nEpochs = 600;#2000;\n",
    "opt.schedule = [0.6, 0.8, 0.9];\n",
    "opt.warmup = 10;\n",
    "############################################Training DataSet Version 4##########################\n",
    "============================\n",
    "opt.batchSize = 64;\n",
    "opt.weightDecay = 5e-4;\n",
    "opt.LR = 0.05;\n",
    "opt.momentum = 0.9;\n",
    "opt.nEpochs = 800;#2000;\n",
    "opt.schedule = [0.1, 0.8, 0.9];\n",
    "opt.warmup = 10;\n",
    "===========================\n",
    "no use\n",
    "opt.batchSize = 128;\n",
    "opt.weightDecay = 5e-4;\n",
    "opt.LR = 0.5;\n",
    "opt.momentum = 0.5;\n",
    "opt.nEpochs = 800;#2000;\n",
    "opt.schedule = [0.3, 0.6\n",
    "==========================\n",
    "pruning ration : 0.85\n",
    "final accuracy : \n",
    "epoch:\n",
    "test acc:\n",
    "opt.batchSize = 128;\n",
    "opt.weightDecay = 5e-3;\n",
    "opt.LR = 0.1;\n",
    "opt.momentum = 0.09;\n",
    "opt.nEpochs = 800;#2000;\n",
    "opt.schedule = [0.3, 0.6, 0.9];\n",
    "opt.warmup = 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b75e728-df29-4468-ae40-06defe6836b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts();#opts.parse();\n",
    "    opt.device = 'cpu';\n",
    "    opt.saveInfo = \"valacc92.7_tracc_82.9_prunInfo_0.8_0.85_ds_ver4\"\n",
    "    opt.model_path = \"../../../trained_models/step_5_retrain_after_step_4/train_after_second_pruning_prunratio0.85_2024050315/retrained_model_ratio85.0_vaacc92.79661560058594_tracc_82.95454545454545@752epoch_20240503162619.pt\"\n",
    "    timeStr = genDataTimeStr();\n",
    "    opt.model_name = \"qat_model_{}_{}\".format(opt.saveInfo,timeStr);\n",
    "   \n",
    "    opt.split = 1;\n",
    "    opt.hasQuated = False;\n",
    "    display_info(opt);\n",
    "    trainer = QATTrainer(opt);\n",
    "\n",
    "    print('Testing performance of the provided model.....');\n",
    "    trainer.TestModel();\n",
    "\n",
    "    print('Quantization process is started.....');\n",
    "    trainer.QuantizeModel();\n",
    "    print('Quantization done');\n",
    "\n",
    "    print('Testing quantized model.');\n",
    "    trainer.TestModel(True);\n",
    "    print('Finished');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76313483-5605-4e87-97aa-30d3ca9e1a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "| TLACDNet Sound classification\n",
      "+------------------------------+\n",
      "| dataset  : uec_iot\n",
      "| nEpochs  : 800\n",
      "| LRInit   : 0.1\n",
      "| schedule : [0.3, 0.6, 0.9]\n",
      "| warmup   : 10\n",
      "| batchSize: 128\n",
      "| nFolds: 1\n",
      "| Splits: [1]\n",
      "| Device: cpu\n",
      "| Model Path: ../../../trained_models/step_5_retrain_after_step_4/train_after_second_pruning_prunratio0.85_2024050315/retrained_model_ratio85.0_vaacc92.79661560058594_tracc_82.95454545454545@752epoch_20240503162619.pt\n",
      "| Model Name: qat_model_valacc92.7_tracc_82.9_prunInfo_0.8_0.85_ds_ver4_20240503181545\n",
      "+------------------------------+\n",
      "length of samples:651\n",
      "Testing performance of the provided model.....\n",
      "has not quanted, load unquanted model...\n",
      "[5, 32, 10, 8, 17, 18, 27, 39, 34, 41, 72, 3]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)     (10, 32, 151)         90      434,880\n",
      " BatchNorm2d-10     (10, 32, 151)     (10, 32, 151)         20            0\n",
      "        ReLu-11     (10, 32, 151)     (10, 32, 151)          0       48,320\n",
      "   MaxPool2d-12     (10, 32, 151)      (10, 16, 75)          0       48,000\n",
      "      Conv2d-13      (10, 16, 75)       (8, 16, 75)        720      864,000\n",
      " BatchNorm2d-14       (8, 16, 75)       (8, 16, 75)         16            0\n",
      "        ReLu-15       (8, 16, 75)       (8, 16, 75)          0        9,600\n",
      "      Conv2d-16       (8, 16, 75)      (17, 16, 75)      1,224    1,468,800\n",
      " BatchNorm2d-17      (17, 16, 75)      (17, 16, 75)         34            0\n",
      "        ReLu-18      (17, 16, 75)      (17, 16, 75)          0       20,400\n",
      "   MaxPool2d-19      (17, 16, 75)       (17, 8, 37)          0       20,128\n",
      "      Conv2d-20       (17, 8, 37)       (18, 8, 37)      2,754      815,184\n",
      " BatchNorm2d-21       (18, 8, 37)       (18, 8, 37)         36            0\n",
      "        ReLu-22       (18, 8, 37)       (18, 8, 37)          0        5,328\n",
      "      Conv2d-23       (18, 8, 37)       (27, 8, 37)      4,374    1,294,704\n",
      " BatchNorm2d-24       (27, 8, 37)       (27, 8, 37)         54            0\n",
      "        ReLu-25       (27, 8, 37)       (27, 8, 37)          0        7,992\n",
      "   MaxPool2d-26       (27, 8, 37)       (27, 4, 18)          0        7,776\n",
      "      Conv2d-27       (27, 4, 18)       (39, 4, 18)      9,477      682,344\n",
      " BatchNorm2d-28       (39, 4, 18)       (39, 4, 18)         78            0\n",
      "        ReLu-29       (39, 4, 18)       (39, 4, 18)          0        2,808\n",
      "      Conv2d-30       (39, 4, 18)       (34, 4, 18)     11,934      859,248\n",
      " BatchNorm2d-31       (34, 4, 18)       (34, 4, 18)         68            0\n",
      "        ReLu-32       (34, 4, 18)       (34, 4, 18)          0        2,448\n",
      "   MaxPool2d-33       (34, 4, 18)        (34, 2, 9)          0        2,448\n",
      "      Conv2d-34        (34, 2, 9)        (41, 2, 9)     12,546      225,828\n",
      " BatchNorm2d-35        (41, 2, 9)        (41, 2, 9)         82            0\n",
      "        ReLu-36        (41, 2, 9)        (41, 2, 9)          0          738\n",
      "      Conv2d-37        (41, 2, 9)        (72, 2, 9)     26,568      478,224\n",
      " BatchNorm2d-38        (72, 2, 9)        (72, 2, 9)        144            0\n",
      "        ReLu-39        (72, 2, 9)        (72, 2, 9)          0        1,296\n",
      "   MaxPool2d-40        (72, 2, 9)        (72, 1, 4)          0        1,152\n",
      "      Conv2d-41        (72, 1, 4)         (3, 1, 4)        216          864\n",
      " BatchNorm2d-42         (3, 1, 4)         (3, 1, 4)          6            0\n",
      "        ReLu-43         (3, 1, 4)         (3, 1, 4)          0           12\n",
      "   AvgPool2d-44         (3, 1, 4)         (3, 1, 1)          0           12\n",
      "     Flatten-45         (3, 1, 1)            (1, 3)          0            0\n",
      "      Linear-46            (1, 3)            (1, 3)         12           12\n",
      "     Softmax-47            (1, 3)            (1, 3)          0            3\n",
      "==============================================================================\n",
      "Total Params: 71,372\n",
      "Total FLOPs : 14,583,695\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.27\n",
      "Total size (MB) : 0.39\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "device is :cpu\n",
      "len of Y:472\n",
      "torch.Size([472, 3])\n",
      "Testing: Acc(top1) 92.37%\n",
      "Quantization process is started.....\n",
      "[5, 32, 10, 8, 17, 18, 27, 39, 34, 41, 72, 3]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)     (10, 32, 151)         90      434,880\n",
      " BatchNorm2d-10     (10, 32, 151)     (10, 32, 151)         20            0\n",
      "        ReLu-11     (10, 32, 151)     (10, 32, 151)          0       48,320\n",
      "   MaxPool2d-12     (10, 32, 151)      (10, 16, 75)          0       48,000\n",
      "      Conv2d-13      (10, 16, 75)       (8, 16, 75)        720      864,000\n",
      " BatchNorm2d-14       (8, 16, 75)       (8, 16, 75)         16            0\n",
      "        ReLu-15       (8, 16, 75)       (8, 16, 75)          0        9,600\n",
      "      Conv2d-16       (8, 16, 75)      (17, 16, 75)      1,224    1,468,800\n",
      " BatchNorm2d-17      (17, 16, 75)      (17, 16, 75)         34            0\n",
      "        ReLu-18      (17, 16, 75)      (17, 16, 75)          0       20,400\n",
      "   MaxPool2d-19      (17, 16, 75)       (17, 8, 37)          0       20,128\n",
      "      Conv2d-20       (17, 8, 37)       (18, 8, 37)      2,754      815,184\n",
      " BatchNorm2d-21       (18, 8, 37)       (18, 8, 37)         36            0\n",
      "        ReLu-22       (18, 8, 37)       (18, 8, 37)          0        5,328\n",
      "      Conv2d-23       (18, 8, 37)       (27, 8, 37)      4,374    1,294,704\n",
      " BatchNorm2d-24       (27, 8, 37)       (27, 8, 37)         54            0\n",
      "        ReLu-25       (27, 8, 37)       (27, 8, 37)          0        7,992\n",
      "   MaxPool2d-26       (27, 8, 37)       (27, 4, 18)          0        7,776\n",
      "      Conv2d-27       (27, 4, 18)       (39, 4, 18)      9,477      682,344\n",
      " BatchNorm2d-28       (39, 4, 18)       (39, 4, 18)         78            0\n",
      "        ReLu-29       (39, 4, 18)       (39, 4, 18)          0        2,808\n",
      "      Conv2d-30       (39, 4, 18)       (34, 4, 18)     11,934      859,248\n",
      " BatchNorm2d-31       (34, 4, 18)       (34, 4, 18)         68            0\n",
      "        ReLu-32       (34, 4, 18)       (34, 4, 18)          0        2,448\n",
      "   MaxPool2d-33       (34, 4, 18)        (34, 2, 9)          0        2,448\n",
      "      Conv2d-34        (34, 2, 9)        (41, 2, 9)     12,546      225,828\n",
      " BatchNorm2d-35        (41, 2, 9)        (41, 2, 9)         82            0\n",
      "        ReLu-36        (41, 2, 9)        (41, 2, 9)          0          738\n",
      "      Conv2d-37        (41, 2, 9)        (72, 2, 9)     26,568      478,224\n",
      " BatchNorm2d-38        (72, 2, 9)        (72, 2, 9)        144            0\n",
      "        ReLu-39        (72, 2, 9)        (72, 2, 9)          0        1,296\n",
      "   MaxPool2d-40        (72, 2, 9)        (72, 1, 4)          0        1,152\n",
      "      Conv2d-41        (72, 1, 4)         (3, 1, 4)        216          864\n",
      " BatchNorm2d-42         (3, 1, 4)         (3, 1, 4)          6            0\n",
      "        ReLu-43         (3, 1, 4)         (3, 1, 4)          0           12\n",
      "   AvgPool2d-44         (3, 1, 4)         (3, 1, 1)          0           12\n",
      "     Flatten-45         (3, 1, 1)            (1, 3)          0            0\n",
      "      Linear-46            (1, 3)            (1, 3)         12           12\n",
      "     Softmax-47            (1, 3)            (1, 3)          0            3\n",
      "==============================================================================\n",
      "Total Params: 71,372\n",
      "Total FLOPs : 14,583,695\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.27\n",
      "Total size (MB) : 0.39\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "net.qconfig : QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Preparing calibration dataset..\n",
      "Calibration dataset is ready\n",
      "error label:tensor([[9.4159e-01, 0.0000e+00, 5.8414e-02],\n",
      "        [0.0000e+00, 8.1612e-01, 1.8388e-01],\n",
      "        [4.4271e-01, 5.5729e-01, 0.0000e+00],\n",
      "        [5.1537e-02, 0.0000e+00, 9.4846e-01],\n",
      "        [0.0000e+00, 3.0319e-01, 6.9681e-01],\n",
      "        [3.9663e-01, 0.0000e+00, 6.0337e-01],\n",
      "        [3.1292e-01, 0.0000e+00, 6.8708e-01],\n",
      "        [2.7999e-01, 7.2001e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 9.4346e-01, 5.6535e-02],\n",
      "        [5.1168e-01, 0.0000e+00, 4.8832e-01],\n",
      "        [9.5149e-01, 0.0000e+00, 4.8514e-02],\n",
      "        [5.0573e-01, 0.0000e+00, 4.9427e-01],\n",
      "        [9.8286e-01, 0.0000e+00, 1.7143e-02],\n",
      "        [8.0807e-01, 0.0000e+00, 1.9193e-01],\n",
      "        [0.0000e+00, 5.4100e-01, 4.5900e-01],\n",
      "        [9.4502e-03, 0.0000e+00, 9.9055e-01],\n",
      "        [0.0000e+00, 2.2977e-01, 7.7023e-01],\n",
      "        [4.3649e-01, 5.6351e-01, 0.0000e+00],\n",
      "        [1.7910e-01, 8.2090e-01, 0.0000e+00],\n",
      "        [3.2297e-01, 6.7703e-01, 0.0000e+00],\n",
      "        [2.5454e-01, 0.0000e+00, 7.4546e-01],\n",
      "        [9.5486e-01, 0.0000e+00, 4.5142e-02],\n",
      "        [4.4696e-01, 5.5304e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 7.9806e-01, 2.0194e-01],\n",
      "        [7.3966e-01, 2.6034e-01, 0.0000e+00],\n",
      "        [1.8739e-01, 8.1261e-01, 0.0000e+00],\n",
      "        [4.0196e-01, 0.0000e+00, 5.9804e-01],\n",
      "        [9.8864e-02, 0.0000e+00, 9.0114e-01],\n",
      "        [0.0000e+00, 2.6892e-01, 7.3108e-01],\n",
      "        [3.6112e-01, 0.0000e+00, 6.3888e-01],\n",
      "        [0.0000e+00, 3.3222e-01, 6.6778e-01],\n",
      "        [0.0000e+00, 6.0321e-01, 3.9679e-01],\n",
      "        [0.0000e+00, 2.6513e-01, 7.3487e-01],\n",
      "        [0.0000e+00, 6.3498e-01, 3.6502e-01],\n",
      "        [0.0000e+00, 2.6916e-02, 9.7308e-01],\n",
      "        [0.0000e+00, 8.4779e-01, 1.5221e-01],\n",
      "        [2.8476e-01, 0.0000e+00, 7.1524e-01],\n",
      "        [5.0661e-01, 0.0000e+00, 4.9339e-01],\n",
      "        [9.1440e-01, 0.0000e+00, 8.5599e-02],\n",
      "        [0.0000e+00, 9.6681e-01, 3.3189e-02],\n",
      "        [4.4201e-01, 0.0000e+00, 5.5799e-01],\n",
      "        [0.0000e+00, 6.9044e-01, 3.0956e-01],\n",
      "        [1.1607e-01, 0.0000e+00, 8.8393e-01],\n",
      "        [4.8293e-01, 0.0000e+00, 5.1707e-01],\n",
      "        [3.5203e-01, 6.4797e-01, 0.0000e+00],\n",
      "        [4.2637e-01, 5.7363e-01, 0.0000e+00],\n",
      "        [4.4500e-03, 0.0000e+00, 9.9555e-01],\n",
      "        [6.3421e-01, 3.6579e-01, 0.0000e+00],\n",
      "        [3.1340e-01, 6.8660e-01, 0.0000e+00],\n",
      "        [7.0599e-01, 0.0000e+00, 2.9401e-01],\n",
      "        [4.8918e-01, 5.1082e-01, 0.0000e+00],\n",
      "        [6.9343e-01, 0.0000e+00, 3.0657e-01],\n",
      "        [8.4199e-01, 0.0000e+00, 1.5801e-01],\n",
      "        [4.9456e-01, 0.0000e+00, 5.0544e-01],\n",
      "        [7.7516e-01, 0.0000e+00, 2.2484e-01],\n",
      "        [9.1064e-03, 0.0000e+00, 9.9089e-01],\n",
      "        [2.9771e-01, 0.0000e+00, 7.0229e-01],\n",
      "        [0.0000e+00, 2.2437e-01, 7.7563e-01],\n",
      "        [1.9456e-01, 0.0000e+00, 8.0544e-01],\n",
      "        [0.0000e+00, 7.0737e-01, 2.9263e-01],\n",
      "        [1.3295e-01, 0.0000e+00, 8.6705e-01],\n",
      "        [8.4356e-01, 1.5644e-01, 0.0000e+00],\n",
      "        [7.5516e-02, 0.0000e+00, 9.2448e-01],\n",
      "        [4.5470e-01, 0.0000e+00, 5.4530e-01],\n",
      "        [3.6516e-01, 0.0000e+00, 6.3484e-01],\n",
      "        [2.6854e-01, 0.0000e+00, 7.3146e-01],\n",
      "        [2.3775e-01, 0.0000e+00, 7.6225e-01],\n",
      "        [8.8215e-01, 0.0000e+00, 1.1785e-01],\n",
      "        [9.4821e-01, 5.1787e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 4.7681e-01, 5.2319e-01],\n",
      "        [0.0000e+00, 8.1711e-01, 1.8289e-01],\n",
      "        [8.9680e-01, 0.0000e+00, 1.0320e-01],\n",
      "        [9.3159e-01, 6.8412e-02, 0.0000e+00],\n",
      "        [8.0106e-02, 0.0000e+00, 9.1989e-01],\n",
      "        [0.0000e+00, 4.1043e-01, 5.8957e-01],\n",
      "        [0.0000e+00, 7.5183e-01, 2.4817e-01],\n",
      "        [0.0000e+00, 7.9455e-01, 2.0545e-01],\n",
      "        [0.0000e+00, 9.5467e-01, 4.5328e-02],\n",
      "        [9.8688e-01, 1.3117e-02, 0.0000e+00],\n",
      "        [6.8294e-01, 0.0000e+00, 3.1706e-01],\n",
      "        [5.1320e-02, 0.0000e+00, 9.4868e-01],\n",
      "        [0.0000e+00, 4.0228e-01, 5.9772e-01],\n",
      "        [9.7431e-01, 2.5687e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 3.1017e-01, 6.8983e-01],\n",
      "        [3.9117e-01, 6.0883e-01, 0.0000e+00],\n",
      "        [2.2857e-01, 7.7143e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0385e-01, 4.9615e-01],\n",
      "        [7.7333e-01, 0.0000e+00, 2.2667e-01],\n",
      "        [2.4455e-01, 7.5545e-01, 0.0000e+00],\n",
      "        [2.5933e-01, 0.0000e+00, 7.4067e-01],\n",
      "        [0.0000e+00, 4.9796e-01, 5.0204e-01],\n",
      "        [8.7699e-01, 1.2301e-01, 0.0000e+00],\n",
      "        [7.8451e-01, 0.0000e+00, 2.1549e-01],\n",
      "        [0.0000e+00, 1.9542e-01, 8.0458e-01],\n",
      "        [6.5358e-01, 3.4642e-01, 0.0000e+00],\n",
      "        [3.0751e-01, 0.0000e+00, 6.9249e-01],\n",
      "        [0.0000e+00, 7.1165e-01, 2.8835e-01],\n",
      "        [0.0000e+00, 8.4902e-02, 9.1510e-01],\n",
      "        [2.3677e-01, 0.0000e+00, 7.6323e-01],\n",
      "        [0.0000e+00, 2.7196e-01, 7.2804e-01],\n",
      "        [5.5043e-01, 4.4957e-01, 0.0000e+00],\n",
      "        [2.6501e-01, 0.0000e+00, 7.3499e-01],\n",
      "        [8.3592e-01, 1.6408e-01, 0.0000e+00],\n",
      "        [2.9884e-01, 7.0116e-01, 0.0000e+00],\n",
      "        [1.6963e-01, 8.3037e-01, 0.0000e+00],\n",
      "        [6.4469e-01, 3.5531e-01, 0.0000e+00],\n",
      "        [1.9070e-01, 8.0930e-01, 0.0000e+00],\n",
      "        [5.6684e-01, 4.3316e-01, 0.0000e+00],\n",
      "        [9.1696e-02, 0.0000e+00, 9.0830e-01],\n",
      "        [7.0489e-01, 0.0000e+00, 2.9511e-01],\n",
      "        [0.0000e+00, 5.2581e-01, 4.7419e-01],\n",
      "        [2.1856e-01, 0.0000e+00, 7.8144e-01],\n",
      "        [0.0000e+00, 2.9303e-02, 9.7070e-01],\n",
      "        [3.9425e-04, 9.9961e-01, 0.0000e+00],\n",
      "        [9.1726e-01, 8.2741e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 9.9154e-01, 8.4581e-03],\n",
      "        [0.0000e+00, 2.8207e-02, 9.7179e-01],\n",
      "        [9.7039e-01, 0.0000e+00, 2.9612e-02],\n",
      "        [1.9928e-02, 0.0000e+00, 9.8007e-01],\n",
      "        [0.0000e+00, 9.1878e-01, 8.1219e-02],\n",
      "        [2.2563e-01, 7.7437e-01, 0.0000e+00],\n",
      "        [6.6344e-01, 0.0000e+00, 3.3656e-01],\n",
      "        [7.0718e-01, 0.0000e+00, 2.9282e-01],\n",
      "        [0.0000e+00, 1.2735e-01, 8.7265e-01],\n",
      "        [7.7910e-01, 0.0000e+00, 2.2090e-01],\n",
      "        [8.3211e-01, 0.0000e+00, 1.6789e-01],\n",
      "        [3.5048e-01, 0.0000e+00, 6.4952e-01],\n",
      "        [4.5398e-01, 0.0000e+00, 5.4602e-01]])\n",
      "error data:tensor([[[[ 9.4497e+02,  2.3713e+03,  2.5878e+03,  ..., -1.9485e+02,\n",
      "           -2.5197e+02, -2.7236e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6621e+02, -1.0259e+02, -1.0862e+02,  ..., -5.0696e+02,\n",
      "           -5.7113e+02, -4.1511e+02]]],\n",
      "\n",
      "\n",
      "        [[[-1.4864e+02, -2.0336e+02, -2.1387e+02,  ..., -4.4301e+00,\n",
      "            8.8794e-02,  2.3935e+01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 5.6602e+02,  4.6856e+02,  3.1218e+02,  ..., -2.8476e+02,\n",
      "           -3.5395e+02, -3.0701e+02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2920e+03, -1.2920e+03, -1.2919e+03,  ...,  1.8131e+02,\n",
      "            1.7998e+02,  1.6405e+02]]],\n",
      "\n",
      "\n",
      "        [[[-4.4141e+02,  7.7709e+01,  2.3579e+02,  ...,  2.4021e+02,\n",
      "            3.4234e+01, -2.7455e+02]]]])\n",
      "error label:tensor([[0.7389, 0.2611, 0.0000],\n",
      "        [0.0582, 0.0000, 0.9418],\n",
      "        [0.1571, 0.0000, 0.8429],\n",
      "        [0.0000, 0.6071, 0.3929],\n",
      "        [0.1761, 0.0000, 0.8239],\n",
      "        [0.0000, 0.0229, 0.9771],\n",
      "        [0.0911, 0.9089, 0.0000],\n",
      "        [0.0000, 0.2633, 0.7367],\n",
      "        [0.1918, 0.0000, 0.8082],\n",
      "        [0.0000, 0.0344, 0.9656],\n",
      "        [0.0000, 0.0347, 0.9653],\n",
      "        [0.0278, 0.0000, 0.9722],\n",
      "        [0.1659, 0.0000, 0.8341],\n",
      "        [0.1482, 0.0000, 0.8518],\n",
      "        [0.7359, 0.0000, 0.2641],\n",
      "        [0.3192, 0.0000, 0.6808],\n",
      "        [0.0000, 0.7314, 0.2686],\n",
      "        [0.0000, 0.0275, 0.9725],\n",
      "        [0.0000, 0.6050, 0.3950],\n",
      "        [0.0000, 0.4835, 0.5165],\n",
      "        [0.4034, 0.0000, 0.5966],\n",
      "        [0.1991, 0.8009, 0.0000],\n",
      "        [0.0000, 0.5869, 0.4131],\n",
      "        [0.3866, 0.6134, 0.0000],\n",
      "        [0.0000, 0.3484, 0.6516],\n",
      "        [0.2238, 0.0000, 0.7762],\n",
      "        [0.0000, 0.8977, 0.1023],\n",
      "        [0.2611, 0.0000, 0.7389],\n",
      "        [0.8100, 0.0000, 0.1900],\n",
      "        [0.7406, 0.2594, 0.0000],\n",
      "        [0.2735, 0.0000, 0.7265],\n",
      "        [0.5292, 0.4708, 0.0000],\n",
      "        [0.7777, 0.0000, 0.2223],\n",
      "        [0.0000, 0.2941, 0.7059],\n",
      "        [0.9288, 0.0000, 0.0712],\n",
      "        [0.9459, 0.0000, 0.0541],\n",
      "        [0.1432, 0.8568, 0.0000],\n",
      "        [0.0000, 0.9953, 0.0047],\n",
      "        [0.1294, 0.0000, 0.8706],\n",
      "        [0.1080, 0.0000, 0.8920],\n",
      "        [0.0000, 0.7737, 0.2263],\n",
      "        [0.9347, 0.0653, 0.0000],\n",
      "        [0.0000, 0.8880, 0.1120],\n",
      "        [0.0000, 0.2728, 0.7272],\n",
      "        [0.7921, 0.0000, 0.2079],\n",
      "        [0.0000, 0.1081, 0.8919],\n",
      "        [0.3048, 0.0000, 0.6952],\n",
      "        [0.0000, 0.6056, 0.3944],\n",
      "        [0.4977, 0.0000, 0.5023],\n",
      "        [0.0000, 0.1604, 0.8396],\n",
      "        [0.0000, 0.9069, 0.0931],\n",
      "        [0.2764, 0.0000, 0.7236],\n",
      "        [0.0000, 0.0998, 0.9002],\n",
      "        [0.0950, 0.9050, 0.0000],\n",
      "        [0.0000, 0.3726, 0.6274],\n",
      "        [0.6899, 0.0000, 0.3101],\n",
      "        [0.8406, 0.0000, 0.1594],\n",
      "        [0.4831, 0.0000, 0.5169],\n",
      "        [0.9779, 0.0000, 0.0221],\n",
      "        [0.6763, 0.0000, 0.3237],\n",
      "        [0.0749, 0.0000, 0.9251],\n",
      "        [0.1829, 0.8171, 0.0000],\n",
      "        [0.2548, 0.0000, 0.7452],\n",
      "        [0.8901, 0.0000, 0.1099],\n",
      "        [0.4000, 0.0000, 0.6000],\n",
      "        [0.4523, 0.5477, 0.0000],\n",
      "        [0.1959, 0.0000, 0.8041],\n",
      "        [0.0000, 0.8922, 0.1078],\n",
      "        [0.9639, 0.0000, 0.0361],\n",
      "        [0.7811, 0.0000, 0.2189],\n",
      "        [0.3938, 0.0000, 0.6062],\n",
      "        [0.0000, 0.5472, 0.4528],\n",
      "        [0.0000, 0.2632, 0.7368],\n",
      "        [0.0000, 0.1327, 0.8673],\n",
      "        [0.7608, 0.0000, 0.2392],\n",
      "        [0.0000, 0.0531, 0.9469],\n",
      "        [0.3094, 0.6906, 0.0000],\n",
      "        [0.1352, 0.0000, 0.8648],\n",
      "        [0.0000, 0.9265, 0.0735],\n",
      "        [0.0000, 0.8300, 0.1700],\n",
      "        [0.0000, 0.7351, 0.2649],\n",
      "        [0.8460, 0.0000, 0.1540],\n",
      "        [0.0676, 0.0000, 0.9324],\n",
      "        [0.0000, 0.7077, 0.2923],\n",
      "        [0.0000, 0.5370, 0.4630],\n",
      "        [0.0000, 0.1763, 0.8237],\n",
      "        [0.0000, 0.8857, 0.1143],\n",
      "        [0.8221, 0.0000, 0.1779],\n",
      "        [0.9177, 0.0000, 0.0823],\n",
      "        [0.8357, 0.0000, 0.1643],\n",
      "        [0.9621, 0.0000, 0.0379],\n",
      "        [0.2586, 0.7414, 0.0000],\n",
      "        [0.0000, 0.2797, 0.7203],\n",
      "        [0.8939, 0.0000, 0.1061],\n",
      "        [0.0000, 0.0308, 0.9692],\n",
      "        [0.8559, 0.1441, 0.0000],\n",
      "        [0.9950, 0.0000, 0.0050],\n",
      "        [0.8515, 0.1485, 0.0000],\n",
      "        [0.0000, 0.1477, 0.8523],\n",
      "        [0.2744, 0.0000, 0.7256],\n",
      "        [0.0000, 0.9001, 0.0999],\n",
      "        [0.0000, 0.4344, 0.5656],\n",
      "        [0.0000, 0.7122, 0.2878],\n",
      "        [0.0000, 0.4454, 0.5546],\n",
      "        [0.8036, 0.0000, 0.1964],\n",
      "        [0.9688, 0.0000, 0.0312],\n",
      "        [0.5279, 0.4721, 0.0000],\n",
      "        [0.0000, 0.5104, 0.4896],\n",
      "        [0.4859, 0.0000, 0.5141],\n",
      "        [0.7359, 0.0000, 0.2641],\n",
      "        [0.6306, 0.0000, 0.3694],\n",
      "        [0.0000, 0.8816, 0.1184],\n",
      "        [0.1651, 0.8349, 0.0000],\n",
      "        [0.5167, 0.0000, 0.4833],\n",
      "        [0.0000, 0.8872, 0.1128],\n",
      "        [0.2115, 0.0000, 0.7885],\n",
      "        [0.6752, 0.3248, 0.0000],\n",
      "        [0.8721, 0.0000, 0.1279],\n",
      "        [0.2583, 0.0000, 0.7417],\n",
      "        [0.0527, 0.0000, 0.9473],\n",
      "        [0.4174, 0.0000, 0.5826],\n",
      "        [0.4094, 0.5906, 0.0000],\n",
      "        [0.3156, 0.0000, 0.6844],\n",
      "        [0.4686, 0.0000, 0.5314],\n",
      "        [0.8970, 0.0000, 0.1030],\n",
      "        [0.3505, 0.0000, 0.6495],\n",
      "        [0.0899, 0.0000, 0.9101],\n",
      "        [0.0000, 0.4802, 0.5198]])\n",
      "error data:tensor([[[[-6.9799e+01, -9.8319e+01,  1.9913e+01,  ..., -3.6801e+02,\n",
      "            5.4200e+01,  5.1586e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3829e+02,  2.2867e+02, -3.6967e+02,  ...,  2.6489e+03,\n",
      "            2.6942e+03,  2.5718e+03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1493e+02,  4.0948e+02,  1.9548e+02,  ..., -2.5053e+01,\n",
      "           -6.5986e+00,  4.2060e+01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.4926e+01,  8.7511e+01,  8.8893e+01,  ...,  1.8999e+02,\n",
      "            2.1367e+02,  2.0553e+02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6094e+02, -2.2267e+02, -1.2782e+02,  ..., -2.4922e+02,\n",
      "           -1.7480e+02, -8.5077e+01]]],\n",
      "\n",
      "\n",
      "        [[[-2.6247e-02,  1.9996e-02, -1.3745e-02,  ..., -1.3360e+01,\n",
      "            1.8409e+02,  1.1947e+02]]]])\n",
      "error label:tensor([[0.0000, 0.4479, 0.5521],\n",
      "        [0.5777, 0.0000, 0.4223],\n",
      "        [0.5534, 0.0000, 0.4466],\n",
      "        [0.0000, 0.6971, 0.3029],\n",
      "        [0.1453, 0.0000, 0.8547],\n",
      "        [0.9622, 0.0000, 0.0378],\n",
      "        [0.0000, 0.3597, 0.6403],\n",
      "        [0.0000, 0.5158, 0.4842],\n",
      "        [0.0000, 0.2203, 0.7797],\n",
      "        [0.0000, 0.7155, 0.2845],\n",
      "        [0.0000, 0.4251, 0.5749],\n",
      "        [0.8029, 0.0000, 0.1971],\n",
      "        [0.0000, 0.7575, 0.2425],\n",
      "        [0.3231, 0.0000, 0.6769],\n",
      "        [0.0000, 0.8322, 0.1678],\n",
      "        [0.9314, 0.0000, 0.0686],\n",
      "        [0.0000, 0.0697, 0.9303],\n",
      "        [0.3154, 0.0000, 0.6846],\n",
      "        [0.0000, 0.2498, 0.7502],\n",
      "        [0.9765, 0.0000, 0.0235],\n",
      "        [0.0000, 0.4190, 0.5810],\n",
      "        [0.0000, 0.0415, 0.9585],\n",
      "        [0.0000, 0.3651, 0.6349],\n",
      "        [0.9001, 0.0000, 0.0999],\n",
      "        [0.0000, 0.4730, 0.5270],\n",
      "        [0.3000, 0.0000, 0.7000],\n",
      "        [0.1862, 0.0000, 0.8138],\n",
      "        [0.4396, 0.0000, 0.5604],\n",
      "        [0.0000, 0.1990, 0.8010],\n",
      "        [0.4896, 0.0000, 0.5104],\n",
      "        [0.1317, 0.0000, 0.8683],\n",
      "        [0.2675, 0.0000, 0.7325],\n",
      "        [0.0000, 0.9655, 0.0345],\n",
      "        [0.2993, 0.7007, 0.0000],\n",
      "        [0.5484, 0.0000, 0.4516],\n",
      "        [0.0000, 0.3094, 0.6906],\n",
      "        [0.0000, 0.0286, 0.9714],\n",
      "        [0.0000, 0.5485, 0.4515],\n",
      "        [0.1227, 0.0000, 0.8773],\n",
      "        [0.0000, 0.5581, 0.4419],\n",
      "        [0.6561, 0.3439, 0.0000],\n",
      "        [0.7980, 0.0000, 0.2020],\n",
      "        [0.3948, 0.0000, 0.6052],\n",
      "        [0.8076, 0.0000, 0.1924],\n",
      "        [0.0000, 0.1824, 0.8176],\n",
      "        [0.0000, 0.8479, 0.1521],\n",
      "        [0.7271, 0.2729, 0.0000],\n",
      "        [0.0000, 0.0020, 0.9980],\n",
      "        [0.0714, 0.9286, 0.0000],\n",
      "        [0.7528, 0.0000, 0.2472],\n",
      "        [0.0000, 0.4728, 0.5272],\n",
      "        [0.1221, 0.0000, 0.8779],\n",
      "        [0.3671, 0.0000, 0.6329],\n",
      "        [0.9688, 0.0312, 0.0000],\n",
      "        [0.6859, 0.0000, 0.3141],\n",
      "        [0.8926, 0.0000, 0.1074],\n",
      "        [0.0000, 0.4387, 0.5613],\n",
      "        [0.2947, 0.7053, 0.0000],\n",
      "        [0.0000, 0.2753, 0.7247],\n",
      "        [0.4584, 0.0000, 0.5416],\n",
      "        [0.6793, 0.3207, 0.0000],\n",
      "        [0.0000, 0.2831, 0.7169],\n",
      "        [0.8214, 0.1786, 0.0000],\n",
      "        [0.6579, 0.0000, 0.3421],\n",
      "        [0.0000, 0.9869, 0.0131],\n",
      "        [0.1780, 0.0000, 0.8220],\n",
      "        [0.3039, 0.0000, 0.6961],\n",
      "        [0.8069, 0.0000, 0.1931],\n",
      "        [0.6745, 0.0000, 0.3255],\n",
      "        [0.6625, 0.0000, 0.3375],\n",
      "        [0.0000, 0.2358, 0.7642],\n",
      "        [0.0000, 0.5502, 0.4498],\n",
      "        [0.0000, 0.0066, 0.9934],\n",
      "        [0.1547, 0.8453, 0.0000],\n",
      "        [0.7670, 0.0000, 0.2330],\n",
      "        [0.3247, 0.0000, 0.6753],\n",
      "        [0.3815, 0.0000, 0.6185],\n",
      "        [0.6339, 0.3661, 0.0000],\n",
      "        [0.3268, 0.6732, 0.0000],\n",
      "        [0.8678, 0.0000, 0.1322],\n",
      "        [0.5655, 0.0000, 0.4345],\n",
      "        [0.1961, 0.0000, 0.8039],\n",
      "        [0.0000, 0.3153, 0.6847],\n",
      "        [0.5939, 0.0000, 0.4061],\n",
      "        [0.9551, 0.0449, 0.0000],\n",
      "        [0.0000, 0.7396, 0.2604],\n",
      "        [0.3062, 0.0000, 0.6938],\n",
      "        [0.5233, 0.4767, 0.0000],\n",
      "        [0.7287, 0.0000, 0.2713],\n",
      "        [0.0000, 0.9809, 0.0191],\n",
      "        [0.0000, 0.0961, 0.9039],\n",
      "        [0.0215, 0.9785, 0.0000],\n",
      "        [0.0000, 0.6532, 0.3468],\n",
      "        [0.0000, 0.3742, 0.6258],\n",
      "        [0.6569, 0.0000, 0.3431],\n",
      "        [0.0000, 0.9097, 0.0903],\n",
      "        [0.1999, 0.0000, 0.8001],\n",
      "        [0.0000, 0.1452, 0.8548],\n",
      "        [0.1841, 0.8159, 0.0000],\n",
      "        [0.0000, 0.9333, 0.0667],\n",
      "        [0.3831, 0.6169, 0.0000],\n",
      "        [0.0000, 0.9205, 0.0795],\n",
      "        [0.1840, 0.0000, 0.8160],\n",
      "        [0.0746, 0.0000, 0.9254],\n",
      "        [0.3121, 0.6879, 0.0000],\n",
      "        [0.0000, 0.7202, 0.2798],\n",
      "        [0.8778, 0.0000, 0.1222],\n",
      "        [0.2912, 0.0000, 0.7088],\n",
      "        [0.4830, 0.0000, 0.5170],\n",
      "        [0.0622, 0.0000, 0.9378],\n",
      "        [0.5903, 0.0000, 0.4097],\n",
      "        [0.0000, 0.7965, 0.2035],\n",
      "        [0.7269, 0.0000, 0.2731],\n",
      "        [0.0877, 0.0000, 0.9123],\n",
      "        [0.1176, 0.0000, 0.8824],\n",
      "        [0.1446, 0.8554, 0.0000],\n",
      "        [0.4757, 0.0000, 0.5243],\n",
      "        [0.6198, 0.0000, 0.3802],\n",
      "        [0.8546, 0.1454, 0.0000],\n",
      "        [0.6609, 0.0000, 0.3391],\n",
      "        [0.7685, 0.0000, 0.2315],\n",
      "        [0.5292, 0.0000, 0.4708],\n",
      "        [0.0000, 0.2703, 0.7297],\n",
      "        [0.2872, 0.0000, 0.7128],\n",
      "        [0.4925, 0.0000, 0.5075],\n",
      "        [0.0000, 0.5351, 0.4649],\n",
      "        [0.0000, 0.5604, 0.4396],\n",
      "        [0.3003, 0.6997, 0.0000]])\n",
      "error data:tensor([[[[ 1.7913e+03,  2.0379e+03,  1.9186e+03,  ..., -3.6502e+02,\n",
      "           -9.0125e+02, -2.0690e+03]]],\n",
      "\n",
      "\n",
      "        [[[-5.6312e+03, -3.3126e+03,  1.2693e+03,  ..., -4.1745e+03,\n",
      "           -5.1207e+03, -2.9516e+03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6329e+03,  1.6673e+03,  1.6740e+03,  ..., -4.3036e+01,\n",
      "           -4.7391e+00,  7.1455e+01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.4419e+03,  5.0291e+03,  4.2137e+03,  ..., -6.4995e+03,\n",
      "           -4.2742e+03, -2.7744e+03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1648e+02,  1.4634e+02, -3.4122e+00,  ..., -5.0954e+03,\n",
      "           -5.4650e+03, -5.7548e+03]]],\n",
      "\n",
      "\n",
      "        [[[-1.2410e+02, -1.6695e+03, -2.0300e+03,  ...,  2.0961e+03,\n",
      "            2.2471e+03,  2.1097e+03]]]])\n",
      "error label:tensor([[7.7720e-02, 0.0000e+00, 9.2228e-01],\n",
      "        [1.0794e-01, 0.0000e+00, 8.9206e-01],\n",
      "        [6.8450e-01, 0.0000e+00, 3.1550e-01],\n",
      "        [0.0000e+00, 2.8046e-01, 7.1954e-01],\n",
      "        [1.3541e-01, 0.0000e+00, 8.6459e-01],\n",
      "        [6.2842e-01, 3.7158e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 8.6646e-02, 9.1335e-01],\n",
      "        [7.3244e-01, 0.0000e+00, 2.6756e-01],\n",
      "        [3.2328e-01, 6.7672e-01, 0.0000e+00],\n",
      "        [9.3234e-01, 0.0000e+00, 6.7663e-02],\n",
      "        [9.7752e-01, 0.0000e+00, 2.2480e-02],\n",
      "        [0.0000e+00, 8.4250e-01, 1.5750e-01],\n",
      "        [8.5844e-01, 0.0000e+00, 1.4156e-01],\n",
      "        [0.0000e+00, 5.4058e-01, 4.5942e-01],\n",
      "        [8.6120e-02, 0.0000e+00, 9.1388e-01],\n",
      "        [0.0000e+00, 8.1285e-01, 1.8715e-01],\n",
      "        [5.2945e-01, 0.0000e+00, 4.7055e-01],\n",
      "        [9.1726e-01, 0.0000e+00, 8.2736e-02],\n",
      "        [0.0000e+00, 4.7865e-01, 5.2135e-01],\n",
      "        [0.0000e+00, 8.1755e-01, 1.8245e-01],\n",
      "        [0.0000e+00, 5.9054e-01, 4.0946e-01],\n",
      "        [0.0000e+00, 2.0824e-01, 7.9176e-01],\n",
      "        [0.0000e+00, 8.8568e-01, 1.1432e-01],\n",
      "        [0.0000e+00, 6.9608e-02, 9.3039e-01],\n",
      "        [0.0000e+00, 8.8726e-01, 1.1274e-01],\n",
      "        [1.9567e-01, 0.0000e+00, 8.0433e-01],\n",
      "        [0.0000e+00, 9.7107e-03, 9.9029e-01],\n",
      "        [3.2100e-02, 0.0000e+00, 9.6790e-01],\n",
      "        [3.9427e-01, 6.0573e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 6.1540e-01, 3.8460e-01],\n",
      "        [9.2825e-01, 0.0000e+00, 7.1746e-02],\n",
      "        [5.0941e-01, 0.0000e+00, 4.9059e-01],\n",
      "        [0.0000e+00, 1.5320e-01, 8.4680e-01],\n",
      "        [0.0000e+00, 2.1767e-02, 9.7823e-01],\n",
      "        [1.6421e-02, 0.0000e+00, 9.8358e-01],\n",
      "        [0.0000e+00, 2.4523e-01, 7.5477e-01],\n",
      "        [9.0359e-01, 0.0000e+00, 9.6415e-02],\n",
      "        [4.2102e-01, 0.0000e+00, 5.7898e-01],\n",
      "        [0.0000e+00, 5.2684e-01, 4.7316e-01],\n",
      "        [8.2510e-02, 9.1749e-01, 0.0000e+00],\n",
      "        [6.0382e-01, 0.0000e+00, 3.9618e-01],\n",
      "        [4.8492e-01, 0.0000e+00, 5.1508e-01],\n",
      "        [8.3761e-01, 0.0000e+00, 1.6239e-01],\n",
      "        [0.0000e+00, 5.4326e-02, 9.4567e-01],\n",
      "        [9.2246e-01, 0.0000e+00, 7.7541e-02],\n",
      "        [6.1800e-01, 3.8200e-01, 0.0000e+00],\n",
      "        [3.8951e-01, 0.0000e+00, 6.1049e-01],\n",
      "        [0.0000e+00, 3.9115e-01, 6.0885e-01],\n",
      "        [0.0000e+00, 9.9321e-01, 6.7852e-03],\n",
      "        [2.7475e-01, 0.0000e+00, 7.2525e-01],\n",
      "        [4.9511e-02, 0.0000e+00, 9.5049e-01],\n",
      "        [1.0400e-01, 0.0000e+00, 8.9600e-01],\n",
      "        [9.6609e-02, 0.0000e+00, 9.0339e-01],\n",
      "        [4.4394e-02, 9.5561e-01, 0.0000e+00],\n",
      "        [3.5452e-01, 0.0000e+00, 6.4548e-01],\n",
      "        [0.0000e+00, 3.4861e-02, 9.6514e-01],\n",
      "        [0.0000e+00, 8.6764e-04, 9.9913e-01],\n",
      "        [0.0000e+00, 9.9630e-01, 3.7038e-03],\n",
      "        [7.7983e-01, 0.0000e+00, 2.2017e-01],\n",
      "        [0.0000e+00, 9.0933e-01, 9.0667e-02],\n",
      "        [8.2338e-01, 0.0000e+00, 1.7662e-01],\n",
      "        [0.0000e+00, 3.1116e-01, 6.8884e-01],\n",
      "        [7.6151e-01, 2.3849e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 9.6296e-01, 3.7043e-02],\n",
      "        [8.7020e-01, 0.0000e+00, 1.2980e-01],\n",
      "        [0.0000e+00, 3.4617e-01, 6.5383e-01],\n",
      "        [3.6734e-01, 0.0000e+00, 6.3266e-01],\n",
      "        [1.1418e-01, 0.0000e+00, 8.8582e-01],\n",
      "        [0.0000e+00, 5.9799e-01, 4.0201e-01],\n",
      "        [1.7345e-01, 8.2655e-01, 0.0000e+00],\n",
      "        [8.3499e-01, 1.6501e-01, 0.0000e+00],\n",
      "        [6.2387e-01, 0.0000e+00, 3.7613e-01],\n",
      "        [5.8378e-01, 0.0000e+00, 4.1622e-01],\n",
      "        [5.8797e-01, 0.0000e+00, 4.1203e-01],\n",
      "        [3.7281e-01, 0.0000e+00, 6.2719e-01],\n",
      "        [6.1892e-01, 0.0000e+00, 3.8108e-01],\n",
      "        [0.0000e+00, 1.9372e-01, 8.0628e-01],\n",
      "        [0.0000e+00, 9.5668e-01, 4.3317e-02],\n",
      "        [9.2914e-01, 0.0000e+00, 7.0859e-02],\n",
      "        [0.0000e+00, 6.7387e-01, 3.2613e-01],\n",
      "        [0.0000e+00, 5.1559e-01, 4.8441e-01],\n",
      "        [1.3640e-01, 8.6360e-01, 0.0000e+00],\n",
      "        [8.8889e-01, 0.0000e+00, 1.1111e-01],\n",
      "        [8.4321e-01, 0.0000e+00, 1.5679e-01],\n",
      "        [5.8830e-01, 0.0000e+00, 4.1170e-01],\n",
      "        [0.0000e+00, 4.2811e-01, 5.7189e-01],\n",
      "        [0.0000e+00, 9.8018e-01, 1.9816e-02],\n",
      "        [8.2129e-01, 0.0000e+00, 1.7871e-01],\n",
      "        [5.0066e-02, 9.4993e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 4.1634e-01, 5.8366e-01],\n",
      "        [3.7319e-01, 0.0000e+00, 6.2681e-01],\n",
      "        [4.2590e-01, 0.0000e+00, 5.7410e-01],\n",
      "        [0.0000e+00, 2.6424e-01, 7.3576e-01],\n",
      "        [6.3443e-01, 0.0000e+00, 3.6557e-01],\n",
      "        [7.6137e-01, 0.0000e+00, 2.3863e-01],\n",
      "        [0.0000e+00, 5.4460e-02, 9.4554e-01],\n",
      "        [4.5198e-01, 0.0000e+00, 5.4802e-01],\n",
      "        [8.9795e-01, 1.0205e-01, 0.0000e+00],\n",
      "        [5.2069e-01, 4.7931e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 3.4670e-01, 6.5330e-01],\n",
      "        [8.3495e-01, 1.6505e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 4.4526e-01, 5.5474e-01],\n",
      "        [9.7072e-01, 2.9276e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 2.8819e-01, 7.1181e-01],\n",
      "        [2.6041e-01, 7.3959e-01, 0.0000e+00],\n",
      "        [4.6013e-01, 0.0000e+00, 5.3987e-01],\n",
      "        [8.5462e-01, 1.4538e-01, 0.0000e+00],\n",
      "        [3.5276e-01, 6.4724e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 9.2229e-01, 7.7708e-02],\n",
      "        [3.0808e-01, 0.0000e+00, 6.9192e-01],\n",
      "        [0.0000e+00, 5.8343e-01, 4.1657e-01],\n",
      "        [5.1156e-01, 0.0000e+00, 4.8844e-01],\n",
      "        [3.5136e-01, 0.0000e+00, 6.4864e-01],\n",
      "        [1.9631e-01, 0.0000e+00, 8.0369e-01],\n",
      "        [9.8838e-01, 0.0000e+00, 1.1618e-02],\n",
      "        [9.9338e-01, 0.0000e+00, 6.6189e-03],\n",
      "        [0.0000e+00, 9.1487e-01, 8.5128e-02],\n",
      "        [0.0000e+00, 1.6541e-01, 8.3459e-01],\n",
      "        [1.6576e-01, 8.3424e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1935e-01, 8.8065e-01],\n",
      "        [8.6835e-01, 0.0000e+00, 1.3165e-01],\n",
      "        [8.3715e-01, 0.0000e+00, 1.6285e-01],\n",
      "        [0.0000e+00, 2.7267e-01, 7.2733e-01],\n",
      "        [4.8613e-01, 0.0000e+00, 5.1387e-01],\n",
      "        [2.0012e-01, 0.0000e+00, 7.9988e-01],\n",
      "        [8.6305e-01, 0.0000e+00, 1.3695e-01],\n",
      "        [2.0489e-01, 0.0000e+00, 7.9511e-01],\n",
      "        [7.6590e-02, 0.0000e+00, 9.2341e-01]])\n",
      "error data:tensor([[[[ 1.7972e+02,  1.8975e+02,  8.6991e+01,  ...,  1.4954e+02,\n",
      "            7.8526e+01, -1.2683e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.3174e+03, -2.8943e+03, -6.9981e+02,  ..., -1.6277e+03,\n",
      "           -8.0104e+02, -1.7104e+03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0790e+03, -7.0651e+03, -1.0037e+04,  ...,  7.4557e+03,\n",
      "           -6.0859e+03, -1.4077e+03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.4004e+01,  2.0902e+01, -2.3470e+00,  ..., -6.2834e+01,\n",
      "           -1.7031e+01,  1.7348e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.0548e+02, -1.9617e+02, -1.4994e+02,  ...,  2.0481e+02,\n",
      "           -6.7347e+01,  6.1856e+02]]],\n",
      "\n",
      "\n",
      "        [[[-5.2375e+02,  1.4744e+02,  5.2845e+02,  ..., -9.6766e-01,\n",
      "           -6.9133e-01, -1.8972e-01]]]])\n",
      "error label:tensor([[0.0000, 0.5231, 0.4769],\n",
      "        [0.1424, 0.8576, 0.0000],\n",
      "        [0.7840, 0.2160, 0.0000],\n",
      "        [0.1189, 0.8811, 0.0000],\n",
      "        [0.9201, 0.0799, 0.0000],\n",
      "        [0.6478, 0.0000, 0.3522],\n",
      "        [0.0000, 0.2368, 0.7632],\n",
      "        [0.5042, 0.0000, 0.4958],\n",
      "        [0.0000, 0.9813, 0.0187],\n",
      "        [0.0000, 0.4803, 0.5197],\n",
      "        [0.0000, 0.8843, 0.1157],\n",
      "        [0.0000, 0.7094, 0.2906],\n",
      "        [0.8207, 0.1793, 0.0000],\n",
      "        [0.8808, 0.0000, 0.1192],\n",
      "        [0.6614, 0.0000, 0.3386],\n",
      "        [0.2352, 0.0000, 0.7648],\n",
      "        [0.0000, 0.3968, 0.6032],\n",
      "        [0.5027, 0.4973, 0.0000],\n",
      "        [0.3270, 0.0000, 0.6730],\n",
      "        [0.5078, 0.0000, 0.4922],\n",
      "        [0.0000, 0.9526, 0.0474],\n",
      "        [0.5403, 0.0000, 0.4597],\n",
      "        [0.0000, 0.6875, 0.3125],\n",
      "        [0.5722, 0.0000, 0.4278],\n",
      "        [0.6169, 0.0000, 0.3831],\n",
      "        [0.9731, 0.0000, 0.0269],\n",
      "        [0.6612, 0.0000, 0.3388],\n",
      "        [0.0000, 0.3376, 0.6624],\n",
      "        [0.0986, 0.9014, 0.0000],\n",
      "        [0.8083, 0.1917, 0.0000],\n",
      "        [0.0000, 0.2995, 0.7005],\n",
      "        [0.0000, 0.2141, 0.7859],\n",
      "        [0.0000, 0.0920, 0.9080],\n",
      "        [0.4753, 0.0000, 0.5247],\n",
      "        [0.3134, 0.6866, 0.0000],\n",
      "        [0.0000, 0.8809, 0.1191],\n",
      "        [0.0000, 0.7321, 0.2679],\n",
      "        [0.7464, 0.0000, 0.2536],\n",
      "        [0.3540, 0.6460, 0.0000],\n",
      "        [0.0000, 0.7544, 0.2456],\n",
      "        [0.2103, 0.0000, 0.7897],\n",
      "        [0.2178, 0.0000, 0.7822],\n",
      "        [0.8968, 0.1032, 0.0000],\n",
      "        [0.1119, 0.0000, 0.8881],\n",
      "        [0.5253, 0.0000, 0.4747],\n",
      "        [0.8323, 0.1677, 0.0000],\n",
      "        [0.0000, 0.4395, 0.5605],\n",
      "        [0.6835, 0.0000, 0.3165],\n",
      "        [0.6454, 0.0000, 0.3546],\n",
      "        [0.0000, 0.8576, 0.1424],\n",
      "        [0.0000, 0.7896, 0.2104],\n",
      "        [0.0000, 0.7795, 0.2205],\n",
      "        [0.0000, 0.3015, 0.6985],\n",
      "        [0.0000, 0.6302, 0.3698],\n",
      "        [0.0000, 0.8422, 0.1578],\n",
      "        [0.4784, 0.0000, 0.5216],\n",
      "        [0.8013, 0.1987, 0.0000],\n",
      "        [0.0207, 0.0000, 0.9793],\n",
      "        [0.0000, 0.9502, 0.0498],\n",
      "        [0.0000, 0.8826, 0.1174],\n",
      "        [0.7120, 0.0000, 0.2880],\n",
      "        [0.7267, 0.0000, 0.2733],\n",
      "        [0.2355, 0.0000, 0.7645],\n",
      "        [0.0000, 0.4689, 0.5311],\n",
      "        [0.0000, 0.7438, 0.2562],\n",
      "        [0.0000, 0.1322, 0.8678],\n",
      "        [0.8779, 0.0000, 0.1221],\n",
      "        [0.0000, 0.9785, 0.0215],\n",
      "        [0.0000, 0.6129, 0.3871],\n",
      "        [0.0000, 0.7637, 0.2363],\n",
      "        [0.5298, 0.0000, 0.4702],\n",
      "        [0.4852, 0.0000, 0.5148],\n",
      "        [0.2501, 0.0000, 0.7499],\n",
      "        [0.0000, 0.4164, 0.5836],\n",
      "        [0.8780, 0.0000, 0.1220],\n",
      "        [0.6595, 0.3405, 0.0000],\n",
      "        [0.0000, 0.6224, 0.3776],\n",
      "        [0.6768, 0.0000, 0.3232],\n",
      "        [0.1780, 0.0000, 0.8220],\n",
      "        [0.7151, 0.0000, 0.2849],\n",
      "        [0.0000, 0.9037, 0.0963],\n",
      "        [0.1146, 0.0000, 0.8854],\n",
      "        [0.5659, 0.0000, 0.4341],\n",
      "        [0.6862, 0.0000, 0.3138],\n",
      "        [0.3004, 0.6996, 0.0000],\n",
      "        [0.0000, 0.1076, 0.8924],\n",
      "        [0.2549, 0.0000, 0.7451],\n",
      "        [0.1571, 0.0000, 0.8429],\n",
      "        [0.0000, 0.3762, 0.6238],\n",
      "        [0.5662, 0.4338, 0.0000],\n",
      "        [0.5949, 0.0000, 0.4051],\n",
      "        [0.0000, 0.9631, 0.0369],\n",
      "        [0.3040, 0.6960, 0.0000],\n",
      "        [0.8421, 0.1579, 0.0000],\n",
      "        [0.0000, 0.9782, 0.0218],\n",
      "        [0.5852, 0.0000, 0.4148],\n",
      "        [0.4957, 0.0000, 0.5043],\n",
      "        [0.0000, 0.2754, 0.7246],\n",
      "        [0.0000, 0.6841, 0.3159],\n",
      "        [0.0208, 0.0000, 0.9792],\n",
      "        [0.0000, 0.9091, 0.0909],\n",
      "        [0.6525, 0.3475, 0.0000],\n",
      "        [0.3329, 0.0000, 0.6671],\n",
      "        [0.6963, 0.3037, 0.0000],\n",
      "        [0.0000, 0.9680, 0.0320],\n",
      "        [0.7422, 0.0000, 0.2578],\n",
      "        [0.1466, 0.0000, 0.8534],\n",
      "        [0.2442, 0.0000, 0.7558],\n",
      "        [0.8709, 0.0000, 0.1291],\n",
      "        [0.8705, 0.1295, 0.0000],\n",
      "        [0.0000, 0.3413, 0.6587],\n",
      "        [0.2455, 0.7545, 0.0000],\n",
      "        [0.5412, 0.4588, 0.0000],\n",
      "        [0.0564, 0.0000, 0.9436],\n",
      "        [0.0000, 0.7592, 0.2408],\n",
      "        [0.4020, 0.0000, 0.5980],\n",
      "        [0.0000, 0.1730, 0.8270],\n",
      "        [0.3756, 0.0000, 0.6244],\n",
      "        [0.0000, 0.6100, 0.3900],\n",
      "        [0.5518, 0.0000, 0.4482],\n",
      "        [0.1678, 0.0000, 0.8322],\n",
      "        [0.0000, 0.5513, 0.4487],\n",
      "        [0.5656, 0.0000, 0.4344],\n",
      "        [0.0000, 0.4272, 0.5728],\n",
      "        [0.5457, 0.4543, 0.0000],\n",
      "        [0.3693, 0.0000, 0.6307],\n",
      "        [0.0000, 0.9310, 0.0690],\n",
      "        [0.8706, 0.0000, 0.1294]])\n",
      "error data:tensor([[[[ 1.8161e+02,  4.7327e+01,  3.5310e+01,  ..., -1.1268e+02,\n",
      "           -1.1173e+02, -1.7313e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9598e+02,  3.1542e+02,  3.5272e+02,  ..., -2.7980e+02,\n",
      "           -2.7979e+02, -2.7980e+02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1397e+02, -1.1766e+02, -1.9743e+02,  ..., -1.5410e+03,\n",
      "            5.9423e+02, -1.0385e+03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.3598e+03,  2.9345e+03,  5.4950e+03,  ..., -6.7714e+03,\n",
      "           -8.2503e+03, -5.2511e+03]]],\n",
      "\n",
      "\n",
      "        [[[-3.9951e+02, -1.6633e+03, -2.5922e+03,  ..., -6.6097e+02,\n",
      "           -1.1355e+02,  3.6627e+02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8163e+04,  1.4794e+04,  3.7067e+04,  ..., -7.9452e+03,\n",
      "           -2.8004e+04, -1.3104e+04]]]])\n",
      "error data:tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.3801e-01,\n",
      "           -4.4333e-01, -7.7240e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.3636e-01, -4.2615e-01, -1.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.9509e-03,\n",
      "           -5.6152e-03,  5.7983e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.0782e-02,  2.2797e-02,  1.8707e-02,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.8452e-03,\n",
      "            3.9978e-03,  4.5471e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5989e-03,  1.7792e-02,  5.2094e-02,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "__chk_bestAcc is called\n",
      "current best Acc is 0.0\n",
      "pass in acc is 0.0\n",
      "SP-[1] Epoch: 1/800 | Time: 0m04s (Train 0m04s  Val 0m00s) | Train: LR 0.010000000000000002  Loss nan  Acc 8.20% | Val: Loss 0.00  Acc(top1) 0.00% | HA 0.00@0\n",
      "error label:tensor([[0.8432, 0.0000, 0.1568],\n",
      "        [0.2896, 0.0000, 0.7104],\n",
      "        [0.0280, 0.0000, 0.9720],\n",
      "        [0.0021, 0.9979, 0.0000],\n",
      "        [0.0000, 0.1655, 0.8345],\n",
      "        [0.3872, 0.6128, 0.0000],\n",
      "        [0.7072, 0.0000, 0.2928],\n",
      "        [0.0000, 0.9552, 0.0448],\n",
      "        [0.1203, 0.0000, 0.8797],\n",
      "        [0.0000, 0.5889, 0.4111],\n",
      "        [0.3174, 0.0000, 0.6826],\n",
      "        [0.8458, 0.0000, 0.1542],\n",
      "        [0.0000, 0.9013, 0.0987],\n",
      "        [0.7635, 0.0000, 0.2365],\n",
      "        [0.4842, 0.5158, 0.0000],\n",
      "        [0.2370, 0.7630, 0.0000],\n",
      "        [0.5004, 0.4996, 0.0000],\n",
      "        [0.9526, 0.0000, 0.0474],\n",
      "        [0.1605, 0.8395, 0.0000],\n",
      "        [0.0000, 0.5631, 0.4369],\n",
      "        [0.3896, 0.0000, 0.6104],\n",
      "        [0.0000, 0.2459, 0.7541],\n",
      "        [0.0000, 0.8663, 0.1337],\n",
      "        [0.3928, 0.0000, 0.6072],\n",
      "        [0.9720, 0.0000, 0.0280],\n",
      "        [0.8752, 0.0000, 0.1248],\n",
      "        [0.0000, 0.9214, 0.0786],\n",
      "        [0.9673, 0.0000, 0.0327],\n",
      "        [0.4309, 0.5691, 0.0000],\n",
      "        [0.0000, 0.4349, 0.5651],\n",
      "        [0.7289, 0.2711, 0.0000],\n",
      "        [0.0020, 0.0000, 0.9980],\n",
      "        [0.5183, 0.4817, 0.0000],\n",
      "        [0.4676, 0.0000, 0.5324],\n",
      "        [0.0000, 0.7744, 0.2256],\n",
      "        [0.6253, 0.3747, 0.0000],\n",
      "        [0.7496, 0.0000, 0.2504],\n",
      "        [0.0000, 0.9435, 0.0565],\n",
      "        [0.9167, 0.0000, 0.0833],\n",
      "        [0.1018, 0.0000, 0.8982],\n",
      "        [0.2331, 0.7669, 0.0000],\n",
      "        [0.3808, 0.0000, 0.6192],\n",
      "        [0.0123, 0.0000, 0.9877],\n",
      "        [0.0000, 0.6446, 0.3554],\n",
      "        [0.0000, 0.5837, 0.4163],\n",
      "        [0.0000, 0.4823, 0.5177],\n",
      "        [0.0000, 0.9765, 0.0235],\n",
      "        [0.4002, 0.5998, 0.0000],\n",
      "        [0.0000, 0.4959, 0.5041],\n",
      "        [0.0000, 0.1739, 0.8261],\n",
      "        [0.3687, 0.0000, 0.6313],\n",
      "        [0.7811, 0.0000, 0.2189],\n",
      "        [0.1761, 0.0000, 0.8239],\n",
      "        [0.3051, 0.0000, 0.6949],\n",
      "        [0.5663, 0.0000, 0.4337],\n",
      "        [0.5995, 0.4005, 0.0000],\n",
      "        [0.2104, 0.7896, 0.0000],\n",
      "        [0.8858, 0.0000, 0.1142],\n",
      "        [0.3881, 0.0000, 0.6119],\n",
      "        [0.2844, 0.0000, 0.7156],\n",
      "        [0.0000, 0.2072, 0.7928],\n",
      "        [0.0000, 0.4286, 0.5714],\n",
      "        [0.0000, 0.5555, 0.4445],\n",
      "        [0.5675, 0.0000, 0.4325],\n",
      "        [0.0000, 0.7344, 0.2656],\n",
      "        [0.7859, 0.2141, 0.0000],\n",
      "        [0.2500, 0.0000, 0.7500],\n",
      "        [0.9915, 0.0000, 0.0085],\n",
      "        [0.2269, 0.0000, 0.7731],\n",
      "        [0.0000, 0.1856, 0.8144],\n",
      "        [0.2096, 0.7904, 0.0000],\n",
      "        [0.0566, 0.0000, 0.9434],\n",
      "        [0.0000, 0.5376, 0.4624],\n",
      "        [0.5682, 0.0000, 0.4318],\n",
      "        [0.0000, 0.3288, 0.6712],\n",
      "        [0.1735, 0.8265, 0.0000],\n",
      "        [0.2911, 0.7089, 0.0000],\n",
      "        [0.6089, 0.0000, 0.3911],\n",
      "        [0.1299, 0.0000, 0.8701],\n",
      "        [0.7168, 0.0000, 0.2832],\n",
      "        [0.3952, 0.6048, 0.0000],\n",
      "        [0.0000, 0.7373, 0.2627],\n",
      "        [0.0000, 0.2491, 0.7509],\n",
      "        [0.7603, 0.0000, 0.2397],\n",
      "        [0.0000, 0.7265, 0.2735],\n",
      "        [0.2335, 0.0000, 0.7665],\n",
      "        [0.9268, 0.0000, 0.0732],\n",
      "        [0.0000, 0.3271, 0.6729],\n",
      "        [0.0000, 0.1584, 0.8416],\n",
      "        [0.1685, 0.0000, 0.8315],\n",
      "        [0.0344, 0.9656, 0.0000],\n",
      "        [0.0000, 0.6410, 0.3590],\n",
      "        [0.0487, 0.0000, 0.9513],\n",
      "        [0.0000, 0.6629, 0.3371],\n",
      "        [0.1679, 0.8321, 0.0000],\n",
      "        [0.9343, 0.0657, 0.0000],\n",
      "        [0.9850, 0.0000, 0.0150],\n",
      "        [0.6499, 0.0000, 0.3501],\n",
      "        [0.7125, 0.2875, 0.0000],\n",
      "        [0.3037, 0.0000, 0.6963],\n",
      "        [0.4821, 0.5179, 0.0000],\n",
      "        [0.0000, 0.2411, 0.7589],\n",
      "        [0.0000, 0.3745, 0.6255],\n",
      "        [0.9886, 0.0000, 0.0114],\n",
      "        [0.4791, 0.5209, 0.0000],\n",
      "        [0.4607, 0.5393, 0.0000],\n",
      "        [0.0000, 0.9300, 0.0700],\n",
      "        [0.4512, 0.0000, 0.5488],\n",
      "        [0.7551, 0.0000, 0.2449],\n",
      "        [0.0000, 0.7813, 0.2187],\n",
      "        [0.5666, 0.0000, 0.4334],\n",
      "        [0.4048, 0.0000, 0.5952],\n",
      "        [0.5621, 0.4379, 0.0000],\n",
      "        [0.9958, 0.0000, 0.0042],\n",
      "        [0.0000, 0.3848, 0.6152],\n",
      "        [0.0000, 0.6350, 0.3650],\n",
      "        [0.9215, 0.0000, 0.0785],\n",
      "        [0.0000, 0.6021, 0.3979],\n",
      "        [0.1703, 0.0000, 0.8297],\n",
      "        [0.0000, 0.3080, 0.6920],\n",
      "        [0.0064, 0.9936, 0.0000],\n",
      "        [0.2670, 0.0000, 0.7330],\n",
      "        [0.0000, 0.0515, 0.9485],\n",
      "        [0.5593, 0.4407, 0.0000],\n",
      "        [0.0297, 0.9703, 0.0000],\n",
      "        [0.3347, 0.0000, 0.6653],\n",
      "        [0.0000, 0.2658, 0.7342],\n",
      "        [0.7252, 0.0000, 0.2748]])\n",
      "error data:tensor([[[[-4049.7920,  -119.3589,  4431.8794,  ..., -2190.4988,\n",
      "            -376.2988,  1521.6381]]],\n",
      "\n",
      "\n",
      "        [[[ 5435.2817,  3741.0024,  1345.4597,  ...,  4430.2739,\n",
      "            3636.8569,  3268.1531]]],\n",
      "\n",
      "\n",
      "        [[[  317.9238,   374.5807,   323.9684,  ...,   817.6220,\n",
      "             888.8926,   925.7196]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2389.2151, -2233.2485,  -484.3408,  ..., 10238.4863,\n",
      "           10918.5947,  8346.1631]]],\n",
      "\n",
      "\n",
      "        [[[-4654.7598, -5358.7881, -5890.5513,  ...,    30.8588,\n",
      "              54.7775,    52.2680]]],\n",
      "\n",
      "\n",
      "        [[[ -607.0385, -2895.0598, -3397.1958,  ...,  3336.3022,\n",
      "             242.4303, -1577.2977]]]])\n",
      "error label:tensor([[1.8495e-01, 0.0000e+00, 8.1505e-01],\n",
      "        [1.2657e-01, 8.7343e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 7.9396e-01, 2.0604e-01],\n",
      "        [8.5684e-01, 0.0000e+00, 1.4316e-01],\n",
      "        [5.3320e-01, 0.0000e+00, 4.6680e-01],\n",
      "        [7.5325e-01, 0.0000e+00, 2.4675e-01],\n",
      "        [7.1964e-01, 0.0000e+00, 2.8036e-01],\n",
      "        [9.0956e-01, 0.0000e+00, 9.0442e-02],\n",
      "        [0.0000e+00, 3.7701e-01, 6.2299e-01],\n",
      "        [0.0000e+00, 9.1086e-01, 8.9139e-02],\n",
      "        [4.2308e-01, 5.7692e-01, 0.0000e+00],\n",
      "        [7.7128e-01, 0.0000e+00, 2.2872e-01],\n",
      "        [9.1885e-03, 0.0000e+00, 9.9081e-01],\n",
      "        [3.2002e-01, 6.7998e-01, 0.0000e+00],\n",
      "        [4.9762e-01, 5.0238e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 7.0414e-01, 2.9586e-01],\n",
      "        [7.7493e-01, 0.0000e+00, 2.2507e-01],\n",
      "        [9.1238e-01, 0.0000e+00, 8.7624e-02],\n",
      "        [7.5771e-01, 0.0000e+00, 2.4229e-01],\n",
      "        [0.0000e+00, 6.0623e-01, 3.9377e-01],\n",
      "        [4.2284e-01, 0.0000e+00, 5.7716e-01],\n",
      "        [0.0000e+00, 8.0434e-01, 1.9566e-01],\n",
      "        [9.8028e-01, 0.0000e+00, 1.9723e-02],\n",
      "        [3.9664e-01, 0.0000e+00, 6.0336e-01],\n",
      "        [3.2926e-01, 0.0000e+00, 6.7074e-01],\n",
      "        [0.0000e+00, 8.2637e-02, 9.1736e-01],\n",
      "        [0.0000e+00, 7.2628e-01, 2.7372e-01],\n",
      "        [4.2858e-01, 0.0000e+00, 5.7142e-01],\n",
      "        [7.8347e-01, 0.0000e+00, 2.1653e-01],\n",
      "        [6.5170e-01, 0.0000e+00, 3.4830e-01],\n",
      "        [4.4528e-01, 5.5472e-01, 0.0000e+00],\n",
      "        [8.5145e-01, 0.0000e+00, 1.4855e-01],\n",
      "        [8.5564e-05, 0.0000e+00, 9.9991e-01],\n",
      "        [0.0000e+00, 3.0490e-01, 6.9510e-01],\n",
      "        [8.5833e-01, 1.4167e-01, 0.0000e+00],\n",
      "        [8.7151e-01, 1.2849e-01, 0.0000e+00],\n",
      "        [4.0006e-01, 0.0000e+00, 5.9994e-01],\n",
      "        [0.0000e+00, 6.2957e-01, 3.7043e-01],\n",
      "        [0.0000e+00, 3.9506e-01, 6.0494e-01],\n",
      "        [2.4750e-01, 7.5250e-01, 0.0000e+00],\n",
      "        [6.3718e-01, 0.0000e+00, 3.6282e-01],\n",
      "        [6.2284e-01, 3.7716e-01, 0.0000e+00],\n",
      "        [4.1117e-01, 0.0000e+00, 5.8883e-01],\n",
      "        [2.7105e-01, 0.0000e+00, 7.2895e-01],\n",
      "        [0.0000e+00, 5.5167e-01, 4.4833e-01],\n",
      "        [8.7363e-01, 0.0000e+00, 1.2637e-01],\n",
      "        [3.2178e-01, 6.7822e-01, 0.0000e+00],\n",
      "        [7.1318e-01, 0.0000e+00, 2.8682e-01],\n",
      "        [0.0000e+00, 2.2942e-01, 7.7058e-01],\n",
      "        [4.2468e-01, 0.0000e+00, 5.7532e-01],\n",
      "        [0.0000e+00, 6.7703e-01, 3.2297e-01],\n",
      "        [5.3241e-01, 0.0000e+00, 4.6759e-01],\n",
      "        [5.3432e-01, 0.0000e+00, 4.6568e-01],\n",
      "        [5.2352e-01, 0.0000e+00, 4.7648e-01],\n",
      "        [4.2272e-01, 0.0000e+00, 5.7728e-01],\n",
      "        [1.4922e-01, 0.0000e+00, 8.5078e-01],\n",
      "        [0.0000e+00, 6.0638e-01, 3.9362e-01],\n",
      "        [9.4250e-01, 0.0000e+00, 5.7497e-02],\n",
      "        [0.0000e+00, 8.1384e-01, 1.8616e-01],\n",
      "        [4.1072e-01, 0.0000e+00, 5.8928e-01],\n",
      "        [0.0000e+00, 8.9131e-01, 1.0869e-01],\n",
      "        [8.8933e-01, 0.0000e+00, 1.1067e-01],\n",
      "        [6.6620e-01, 0.0000e+00, 3.3380e-01],\n",
      "        [2.8568e-01, 7.1432e-01, 0.0000e+00],\n",
      "        [8.4446e-01, 0.0000e+00, 1.5554e-01],\n",
      "        [2.6257e-01, 7.3743e-01, 0.0000e+00],\n",
      "        [8.1314e-01, 0.0000e+00, 1.8686e-01],\n",
      "        [1.9841e-01, 8.0159e-01, 0.0000e+00],\n",
      "        [6.2891e-01, 0.0000e+00, 3.7109e-01],\n",
      "        [9.8234e-01, 0.0000e+00, 1.7661e-02],\n",
      "        [2.2178e-01, 0.0000e+00, 7.7822e-01],\n",
      "        [3.6358e-01, 0.0000e+00, 6.3642e-01],\n",
      "        [0.0000e+00, 9.4787e-01, 5.2131e-02],\n",
      "        [0.0000e+00, 5.1672e-02, 9.4833e-01],\n",
      "        [1.3803e-01, 8.6197e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1281e-01, 8.8719e-01],\n",
      "        [7.4499e-02, 0.0000e+00, 9.2550e-01],\n",
      "        [0.0000e+00, 9.5619e-01, 4.3811e-02],\n",
      "        [0.0000e+00, 2.4874e-02, 9.7513e-01],\n",
      "        [7.9089e-01, 0.0000e+00, 2.0911e-01],\n",
      "        [2.5928e-01, 7.4072e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 4.0789e-01, 5.9211e-01],\n",
      "        [0.0000e+00, 2.9184e-01, 7.0816e-01],\n",
      "        [1.0464e-02, 9.8954e-01, 0.0000e+00],\n",
      "        [5.9166e-01, 4.0834e-01, 0.0000e+00],\n",
      "        [2.6952e-01, 7.3048e-01, 0.0000e+00],\n",
      "        [4.1674e-01, 5.8326e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.8315e-01, 8.1685e-01],\n",
      "        [8.5167e-01, 0.0000e+00, 1.4833e-01],\n",
      "        [4.3034e-01, 0.0000e+00, 5.6966e-01],\n",
      "        [6.8737e-01, 3.1263e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 2.7569e-01, 7.2431e-01],\n",
      "        [1.3552e-02, 0.0000e+00, 9.8645e-01],\n",
      "        [3.3639e-02, 9.6636e-01, 0.0000e+00],\n",
      "        [2.0232e-01, 0.0000e+00, 7.9768e-01],\n",
      "        [6.9204e-01, 3.0796e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 9.9879e-01, 1.2104e-03],\n",
      "        [0.0000e+00, 9.2169e-01, 7.8309e-02],\n",
      "        [3.5731e-01, 0.0000e+00, 6.4269e-01],\n",
      "        [6.0451e-01, 0.0000e+00, 3.9549e-01],\n",
      "        [6.9844e-01, 0.0000e+00, 3.0156e-01],\n",
      "        [1.8928e-01, 0.0000e+00, 8.1072e-01],\n",
      "        [4.2216e-01, 5.7784e-01, 0.0000e+00],\n",
      "        [6.4739e-01, 3.5261e-01, 0.0000e+00],\n",
      "        [4.9260e-01, 5.0740e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 8.1129e-01, 1.8871e-01],\n",
      "        [3.6240e-01, 6.3760e-01, 0.0000e+00],\n",
      "        [8.3525e-01, 0.0000e+00, 1.6475e-01],\n",
      "        [0.0000e+00, 8.8276e-01, 1.1724e-01],\n",
      "        [5.4083e-02, 0.0000e+00, 9.4592e-01],\n",
      "        [8.5872e-01, 0.0000e+00, 1.4128e-01],\n",
      "        [4.6976e-01, 0.0000e+00, 5.3024e-01],\n",
      "        [0.0000e+00, 3.0475e-01, 6.9525e-01],\n",
      "        [8.3268e-01, 0.0000e+00, 1.6732e-01],\n",
      "        [0.0000e+00, 2.7183e-01, 7.2817e-01],\n",
      "        [6.0771e-01, 0.0000e+00, 3.9229e-01],\n",
      "        [1.6482e-01, 0.0000e+00, 8.3518e-01],\n",
      "        [0.0000e+00, 2.2120e-01, 7.7880e-01],\n",
      "        [9.0969e-01, 0.0000e+00, 9.0313e-02],\n",
      "        [9.5513e-01, 4.4875e-02, 0.0000e+00],\n",
      "        [1.9458e-02, 0.0000e+00, 9.8054e-01],\n",
      "        [0.0000e+00, 2.3685e-02, 9.7631e-01],\n",
      "        [9.3291e-02, 9.0671e-01, 0.0000e+00],\n",
      "        [4.8370e-01, 5.1630e-01, 0.0000e+00],\n",
      "        [9.5999e-01, 0.0000e+00, 4.0013e-02],\n",
      "        [0.0000e+00, 4.3310e-01, 5.6690e-01],\n",
      "        [3.1372e-01, 6.8628e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 7.7194e-01, 2.2806e-01]])\n",
      "error data:tensor([[[[ 5.2511e+02,  3.2935e+02,  4.0583e+02,  ..., -3.5791e+02,\n",
      "           -7.1790e+02, -2.0384e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3366e+01,  2.1338e+00, -1.0258e+01,  ..., -1.7821e+02,\n",
      "           -6.9369e+02, -1.6100e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5651e+02,  6.7999e+02,  5.6817e+02,  ..., -8.5839e+01,\n",
      "           -1.1083e+02, -1.2490e+02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4571e+03,  1.3999e+03,  1.2146e+03,  ...,  1.6052e+03,\n",
      "            2.3640e+03,  2.4368e+03]]],\n",
      "\n",
      "\n",
      "        [[[-1.9019e+02, -1.8248e+02,  9.8811e+01,  ..., -1.1672e+03,\n",
      "           -1.6964e+03, -2.0525e+03]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3611e+02,  7.0813e+02,  7.7125e+02,  ...,  2.3143e+03,\n",
      "            2.4113e+03,  2.3030e+03]]]])\n",
      "error label:tensor([[0.0000, 0.0602, 0.9398],\n",
      "        [0.0829, 0.0000, 0.9171],\n",
      "        [0.0000, 0.7372, 0.2628],\n",
      "        [0.6997, 0.0000, 0.3003],\n",
      "        [0.0000, 0.3882, 0.6118],\n",
      "        [0.1091, 0.0000, 0.8909],\n",
      "        [0.3173, 0.0000, 0.6827],\n",
      "        [0.0000, 0.9617, 0.0383],\n",
      "        [0.2258, 0.0000, 0.7742],\n",
      "        [0.0000, 0.7302, 0.2698],\n",
      "        [0.8713, 0.1287, 0.0000],\n",
      "        [0.3112, 0.6888, 0.0000],\n",
      "        [0.6165, 0.0000, 0.3835],\n",
      "        [0.1357, 0.8643, 0.0000],\n",
      "        [0.0000, 0.1675, 0.8325],\n",
      "        [0.9717, 0.0000, 0.0283],\n",
      "        [0.5021, 0.0000, 0.4979],\n",
      "        [0.7150, 0.0000, 0.2850],\n",
      "        [0.0000, 0.8372, 0.1628],\n",
      "        [0.6238, 0.0000, 0.3762],\n",
      "        [0.5516, 0.4484, 0.0000],\n",
      "        [0.2962, 0.0000, 0.7038],\n",
      "        [0.2645, 0.0000, 0.7355],\n",
      "        [0.0000, 0.8466, 0.1534],\n",
      "        [0.0000, 0.5557, 0.4443],\n",
      "        [0.4031, 0.0000, 0.5969],\n",
      "        [0.5769, 0.0000, 0.4231],\n",
      "        [0.0000, 0.9067, 0.0933],\n",
      "        [0.9206, 0.0000, 0.0794],\n",
      "        [0.3685, 0.0000, 0.6315],\n",
      "        [0.0000, 0.7395, 0.2605],\n",
      "        [0.8773, 0.0000, 0.1227],\n",
      "        [0.7920, 0.2080, 0.0000],\n",
      "        [0.3753, 0.0000, 0.6247],\n",
      "        [0.4327, 0.0000, 0.5673],\n",
      "        [0.0000, 0.9770, 0.0230],\n",
      "        [0.3275, 0.6725, 0.0000],\n",
      "        [0.7081, 0.2919, 0.0000],\n",
      "        [0.0000, 0.7189, 0.2811],\n",
      "        [0.5763, 0.4237, 0.0000],\n",
      "        [0.0000, 0.3529, 0.6471],\n",
      "        [0.5869, 0.0000, 0.4131],\n",
      "        [0.6423, 0.0000, 0.3577],\n",
      "        [0.3289, 0.6711, 0.0000],\n",
      "        [0.8971, 0.0000, 0.1029],\n",
      "        [0.6822, 0.0000, 0.3178],\n",
      "        [0.5788, 0.4212, 0.0000],\n",
      "        [0.2850, 0.0000, 0.7150],\n",
      "        [0.9630, 0.0000, 0.0370],\n",
      "        [0.1701, 0.0000, 0.8299],\n",
      "        [0.0000, 0.6123, 0.3877],\n",
      "        [0.8255, 0.0000, 0.1745],\n",
      "        [0.0000, 0.8161, 0.1839],\n",
      "        [0.3121, 0.6879, 0.0000],\n",
      "        [0.0000, 0.8737, 0.1263],\n",
      "        [0.5099, 0.0000, 0.4901],\n",
      "        [0.0000, 0.4415, 0.5585],\n",
      "        [0.3295, 0.6705, 0.0000],\n",
      "        [0.0000, 0.2384, 0.7616],\n",
      "        [0.0833, 0.0000, 0.9167],\n",
      "        [0.0000, 0.8358, 0.1642],\n",
      "        [0.9834, 0.0000, 0.0166],\n",
      "        [0.8817, 0.1183, 0.0000],\n",
      "        [0.5587, 0.4413, 0.0000],\n",
      "        [0.4027, 0.0000, 0.5973],\n",
      "        [0.7804, 0.0000, 0.2196],\n",
      "        [0.2265, 0.0000, 0.7735],\n",
      "        [0.5048, 0.0000, 0.4952],\n",
      "        [0.0000, 0.1851, 0.8149],\n",
      "        [0.1418, 0.8582, 0.0000],\n",
      "        [0.3208, 0.0000, 0.6792],\n",
      "        [0.0174, 0.0000, 0.9826],\n",
      "        [0.0000, 0.2573, 0.7427],\n",
      "        [0.8570, 0.0000, 0.1430],\n",
      "        [0.9786, 0.0214, 0.0000],\n",
      "        [0.3594, 0.0000, 0.6406],\n",
      "        [0.0000, 0.5449, 0.4551],\n",
      "        [0.6090, 0.0000, 0.3910],\n",
      "        [0.5665, 0.4335, 0.0000],\n",
      "        [0.0000, 0.8529, 0.1471],\n",
      "        [0.0000, 0.7117, 0.2883],\n",
      "        [0.0532, 0.0000, 0.9468],\n",
      "        [0.3347, 0.6653, 0.0000],\n",
      "        [0.7092, 0.2908, 0.0000],\n",
      "        [0.0000, 0.0331, 0.9669],\n",
      "        [0.0000, 0.1956, 0.8044],\n",
      "        [0.5667, 0.0000, 0.4333],\n",
      "        [0.5581, 0.0000, 0.4419],\n",
      "        [0.6152, 0.0000, 0.3848],\n",
      "        [0.0000, 0.5656, 0.4344],\n",
      "        [0.1251, 0.0000, 0.8749],\n",
      "        [0.0974, 0.9026, 0.0000],\n",
      "        [0.0000, 0.4892, 0.5108],\n",
      "        [0.0000, 0.8523, 0.1477],\n",
      "        [0.9329, 0.0671, 0.0000],\n",
      "        [0.8327, 0.0000, 0.1673],\n",
      "        [0.0000, 0.8477, 0.1523],\n",
      "        [0.1814, 0.0000, 0.8186],\n",
      "        [0.2900, 0.0000, 0.7100],\n",
      "        [0.0000, 0.5831, 0.4169],\n",
      "        [0.3028, 0.6972, 0.0000],\n",
      "        [0.2769, 0.0000, 0.7231],\n",
      "        [0.0000, 0.8255, 0.1745],\n",
      "        [0.9318, 0.0000, 0.0682],\n",
      "        [0.2592, 0.0000, 0.7408],\n",
      "        [0.6763, 0.3237, 0.0000],\n",
      "        [0.2442, 0.7558, 0.0000],\n",
      "        [0.0000, 0.2366, 0.7634],\n",
      "        [0.4262, 0.5738, 0.0000],\n",
      "        [0.0000, 0.1621, 0.8379],\n",
      "        [0.7965, 0.2035, 0.0000],\n",
      "        [0.3407, 0.0000, 0.6593],\n",
      "        [0.1260, 0.0000, 0.8740],\n",
      "        [0.6015, 0.0000, 0.3985],\n",
      "        [0.0000, 0.9681, 0.0319],\n",
      "        [0.0000, 0.9434, 0.0566],\n",
      "        [0.3380, 0.0000, 0.6620],\n",
      "        [0.0758, 0.0000, 0.9242],\n",
      "        [0.8350, 0.1650, 0.0000],\n",
      "        [0.1125, 0.0000, 0.8875],\n",
      "        [0.2252, 0.7748, 0.0000],\n",
      "        [0.0000, 0.9736, 0.0264],\n",
      "        [0.0000, 0.0181, 0.9819],\n",
      "        [0.1180, 0.0000, 0.8820],\n",
      "        [0.0000, 0.5886, 0.4114],\n",
      "        [0.9563, 0.0437, 0.0000],\n",
      "        [0.0764, 0.0000, 0.9236],\n",
      "        [0.0000, 0.4435, 0.5565]])\n",
      "error data:tensor([[[[-3.5435e+02, -1.5413e+02,  2.3718e+01,  ..., -1.0955e+02,\n",
      "           -1.1390e+02, -1.2437e+02]]],\n",
      "\n",
      "\n",
      "        [[[-6.2657e+02, -7.4890e+02, -7.4758e+02,  ..., -3.1514e+02,\n",
      "           -3.6872e+02, -3.3050e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6558e+04,  7.6048e+03, -1.6226e+03,  ...,  5.1029e+03,\n",
      "            4.8411e+03,  4.5866e+03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.4654e+00, -2.4282e+02, -4.8146e+02,  ..., -9.3243e+03,\n",
      "           -4.6998e+03,  3.4857e+03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0910e+01,  1.8126e+02,  1.4485e+02,  ...,  1.6804e+03,\n",
      "            2.3518e+03,  3.3278e+03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0638e+01, -6.6634e+00,  4.5713e+00,  ..., -7.4913e+01,\n",
      "           -6.8578e+01,  6.1152e+00]]]])\n",
      "error label:tensor([[0.0000e+00, 6.5336e-01, 3.4664e-01],\n",
      "        [7.6946e-01, 2.3054e-01, 0.0000e+00],\n",
      "        [3.0166e-01, 6.9834e-01, 0.0000e+00],\n",
      "        [6.1582e-01, 0.0000e+00, 3.8418e-01],\n",
      "        [0.0000e+00, 4.4402e-01, 5.5598e-01],\n",
      "        [0.0000e+00, 9.6869e-01, 3.1306e-02],\n",
      "        [8.1024e-01, 0.0000e+00, 1.8976e-01],\n",
      "        [7.8261e-01, 0.0000e+00, 2.1739e-01],\n",
      "        [0.0000e+00, 5.1891e-01, 4.8109e-01],\n",
      "        [9.1446e-01, 8.5537e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 6.3831e-01, 3.6169e-01],\n",
      "        [1.5241e-01, 0.0000e+00, 8.4759e-01],\n",
      "        [6.3774e-01, 0.0000e+00, 3.6226e-01],\n",
      "        [4.6707e-02, 0.0000e+00, 9.5329e-01],\n",
      "        [0.0000e+00, 6.5598e-01, 3.4402e-01],\n",
      "        [5.5341e-01, 4.4659e-01, 0.0000e+00],\n",
      "        [1.0916e-01, 0.0000e+00, 8.9084e-01],\n",
      "        [8.2914e-01, 0.0000e+00, 1.7086e-01],\n",
      "        [7.8816e-01, 0.0000e+00, 2.1184e-01],\n",
      "        [0.0000e+00, 6.9691e-02, 9.3031e-01],\n",
      "        [0.0000e+00, 4.9182e-01, 5.0818e-01],\n",
      "        [0.0000e+00, 6.1891e-01, 3.8109e-01],\n",
      "        [0.0000e+00, 4.1808e-01, 5.8192e-01],\n",
      "        [6.8306e-01, 0.0000e+00, 3.1694e-01],\n",
      "        [0.0000e+00, 7.6911e-01, 2.3089e-01],\n",
      "        [5.6046e-01, 4.3954e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 7.4283e-02, 9.2572e-01],\n",
      "        [8.1506e-02, 0.0000e+00, 9.1849e-01],\n",
      "        [1.5512e-02, 0.0000e+00, 9.8449e-01],\n",
      "        [0.0000e+00, 8.6946e-01, 1.3054e-01],\n",
      "        [8.8097e-01, 0.0000e+00, 1.1903e-01],\n",
      "        [8.3480e-01, 0.0000e+00, 1.6520e-01],\n",
      "        [6.3379e-02, 0.0000e+00, 9.3662e-01],\n",
      "        [3.2044e-01, 0.0000e+00, 6.7956e-01],\n",
      "        [0.0000e+00, 6.5888e-01, 3.4112e-01],\n",
      "        [0.0000e+00, 2.6537e-01, 7.3463e-01],\n",
      "        [0.0000e+00, 9.9467e-02, 9.0053e-01],\n",
      "        [2.8848e-01, 7.1152e-01, 0.0000e+00],\n",
      "        [7.9316e-01, 0.0000e+00, 2.0684e-01],\n",
      "        [9.9333e-01, 0.0000e+00, 6.6651e-03],\n",
      "        [5.2089e-01, 0.0000e+00, 4.7911e-01],\n",
      "        [5.0674e-01, 4.9326e-01, 0.0000e+00],\n",
      "        [7.4428e-01, 2.5572e-01, 0.0000e+00],\n",
      "        [6.7208e-01, 3.2792e-01, 0.0000e+00],\n",
      "        [8.0960e-01, 1.9040e-01, 0.0000e+00],\n",
      "        [4.1366e-01, 0.0000e+00, 5.8634e-01],\n",
      "        [0.0000e+00, 1.3179e-01, 8.6821e-01],\n",
      "        [6.7261e-02, 0.0000e+00, 9.3274e-01],\n",
      "        [3.6897e-01, 6.3103e-01, 0.0000e+00],\n",
      "        [3.3202e-01, 0.0000e+00, 6.6798e-01],\n",
      "        [3.0638e-02, 9.6936e-01, 0.0000e+00],\n",
      "        [3.0303e-01, 0.0000e+00, 6.9697e-01],\n",
      "        [9.3920e-01, 6.0803e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 6.6016e-01, 3.3984e-01],\n",
      "        [0.0000e+00, 1.1032e-01, 8.8968e-01],\n",
      "        [0.0000e+00, 1.9759e-01, 8.0241e-01],\n",
      "        [8.0156e-01, 0.0000e+00, 1.9844e-01],\n",
      "        [9.4001e-01, 0.0000e+00, 5.9986e-02],\n",
      "        [1.5695e-01, 0.0000e+00, 8.4305e-01],\n",
      "        [0.0000e+00, 8.1300e-01, 1.8700e-01],\n",
      "        [9.2347e-01, 0.0000e+00, 7.6527e-02],\n",
      "        [9.0444e-01, 0.0000e+00, 9.5557e-02],\n",
      "        [0.0000e+00, 5.7485e-01, 4.2515e-01],\n",
      "        [0.0000e+00, 7.0876e-01, 2.9124e-01],\n",
      "        [0.0000e+00, 3.8241e-01, 6.1759e-01],\n",
      "        [2.2879e-01, 0.0000e+00, 7.7121e-01],\n",
      "        [5.6089e-01, 4.3911e-01, 0.0000e+00],\n",
      "        [2.7697e-01, 7.2303e-01, 0.0000e+00],\n",
      "        [1.6568e-01, 8.3432e-01, 0.0000e+00],\n",
      "        [4.5441e-01, 0.0000e+00, 5.4559e-01],\n",
      "        [8.3380e-01, 0.0000e+00, 1.6620e-01],\n",
      "        [4.1815e-01, 0.0000e+00, 5.8185e-01],\n",
      "        [2.7111e-01, 0.0000e+00, 7.2889e-01],\n",
      "        [0.0000e+00, 3.7125e-01, 6.2875e-01],\n",
      "        [3.1184e-01, 6.8816e-01, 0.0000e+00],\n",
      "        [5.9445e-01, 0.0000e+00, 4.0555e-01],\n",
      "        [0.0000e+00, 7.1581e-01, 2.8419e-01],\n",
      "        [0.0000e+00, 5.4102e-01, 4.5898e-01],\n",
      "        [5.6366e-01, 0.0000e+00, 4.3634e-01],\n",
      "        [0.0000e+00, 3.3150e-01, 6.6850e-01],\n",
      "        [3.2334e-01, 0.0000e+00, 6.7666e-01],\n",
      "        [2.9376e-01, 0.0000e+00, 7.0624e-01],\n",
      "        [0.0000e+00, 5.4510e-02, 9.4549e-01],\n",
      "        [6.8537e-01, 0.0000e+00, 3.1463e-01],\n",
      "        [0.0000e+00, 3.7421e-01, 6.2579e-01],\n",
      "        [3.1039e-01, 6.8961e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 3.5278e-01, 6.4722e-01],\n",
      "        [9.4277e-01, 0.0000e+00, 5.7227e-02],\n",
      "        [4.7345e-01, 0.0000e+00, 5.2655e-01],\n",
      "        [8.4681e-01, 0.0000e+00, 1.5319e-01],\n",
      "        [7.2252e-01, 0.0000e+00, 2.7748e-01],\n",
      "        [1.0226e-01, 0.0000e+00, 8.9774e-01],\n",
      "        [9.3505e-01, 0.0000e+00, 6.4955e-02],\n",
      "        [7.2447e-01, 0.0000e+00, 2.7553e-01],\n",
      "        [2.7376e-01, 0.0000e+00, 7.2624e-01],\n",
      "        [8.1750e-01, 0.0000e+00, 1.8250e-01],\n",
      "        [0.0000e+00, 2.1753e-01, 7.8247e-01],\n",
      "        [1.3273e-01, 8.6727e-01, 0.0000e+00],\n",
      "        [1.6472e-01, 8.3528e-01, 0.0000e+00],\n",
      "        [9.0759e-01, 9.2412e-02, 0.0000e+00],\n",
      "        [4.3129e-01, 0.0000e+00, 5.6871e-01],\n",
      "        [0.0000e+00, 5.3231e-01, 4.6769e-01],\n",
      "        [5.8682e-01, 0.0000e+00, 4.1318e-01],\n",
      "        [1.5875e-01, 8.4125e-01, 0.0000e+00],\n",
      "        [4.5322e-01, 0.0000e+00, 5.4678e-01],\n",
      "        [0.0000e+00, 8.9214e-01, 1.0786e-01],\n",
      "        [0.0000e+00, 9.2235e-01, 7.7646e-02],\n",
      "        [0.0000e+00, 3.3659e-01, 6.6341e-01],\n",
      "        [9.9916e-01, 8.4321e-04, 0.0000e+00],\n",
      "        [4.3655e-01, 5.6345e-01, 0.0000e+00],\n",
      "        [3.4471e-01, 0.0000e+00, 6.5529e-01],\n",
      "        [0.0000e+00, 2.9378e-01, 7.0622e-01],\n",
      "        [8.9802e-01, 0.0000e+00, 1.0198e-01],\n",
      "        [2.2169e-01, 0.0000e+00, 7.7831e-01],\n",
      "        [0.0000e+00, 5.9917e-01, 4.0083e-01],\n",
      "        [4.3540e-01, 5.6460e-01, 0.0000e+00],\n",
      "        [3.4827e-02, 0.0000e+00, 9.6517e-01],\n",
      "        [1.9377e-01, 0.0000e+00, 8.0623e-01],\n",
      "        [5.7468e-01, 0.0000e+00, 4.2532e-01],\n",
      "        [6.9705e-01, 0.0000e+00, 3.0295e-01],\n",
      "        [2.8798e-01, 0.0000e+00, 7.1202e-01],\n",
      "        [1.2969e-01, 0.0000e+00, 8.7031e-01],\n",
      "        [5.1490e-01, 0.0000e+00, 4.8510e-01],\n",
      "        [3.6710e-01, 0.0000e+00, 6.3290e-01],\n",
      "        [0.0000e+00, 4.5725e-01, 5.4275e-01],\n",
      "        [3.5724e-01, 0.0000e+00, 6.4276e-01],\n",
      "        [0.0000e+00, 3.6708e-01, 6.3292e-01],\n",
      "        [0.0000e+00, 8.2049e-01, 1.7951e-01]])\n",
      "error data:tensor([[[[-3.4266e+02, -1.1700e+02, -8.5684e+01,  ..., -2.3583e+02,\n",
      "           -4.8103e+02, -3.8850e+02]]],\n",
      "\n",
      "\n",
      "        [[[-2.9847e+03, -1.5150e+03,  5.2872e+02,  ..., -5.9295e+03,\n",
      "           -5.9588e+02, -9.8624e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4469e+03,  1.5716e+03,  1.2476e+03,  ...,  1.5786e+03,\n",
      "            7.5435e+02, -2.5172e+02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.7764e-01,  2.4874e-02, -1.0056e+00,  ..., -2.3434e+01,\n",
      "           -2.2920e+01, -2.1522e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0700e+03,  5.9048e+02,  3.5173e+02,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4642e+02,  1.9514e+02,  1.3924e+02,  ..., -1.1571e+01,\n",
      "           -1.3722e+01, -9.0844e+00]]]])\n",
      "error label:tensor([[0.0000, 0.3378, 0.6622],\n",
      "        [0.0000, 0.7216, 0.2784],\n",
      "        [0.6156, 0.0000, 0.3844],\n",
      "        [0.7252, 0.2748, 0.0000],\n",
      "        [0.9949, 0.0000, 0.0051],\n",
      "        [0.6818, 0.0000, 0.3182],\n",
      "        [0.6093, 0.3907, 0.0000],\n",
      "        [0.0000, 0.6570, 0.3430],\n",
      "        [0.3023, 0.0000, 0.6977],\n",
      "        [0.0000, 0.9849, 0.0151],\n",
      "        [0.0000, 0.1667, 0.8333],\n",
      "        [0.9212, 0.0788, 0.0000],\n",
      "        [0.4293, 0.0000, 0.5707],\n",
      "        [0.9005, 0.0000, 0.0995],\n",
      "        [0.3922, 0.0000, 0.6078],\n",
      "        [0.0000, 0.5327, 0.4673],\n",
      "        [0.5819, 0.0000, 0.4181],\n",
      "        [0.0011, 0.9989, 0.0000],\n",
      "        [0.5281, 0.0000, 0.4719],\n",
      "        [0.4887, 0.0000, 0.5113],\n",
      "        [0.3227, 0.0000, 0.6773],\n",
      "        [0.4437, 0.5563, 0.0000],\n",
      "        [0.3313, 0.0000, 0.6687],\n",
      "        [0.6820, 0.3180, 0.0000],\n",
      "        [0.8790, 0.0000, 0.1210],\n",
      "        [0.1717, 0.0000, 0.8283],\n",
      "        [0.4286, 0.0000, 0.5714],\n",
      "        [0.7220, 0.2780, 0.0000],\n",
      "        [0.9029, 0.0000, 0.0971],\n",
      "        [0.3424, 0.0000, 0.6576],\n",
      "        [0.0000, 0.7613, 0.2387],\n",
      "        [0.7088, 0.2912, 0.0000],\n",
      "        [0.2949, 0.0000, 0.7051],\n",
      "        [0.0000, 0.1491, 0.8509],\n",
      "        [0.8702, 0.0000, 0.1298],\n",
      "        [0.5233, 0.0000, 0.4767],\n",
      "        [0.9858, 0.0000, 0.0142],\n",
      "        [0.0000, 0.2220, 0.7780],\n",
      "        [0.0000, 0.8400, 0.1600],\n",
      "        [0.0000, 0.3736, 0.6264],\n",
      "        [0.1274, 0.0000, 0.8726],\n",
      "        [0.6897, 0.0000, 0.3103],\n",
      "        [0.9086, 0.0000, 0.0914],\n",
      "        [0.0000, 0.5357, 0.4643],\n",
      "        [0.4419, 0.5581, 0.0000],\n",
      "        [0.0096, 0.0000, 0.9904],\n",
      "        [0.6144, 0.0000, 0.3856],\n",
      "        [0.3286, 0.6714, 0.0000],\n",
      "        [0.0000, 0.3088, 0.6912],\n",
      "        [0.1788, 0.0000, 0.8212],\n",
      "        [0.4614, 0.5386, 0.0000],\n",
      "        [0.4719, 0.5281, 0.0000],\n",
      "        [0.0000, 0.2336, 0.7664],\n",
      "        [0.6535, 0.0000, 0.3465],\n",
      "        [0.0000, 0.3062, 0.6938],\n",
      "        [0.0000, 0.0057, 0.9943],\n",
      "        [0.2742, 0.0000, 0.7258],\n",
      "        [0.2610, 0.0000, 0.7390],\n",
      "        [0.0000, 0.5203, 0.4797],\n",
      "        [0.7502, 0.0000, 0.2498],\n",
      "        [0.1110, 0.0000, 0.8890],\n",
      "        [0.5898, 0.4102, 0.0000],\n",
      "        [0.3393, 0.0000, 0.6607],\n",
      "        [0.0428, 0.9572, 0.0000],\n",
      "        [0.0000, 0.6670, 0.3330],\n",
      "        [0.1939, 0.0000, 0.8061],\n",
      "        [0.5628, 0.0000, 0.4372],\n",
      "        [0.0000, 0.3656, 0.6344],\n",
      "        [0.2014, 0.0000, 0.7986],\n",
      "        [0.7519, 0.0000, 0.2481],\n",
      "        [0.0000, 0.9270, 0.0730],\n",
      "        [0.0189, 0.0000, 0.9811],\n",
      "        [0.6734, 0.0000, 0.3266],\n",
      "        [0.0000, 0.2061, 0.7939],\n",
      "        [0.3485, 0.0000, 0.6515],\n",
      "        [0.2277, 0.7723, 0.0000],\n",
      "        [0.6196, 0.0000, 0.3804],\n",
      "        [0.0000, 0.9071, 0.0929],\n",
      "        [0.0046, 0.9954, 0.0000],\n",
      "        [0.8880, 0.1120, 0.0000],\n",
      "        [0.3547, 0.0000, 0.6453],\n",
      "        [0.6504, 0.0000, 0.3496],\n",
      "        [0.9546, 0.0000, 0.0454],\n",
      "        [0.7623, 0.0000, 0.2377],\n",
      "        [0.2580, 0.0000, 0.7420],\n",
      "        [0.4976, 0.0000, 0.5024],\n",
      "        [0.0000, 0.8888, 0.1112],\n",
      "        [0.5252, 0.0000, 0.4748],\n",
      "        [0.1124, 0.0000, 0.8876],\n",
      "        [0.5238, 0.0000, 0.4762],\n",
      "        [0.3316, 0.6684, 0.0000],\n",
      "        [0.7744, 0.2256, 0.0000],\n",
      "        [0.2507, 0.0000, 0.7493],\n",
      "        [0.0000, 0.3388, 0.6612],\n",
      "        [0.0143, 0.0000, 0.9857],\n",
      "        [0.1423, 0.8577, 0.0000],\n",
      "        [0.0000, 0.2550, 0.7450],\n",
      "        [0.0000, 0.3368, 0.6632],\n",
      "        [0.8738, 0.1262, 0.0000],\n",
      "        [0.6930, 0.0000, 0.3070],\n",
      "        [0.1949, 0.0000, 0.8051],\n",
      "        [0.8482, 0.1518, 0.0000],\n",
      "        [0.0528, 0.0000, 0.9472],\n",
      "        [0.7737, 0.0000, 0.2263],\n",
      "        [0.2784, 0.0000, 0.7216],\n",
      "        [0.0000, 0.5320, 0.4680],\n",
      "        [0.7912, 0.2088, 0.0000],\n",
      "        [0.1776, 0.0000, 0.8224],\n",
      "        [0.0000, 0.1934, 0.8066],\n",
      "        [0.1235, 0.8765, 0.0000],\n",
      "        [0.0108, 0.9892, 0.0000],\n",
      "        [0.0000, 0.6604, 0.3396],\n",
      "        [0.8752, 0.0000, 0.1248],\n",
      "        [0.4259, 0.0000, 0.5741],\n",
      "        [0.0000, 0.3967, 0.6033],\n",
      "        [0.0000, 0.0368, 0.9632],\n",
      "        [0.6403, 0.3597, 0.0000],\n",
      "        [0.0000, 0.2427, 0.7573],\n",
      "        [0.3934, 0.6066, 0.0000],\n",
      "        [0.0459, 0.0000, 0.9541],\n",
      "        [0.4218, 0.5782, 0.0000],\n",
      "        [0.8634, 0.0000, 0.1366],\n",
      "        [0.9012, 0.0000, 0.0988],\n",
      "        [0.3440, 0.0000, 0.6560],\n",
      "        [0.2314, 0.0000, 0.7686],\n",
      "        [0.0000, 0.4998, 0.5002],\n",
      "        [0.0000, 0.9096, 0.0904],\n",
      "        [0.0000, 0.8958, 0.1042]])\n",
      "error data:tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.9887e+03,\n",
      "           -6.1181e+03, -6.2691e+03]]],\n",
      "\n",
      "\n",
      "        [[[-3.8582e+03, -3.6132e+03, -3.3373e+03,  ...,  6.6935e+02,\n",
      "            4.2872e+02,  5.5527e+02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0448e+02,  5.1181e+01,  4.4242e+01,  ..., -3.6720e+02,\n",
      "           -5.3758e+02, -6.7258e+02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.9179e+02, -1.5615e+02, -1.3867e+02,  ...,  1.5935e+01,\n",
      "            1.4854e+01,  1.2101e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0912e+01,  4.5881e+01,  4.0333e+01,  ..., -4.3759e+00,\n",
      "           -4.2415e+00, -2.7456e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.0612e+02,  8.8398e+02,  7.2279e+02,  ..., -1.0204e+02,\n",
      "            9.0862e+01,  7.2107e+02]]]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mTestModel();\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantization process is started.....\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantizeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantization done\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting quantized model.\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "Cell \u001b[0;32mIn[16], line 259\u001b[0m, in \u001b[0;36mQATTrainer.QuantizeModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mprepare_qat(net, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Calibrate with the training data\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# self.__calibrate(net);\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m#place trained model to cpu\u001b[39;00m\n\u001b[1;32m    262\u001b[0m net\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "Cell \u001b[0;32mIn[16], line 143\u001b[0m, in \u001b[0;36mQATTrainer.__train\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m    140\u001b[0m n_batches \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainGen\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mbatchSize);\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batchIdx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainGen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatchIdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mmoveaxis(x, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mdevice);\n\u001b[1;32m    145\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mdevice);\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mTLGenerator.__getitem__\u001b[0;34m(self, batchIndex)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batchIndex):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#Generate one batch of data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     batchX, batchY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatchIndex\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     20\u001b[0m     batchX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(batchX, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m);\n\u001b[1;32m     21\u001b[0m     batchX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(batchX, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m);\n",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36mTLGenerator.generate_batch\u001b[0;34m(self, batchIndex)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Mix two examples\u001b[39;00m\n\u001b[1;32m     41\u001b[0m r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(random\u001b[38;5;241m.\u001b[39mrandom())\n\u001b[0;32m---> 42\u001b[0m sound \u001b[38;5;241m=\u001b[39m \u001b[43mU\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmix\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msound2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(f\"sound length after U.mix is {len(sound)}\")\u001b[39;00m\n\u001b[1;32m     44\u001b[0m eye \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mnClasses)\n",
      "File \u001b[0;32m~/RLRepo/Works/Projects/uec-iot-ai-models/src/Training/Step_5_Quant_and_Retrain_and_Convert2TFLite/../../common/utils.py:124\u001b[0m, in \u001b[0;36mmix\u001b[0;34m(sound1, sound2, r, fs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmix\u001b[39m(sound1, sound2, r, fs):\n\u001b[1;32m    123\u001b[0m     gain1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(compute_gain(sound1, fs))  \u001b[38;5;66;03m# Decibel\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     gain2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mcompute_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10\u001b[39m, (gain1 \u001b[38;5;241m-\u001b[39m gain2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20.\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m r)\n\u001b[1;32m    126\u001b[0m     sound \u001b[38;5;241m=\u001b[39m ((sound1 \u001b[38;5;241m*\u001b[39m t \u001b[38;5;241m+\u001b[39m sound2 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m t)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(t \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/RLRepo/Works/Projects/uec-iot-ai-models/src/Training/Step_5_Quant_and_Retrain_and_Convert2TFLite/../../common/utils.py:109\u001b[0m, in \u001b[0;36mcompute_gain\u001b[0;34m(sound, fs, min_db, mode)\u001b[0m\n\u001b[1;32m    107\u001b[0m     spec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mrfft(np\u001b[38;5;241m.\u001b[39mhanning(n_fft \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m sound[i: i \u001b[38;5;241m+\u001b[39m n_fft])\n\u001b[1;32m    108\u001b[0m     power_spec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(spec) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 109\u001b[0m     a_weighted_spec \u001b[38;5;241m=\u001b[39m power_spec \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     g \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(a_weighted_spec)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3176d2-4a2d-4ed6-85c8-20c1c925d73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b9046-02c3-4ea8-a107-b6498c4487af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
