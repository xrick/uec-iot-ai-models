{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e303567-45fd-4162-b2fa-53462fb6dffc",
   "metadata": {},
   "source": [
    "## Train_Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a577c748-8e1f-4d4d-8d0f-ea1257a28b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import sys;\n",
    "import numpy as np;\n",
    "import random;\n",
    "import glob;\n",
    "import math;\n",
    "import time;\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import tensorflow.keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e314830e-9d57-4eee-8dea-3b19b73d5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd());\n",
    "sys.path.append(os.path.join(os.getcwd(), 'common'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e4db9fa-adbd-414f-866c-18f9cfd79682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.tlopts as tlopts\n",
    "# import resources.TLModels as tlmodels;\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b7f6a4-427f-45ba-9035-0c4c5db23a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc84b1f0-9afe-4d45-b8f5-e73542a11ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb43c6c5-5149-42af-bc36-5be4ec3e1251",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ./tf/trained_models/acdnet_tl_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tmp_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tf/trained_models/acdnet_tl_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/acdnetenv/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/acdnetenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/acdnetenv/lib/python3.11/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at ./tf/trained_models/acdnet_tl_model.h5"
     ]
    }
   ],
   "source": [
    "tmp_model = load_model(\"./tf/trained_models/acdnet_tl_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91df731-59af-4879-b20a-e8d9b500aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLACDNet:\n",
    "\tdef __init__(self, pretrained_model_path=None,opt=None, num_class=6):\n",
    "\t\tself.opt = opt\n",
    "\t\tself.pretrained_model_path = pretrained_model_path\n",
    "\t\tself.new_model = None\n",
    "\t\tself.num_class = num_class\n",
    "\n",
    "\tdef Create_TLACDNet(self):\n",
    "\t\tmodel = load_model(self.pretrained_model_path)\n",
    "\t\tprint(f\"original model loaded....\")\n",
    "\t\t# for l in model.layers:\n",
    "\t\t# \tprint(f\"layer:{l} trainable weight length is {len(l.wei)}\")\n",
    "\t\ttotal_layers_num = len(model.layers)\n",
    "\t\treplaced_layers_num = 2\n",
    "\t\tfreeze_layers_num = total_layers_num-replaced_layers_num\n",
    "\n",
    "\t\t## freeze layers\n",
    "\t\tfor i in range(freeze_layers_num):\n",
    "\t\t\tmodel.layers[i].trainable = False\n",
    "\n",
    "\t\tfor j in range(freeze_layers_num, total_layers_num):\n",
    "\t\t\tmodel.layers[j].trainable = True\n",
    "\n",
    "\t\tcustom_layers = model.layers[freeze_layers_num-1].output\n",
    "\t\tcustom_layers = L.Dense(self.num_class)(custom_layers)\n",
    "\t\t# custom_layers = Softmax()(custom_layers)\n",
    "\t\tcustom_layers = L.Dense(self.num_class,activation=\"softmax\")(custom_layers)\n",
    "\n",
    "\t\tnew_model = Model(inputs=model.input,outputs=custom_layers)\n",
    "\t\tprint(\"new model info:\\n\")\n",
    "\t\tprint(new_model.summary())\n",
    "\t\tprint(\"\\n\")\n",
    "\t\treturn new_model\n",
    "\n",
    "def GetTLACDNet(pretrained_model_path=None,opt=None, num_class=6):\n",
    "\ttrainedModelPath = pretrained_model_path\n",
    "\ttlacdnet = TLACDNet(trainedModelPath, opt, num_class)\n",
    "\treturn tlacdnet.Create_TLACDNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40615141-eea2-4c5c-8c04-430fbece4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator(keras.utils.Sequence):\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([(17,1),(18,2),(24,3),\n",
    "                             (51,4),(52,5),(53,6)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e8513ae-3a7e-40c0-b94d-a6d095454821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup(opt, split):\n",
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    dataset = np.load(\"datasets/fold1_dataset.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen\n",
    "    # print(len(train_sounds))\n",
    "    # print(len(train_labels))\n",
    "    \n",
    "        # if i != split:\n",
    "    # train_sounds.extend(sounds)\n",
    "    # train_labels.extend(labels)\n",
    "    # print(\"sounds:\\n\")\n",
    "    # print(train_sounds);\n",
    "    # print(\"\\n********************************************\\n\");\n",
    "    # print(\"labels:\\n\")\n",
    "    # print(train_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a78acb-f58e-4386-8a33-9a341044f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getTrainGen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471fd95-4ac2-4d55-9e37-0241886e42ca",
   "metadata": {},
   "source": [
    "## Transfer-Learning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee8bd1b-9243-4485-903b-f39e8c7324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLTrainer:\n",
    "    def __init__(self, opt=None):\n",
    "        self.opt = opt;\n",
    "        # self.opt.trainer = self;\n",
    "        self.trainGen = getTrainGen(self.opt, self.opt.splits)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.pretrainedmodelpath = \"./resources/pretrained_models/acdnet20_20khz_fold4.h5\"\n",
    "        \n",
    "    def Train(self):\n",
    "        # model = tlmodels.GetAcdnetModel(self.opt.inputLength, 50, self.opt.sr, ch_config = self.opt.model_config);\n",
    "        model  = tlmodels.GetTLACDNet(pretrained_model_path=self.pretrainedmodelpath, opt=self.opt);\n",
    "        model.summary();\n",
    "\n",
    "        loss = 'kullback_leibler_divergence';\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True)\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer , metrics=['accuracy']);\n",
    "\n",
    "        # learning schedule callback\n",
    "        lrate = keras.callbacks.LearningRateScheduler(self.GetLR);\n",
    "        #mark out the following line to not execu modelcheckpoint\n",
    "        best_model = keras.callbacks.ModelCheckpoint('tf/trained_models/{}.h5'.format(self.opt.model_name), monitor='val_acc', save_best_only=True, verbose=0);\n",
    "        custom_evaluator = CustomCallback(self.opt);\n",
    "        # callbacks_list = [lrate, custom_evaluator, best_model];\n",
    "        callbacks_list = [lrate, custom_evaluator,best_model];\n",
    "        print(f\"Length of trainGen.data:{len(self.trainGen.data)}\");\n",
    "        print(f\"Length of batch size:{self.trainGen.batch_size}\")\n",
    "        print(f\"type of trainGen: {type(self.trainGen)}\")\n",
    "        model.fit(self.trainGen, epochs=self.opt.nEpochs, steps_per_epoch=len(self.trainGen.data)//self.trainGen.batch_size, callbacks=callbacks_list, verbose=0);\n",
    "# \n",
    "    def GetLR(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5efa2999-405f-45db-a5a7-c20482faf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.curEpoch = 0;\n",
    "        self.curLr = opt.LR;\n",
    "        self.cur_epoch_start_time = time.time();\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.curEpoch = epoch+1;\n",
    "        self.curLr = TLTrainer(self.opt).GetLR(epoch+1);\n",
    "        self.cur_epoch_start_time = time.time();\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_time = time.time() - self.cur_epoch_start_time;\n",
    "        print(f\"training time:{train_time}\")\n",
    "        self.load_test_data();\n",
    "        val_acc, val_loss = self.validate(self.model);\n",
    "        logs['val_acc'] = val_acc;\n",
    "        logs['val_loss'] = val_loss;\n",
    "        if val_acc > self.bestAcc:\n",
    "            self.bestAcc = val_acc;\n",
    "            self.bestAccEpoch = epoch + 1;\n",
    "        epoch_time = time.time() - self.cur_epoch_start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        \n",
    "        line = 'SP-{}, Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            1, epoch+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            self.curLr, logs['loss'], logs['accuracy'] if 'accuracy' in logs else logs['acc'], val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if self.testX is None:\n",
    "            data = np.load('datasets/fold1_test16000.npz', allow_pickle=True);\n",
    "            self.testX = data['x'];\n",
    "            self.testY = data['y'];\n",
    "    # def load_test_data(self):\n",
    "    #     if self.testX is None:\n",
    "    #         data = np.load('datasets/fold1_test16000.npz', allow_pickle=True);\n",
    "    #         self.testX = data['fold{}'.format(1)].item()['sounds']\n",
    "    #         self.testY = data['fold{}'.format(1)].item()['labels']\n",
    "\n",
    "    def validate(self, model):\n",
    "        y_pred = None;\n",
    "        y_target = None;\n",
    "        batch_size = 5#(self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "        loopend = math.ceil(len(self.testX) / batch_size)\n",
    "        print(f\"in validate loopend is {loopend}\")\n",
    "        batch_size_ = 5\n",
    "        for batchIndex in range(loopend):\n",
    "            x = self.testX[batchIndex*batch_size : (batchIndex+1)*batch_size];\n",
    "            print(f\"type of x : {type(x)}\")\n",
    "            y = self.testY[batchIndex*batch_size : (batchIndex+1)*batch_size];\n",
    "            # for k in range(len(x)):\n",
    "            #     print(f\"itme{x[k]} length is {len(x[k])}\")\n",
    "            #     if len(x[k]) < 80000:\n",
    "            #         x[k] = np.pad(x[k],(0,80000-len(x[k])),'constant')\n",
    "            # print(f\"length of Test X is {len(x)}\")\n",
    "            # print(f\"length of Test Y is {len(y)}\")\n",
    "            \n",
    "            for itm in x:\n",
    "                print(f\"itm length is {len(itm)}\")\n",
    "            scores = model.predict(x, batch_size=len(y), verbose=0);\n",
    "            # scores = model.predict(x, batch_size=batch_size_, verbose=0);\n",
    "            y_pred = scores if y_pred is None else np.concatenate((y_pred, scores));\n",
    "            y_target = y if y_target is None else np.concatenate((y_target, y));\n",
    "            #break;\n",
    "\n",
    "        acc, loss = self.compute_accuracy(y_pred, y_target);\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        #Reshape y_pred to shape it like each sample comtains 10 samples.\n",
    "        if self.opt.nCrops > 1:\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(axis=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(axis=1);\n",
    "\n",
    "        loss = keras.losses.KLD(y_target, y_pred).numpy().mean();\n",
    "\n",
    "        #Get the indices that has highest average value for each sample\n",
    "        y_pred = y_pred.argmax(axis=1);\n",
    "        y_target = y_target.argmax(axis=1);\n",
    "        accuracy = (y_pred==y_target).mean()*100;\n",
    "\n",
    "        return accuracy, loss;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b2431-2198-4b2c-8aba-e402e89f3fce",
   "metadata": {},
   "source": [
    "## getOpts method is for generate options for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0619881f-5318-426e-8852-9dd4764ba6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='{}/datasets/processed/'.format(os.getcwd()),  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 5;#64;\n",
    "    opt.weightDecay = 5e-4;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 10;#2000;\n",
    "    opt.LR = 0.01#0.1;\n",
    "    opt.schedule = [0.03, 0.06, 0.09]#[0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 6#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.splits = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 16000#20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 5;\n",
    "    return opt\n",
    "    # opt = parser.parse_args();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a96905-0644-40db-a26d-7aaa9707ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # opt = tlopts.parse();\n",
    "    opt = getOpts()\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    opt.trainer = None\n",
    "    # import torch;\n",
    "    opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "    tlopts.display_info(opt)\n",
    "    opt.model_name = \"acdnet_tl_model\"\n",
    "    # valid_path = False;\n",
    "    print(\"Initializing TLTrainer Object.....\")\n",
    "    trainer = TLTrainer(opt);\n",
    "    print(\"Start to training.....\")\n",
    "    trainer.Train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83e24d10-a0d2-4a39-bbdb-0738d443ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "| TLACDNet Sound classification\n",
      "+------------------------------+\n",
      "| dataset  : uec_iot\n",
      "| nEpochs  : 10\n",
      "| LRInit   : 0.01\n",
      "| schedule : [0.03, 0.06, 0.09]\n",
      "| warmup   : 10\n",
      "| batchSize: 5\n",
      "| nFolds: 1\n",
      "| Splits: 1\n",
      "+------------------------------+\n",
      "Initializing TLTrainer Object.....\n",
      "length of samples:65\n",
      "Start to training.....\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tlmodels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TLTrainer(opt);\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart to training.....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mTLTrainer.Train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# model = tlmodels.GetAcdnetModel(self.opt.inputLength, 50, self.opt.sr, ch_config = self.opt.model_config);\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     model  \u001b[38;5;241m=\u001b[39m \u001b[43mtlmodels\u001b[49m\u001b[38;5;241m.\u001b[39mGetTLACDNet(pretrained_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrainedmodelpath, opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt);\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39msummary();\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkullback_leibler_divergence\u001b[39m\u001b[38;5;124m'\u001b[39m;\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tlmodels' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539121ac-d02b-4f25-830d-4c60b9f5c230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0538eea-b356-4cfa-be5a-1f574f96308c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
