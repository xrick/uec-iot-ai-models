{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a69029-4fb8-4420-9be3-06ea3cf14f0d",
   "metadata": {},
   "source": [
    "## 測試項目\n",
    "1. 訓練集中的alarm聲\n",
    "2. 使用pruned and quantized tflite model\n",
    "4. export to cc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f0a4a7-cc91-44ff-a761-5ec09ce97599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 16:21:36.491873: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 16:21:36.511074: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 16:21:36.511093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 16:21:36.511565: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 16:21:36.515057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 16:21:36.866698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import zipfile\n",
    "import wavio\n",
    "from common import utils as U\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218705f2-4f93-4cb9-bf6b-84abd01d199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30027b0c-feb4-45ba-aac1-d14089a7516a",
   "metadata": {},
   "source": [
    "### Loading TFlite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff7e097-1237-4564-96dd-678ba95ffd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_quant_model_path = \"./models/tflite/retrain_pruned_model_20240130161638_from_keras.tflite\";\n",
    "# Load quantized TFLite model\n",
    "tflite_interpreter_quant = tf.lite.Interpreter(model_path=tflite_quant_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8031ec-a85c-401f-a714-0579f2a43833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_input_2:0\n",
      "shape: [    1     1 30225     1]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [1 2]\n",
      "type: <class 'numpy.int8'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# model.summary()\n",
    "input_details = tflite_interpreter_quant.get_input_details()\n",
    "output_details = tflite_interpreter_quant.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "#resize the input and output\n",
    "# tflite_interpreter_quant.resize_tensor_input(input_details[0]['index'], (2, 1, 30225, 1))\n",
    "# tflite_interpreter_quant.resize_tensor_input(output_details[0]['index'], (2, 2))\n",
    "\n",
    "# input_details = tflite_interpreter_quant.get_input_details()\n",
    "# output_details = tflite_interpreter_quant.get_output_details()\n",
    "\n",
    "# print(\"== Input details ==\")\n",
    "# print(\"name:\", input_details[0]['name'])\n",
    "# print(\"shape:\", input_details[0]['shape'])\n",
    "# print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "# print(\"\\n== Output details ==\")\n",
    "# print(\"name:\", output_details[0]['name'])\n",
    "# print(\"shape:\", output_details[0]['shape'])\n",
    "# print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "tflite_interpreter_quant.allocate_tensors();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c2bc6-26c8-41a0-908e-0062f48168a1",
   "metadata": {},
   "source": [
    "### loading dataset from npz format\n",
    "1. ACDNet input length is 30225\n",
    "2. sr is 44100 and 20000\n",
    "3. need to convert 16K to 20000\n",
    "### ACDNet Config Setting\n",
    "#### Training Parameters\n",
    "1. opt.batchSize = 64;\n",
    "2. opt.weightDecay = 5e-4;\n",
    "3. opt.momentum = 0.9;\n",
    "4. opt.nEpochs = 2000;\n",
    "5. opt.LR = 0.1;\n",
    "6. opt.schedule = [0.3, 0.6, 0.9];\n",
    "7. opt.warmup = 10; \n",
    "#### Basic Net Configuration\n",
    "- nClasses = 50\n",
    "- nFolds = 5\n",
    "- splits = \\[i for in range(1, nFolds + 1)\\]\n",
    "- sr = 20000\n",
    "- inputLength = 30225\n",
    "<br>ngth = 30225;\n",
    "### How to convert 16K sound to 44.1K with python and sox\n",
    "\n",
    "if using sox the command is as following: <br />\n",
    "    sox old.wav -b 16 new.wav \n",
    "if using python you can do as following: <br />\n",
    "    import soundfile\n",
    "    \n",
    "data, samplerate = soundfile.read('old.wav\n",
    "    <br />)\n",
    "soundfile.write('new.wav', data, samplerate, subtype='PCM_1\n",
    "6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08dc6913-7290-414b-a6f1-7172edd43562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4730cca4-ba34-43e0-8c32-13afb77d5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileList(srcDir,regex='.*\\.wav'):\n",
    "    results = os.listdir(srcDir)\n",
    "    out_files = []\n",
    "    cnt_files = 0\n",
    "    for file in results:\n",
    "        if os.path.isdir(os.path.join(srcDir, file)):\n",
    "            out_files += getFileList(os.path.join(srcDir, file))\n",
    "        elif re.match(regex, file,  re.I):  # file.startswith(startExtension) or file.endswith(\".txt\") or file.endswith(endExtension):\n",
    "            out_files.append(os.path.join(srcDir, file))\n",
    "            cnt_files = cnt_files + 1\n",
    "    return out_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a0a0b-2b1a-45c3-9eed-0bf6b10f3f24",
   "metadata": {},
   "source": [
    "## sound preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8156e68-94c4-44b4-af29-166c3533301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputLen = 30225\n",
    "_nCrops = 2\n",
    "def preprocess_setup():\n",
    "    funcs = []\n",
    "    funcs += [U.padding( _inputLen// 2),\n",
    "              U.normalize(32768.0),\n",
    "              U.multi_crop(_inputLen, _nCrops)]\n",
    "              # U.single_crop(_inputLen)]\n",
    "              # \n",
    "\n",
    "    return funcs\n",
    "\n",
    "def preprocess_debug():\n",
    "    debug_funcs = []\n",
    "    debug_funcs += [U.padding( _inputLen// 2),\n",
    "              U.normalize(32768.0),]\n",
    "              # U.multi_crop(_inputLen, _nCrops)]\n",
    "              # U.single_crop(_inputLen)]\n",
    "              # \n",
    "\n",
    "    return debug_funcs\n",
    "\n",
    "def preprocess(sound, funcs):\n",
    "    for f in funcs:\n",
    "        sound = f(sound)\n",
    "\n",
    "    return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d59fc9-c11b-4027-8f53-8ae3b743f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_funcs = preprocess_setup()\n",
    "# _debug_funcs = preprocess_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9dcde-83a3-4b49-9603-27acdb123b72",
   "metadata": {},
   "source": [
    "### Read Test Wav File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0d4c18-078b-469a-8045-e2c79cb327ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in p_wav_list:\n",
    "#     sound = wavio.read(i).data.T[0]\n",
    "#     start = sound.nonzero()[0].min();\n",
    "#     end = sound.nonzero()[0].max();\n",
    "#     sound = sound[start: end + 1]\n",
    "#     sound_debug = np.array(preprocess(sound,_debug_funcs));\n",
    "#     print(f\"sound content after pad and normalize:\\n{sound_debug}\")\n",
    "#     label = int(p_label);\n",
    "#     print(f\"current sound start:{start} to end:{end}\");\n",
    "#     label = int(p_label);\n",
    "#     if len(sound)> 220500:\n",
    "#         sound = sound[:220500]\n",
    "#     sound = preprocess(sound, _funcs);\n",
    "#     print(f\"label is {label} and sound len is {len(sound)}\");\n",
    "#     sounds.append(sound);\n",
    "#     labels.append(label);\n",
    "    \n",
    "# for j in n_wav_list:\n",
    "#     sound = wavio.read(j).data.T[0]\n",
    "#     start = sound.nonzero()[0].min();\n",
    "#     end = sound.nonzero()[0].max();\n",
    "#     sound = sound[start: end + 1];\n",
    "#     sound_debug = np.array(preprocess(sound,_debug_funcs));\n",
    "#     print(f\"sound content after pad and normalize:\\n{sound_debug}\")\n",
    "#     label = int(n_label);\n",
    "#     print(f\"current sound start:{start} to end:{end}\");\n",
    "    \n",
    "#     sound = np.float32(preprocess(sound, _funcs));\n",
    "#     print(f\"label is {label} and sound len is {len(sound)}\")\n",
    "#     sounds.append(sound);\n",
    "#     labels.append(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06702eca-28ee-4af3-a0d4-3db6d1496f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total alarm sounds:116 \n",
      "total other sounds:128\n",
      "total siren sounds:91\n",
      "sound testset 1 length: 335\n",
      "sound testset 2 length: 207\n"
     ]
    }
   ],
   "source": [
    "positive_test_wavs = \"./test_sounds/positive/\"\n",
    "negative_test_wavs_others = \"./test_sounds/negative/othersounds/\"\n",
    "negative_test_wavs_siren = \"./test_sounds/negative/siren/\"\n",
    "\n",
    "p_wav_list = getFileList(positive_test_wavs);\n",
    "n_wav_list_other = getFileList(negative_test_wavs_others);\n",
    "n_wav_list_siren = getFileList(negative_test_wavs_siren);\n",
    "print(f\"total alarm sounds:{len(p_wav_list)} \");\n",
    "print(f\"total other sounds:{len(n_wav_list_other)}\")\n",
    "print(f\"total siren sounds:{len(n_wav_list_siren)}\")\n",
    "p_label = 52;\n",
    "n_label = 99;\n",
    "\n",
    "sounds_1 = [];\n",
    "labels_1 = [];\n",
    "sounds_2 = [];\n",
    "labels_2 = [];\n",
    "## sound = preprocess(sound, _funcs)\n",
    "for i in p_wav_list:\n",
    "    sound = wavio.read(i).data.T[0]\n",
    "    start = sound.nonzero()[0].min();\n",
    "    end = sound.nonzero()[0].max();\n",
    "    sound = sound[start: end + 1]\n",
    "    label = int(p_label);\n",
    "    if len(sound)> 220500:\n",
    "        sound = sound[:220500]\n",
    "    sound = preprocess(sound, _funcs);\n",
    "    sounds_1.append(sound);\n",
    "    labels_1.append(label);\n",
    "    sounds_2.append(sound);\n",
    "    labels_2.append(label);\n",
    "    \n",
    "for j in n_wav_list_other:\n",
    "    sound = wavio.read(j).data.T[0]\n",
    "    start = sound.nonzero()[0].min();\n",
    "    end = sound.nonzero()[0].max();\n",
    "    sound = sound[start: end + 1];\n",
    "    label = int(n_label);\n",
    "    sound = np.float32(preprocess(sound, _funcs));\n",
    "    sounds_1.append(sound);\n",
    "    labels_1.append(label);\n",
    "\n",
    "for k in n_wav_list_siren:\n",
    "    sound = wavio.read(j).data.T[0]\n",
    "    start = sound.nonzero()[0].min();\n",
    "    end = sound.nonzero()[0].max();\n",
    "    sound = sound[start: end + 1];\n",
    "    label = int(n_label);\n",
    "    sound = np.float32(preprocess(sound, _funcs));\n",
    "    sounds_1.append(sound);\n",
    "    labels_1.append(label);\n",
    "    sounds_2.append(sound);\n",
    "    labels_2.append(label);\n",
    "\n",
    "print(f\"sound testset 1 length: {len(sounds_1)}\");\n",
    "print(f\"sound testset 2 length: {len(sounds_2)}\");\n",
    "\n",
    "# sound = wavio.read(test_sound_file).data.T[0]\n",
    "# start = sound.nonzero()[0].min()\n",
    "# end = sound.nonzero()[0].max()\n",
    "# sound = sound[start: end + 1]  # Remove silent sections\n",
    "# label = 18 #int(os.path.splitext(test_sound_file)[0].split('-')[-1])\n",
    "\n",
    "# if len(sound)> 220500:\n",
    "#     sound = sound[:220500]\n",
    "\n",
    "# ec50_sound1 =  wavio.read(ec50_18_sound).data.T[0]\n",
    "# start_ec50 = ec50_sound1.nonzero()[0].min()\n",
    "# end_ec50 = ec50_sound1.nonzero()[0].max()\n",
    "# ec50_sound1 = ec50_sound1[start_ec50:end_ec50+1]\n",
    "# ec50_18_label = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ee0121-918b-4c85-8514-b82aceae4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sounds)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e3b08b-bc1b-4dde-bda8-b212fc4352a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30225,)\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(sounds_1[0][1].shape)\n",
    "print(type(labels_1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992df970-da8d-4c7a-ba72-3f97efe247fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound = np.expand_dims(sound, axis=1)\n",
    "# sound = np.expand_dims(sound, axis=3)\n",
    "# print(sound.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43e8d6c5-d9ce-461a-9ec9-497a8cef4ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total alarm sounds is 116\n",
      "total other sounds is 219(other:128 + siren:91)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type FLOAT32 but expected type INT8 for input 0, name: serving_default_input_2:0 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m s_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(s_test);\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(s_test.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(f\"the {w+1} item's shape:\\n  {s.shape}\");\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(f\"s type:{type(s)}, and s shape:{s.shape}\")\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtflite_interpreter_quant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_test\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     29\u001b[0m tflite_interpreter_quant\u001b[38;5;241m.\u001b[39minvoke()\n\u001b[1;32m     30\u001b[0m pred \u001b[38;5;241m=\u001b[39m tflite_interpreter_quant\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type FLOAT32 but expected type INT8 for input 0, name: serving_default_input_2:0 "
     ]
    }
   ],
   "source": [
    "test1_len = len(sounds_1);\n",
    "test2_len = len(sounds_2)\n",
    "len_alarm = len(p_wav_list);\n",
    "len_siren = len(n_wav_list_siren);\n",
    "len_other = len(n_wav_list_other);\n",
    "n_total_len = len_other + len_siren ;\n",
    "# s_total_len = len(n_wav_list_siren);\n",
    "print(f\"total alarm sounds is {len_alarm}\\ntotal other sounds is {n_total_len}(other:{len_other} + siren:{len_siren})\");\n",
    "test1_correct_num = 0;\n",
    "test2_correct_num = 0;\n",
    "test1_p_correct_num = 0;\n",
    "test2_p_correct_num = 0;\n",
    "n_correct_num = 0;\n",
    "n_correct_siren_num = 0;\n",
    "n_correct_othersounds_num = 0;\n",
    "\n",
    "for w in range(test1_len): \n",
    "    s = sounds_1[w]\n",
    "    l = labels_1[w]\n",
    "    s_test = np.expand_dims(s[0], axis=0);\n",
    "    s_test = np.expand_dims(s_test, axis=1);\n",
    "    s_test = np.expand_dims(s_test, axis=3);\n",
    "    s_test = np.float32(s_test);\n",
    "    # print(s_test.shape)\n",
    "    # print(f\"the {w+1} item's shape:\\n  {s.shape}\");\n",
    "    # Run inference\n",
    "    # print(f\"s type:{type(s)}, and s shape:{s.shape}\")\n",
    "    tflite_interpreter_quant.set_tensor(input_details[0]['index'], s_test);\n",
    "    tflite_interpreter_quant.invoke()\n",
    "    pred = tflite_interpreter_quant.get_tensor(output_details[0]['index'])\n",
    "    # print(f\"Prediction result shape:{pred.shape}\\n\");\n",
    "    print(f\"Prediction result: {pred}, and true label: {l}\")\n",
    "    if l == 52: #positive\n",
    "        if pred[0][0] > pred[0][1]:\n",
    "            test1_correct_num += 1;\n",
    "            test1_p_correct_num += 1;\n",
    "    if l == 99: #negative\n",
    "        if pred[0][0] < pred[0][1]:\n",
    "            test1_correct_num += 1;\n",
    "            n_correct_num += 1;\n",
    "            \n",
    "for w in range(test2_len): \n",
    "    s = sounds_2[w]\n",
    "    l = labels_2[w]\n",
    "    s_test = np.expand_dims(s[0], axis=0);\n",
    "    s_test = np.expand_dims(s_test, axis=1);\n",
    "    s_test = np.expand_dims(s_test, axis=3);\n",
    "    s_test = np.float32(s_test);\n",
    "    # print(s_test.shape)\n",
    "    # print(f\"the {w+1} item's shape:\\n  {s.shape}\");\n",
    "    # Run inference\n",
    "    # print(f\"s type:{type(s)}, and s shape:{s.shape}\")\n",
    "    tflite_interpreter_quant.set_tensor(input_details[0]['index'], s_test);\n",
    "    tflite_interpreter_quant.invoke()\n",
    "    pred = tflite_interpreter_quant.get_tensor(output_details[0]['index'])\n",
    "    # print(f\"Prediction result shape:{pred.shape}\\n\");\n",
    "    print(f\"Prediction result: {pred}, and true label: {l}\")\n",
    "    if l == 52: #positive\n",
    "        if pred[0][0] > pred[0][1]:\n",
    "            test2_correct_num += 1;\n",
    "            test2_p_correct_num += 1;\n",
    "    if l == 99: #negative\n",
    "        if pred[0][0] < pred[0][1]:\n",
    "            test2_correct_num += 1;\n",
    "            n_correct_siren_num += 1;\n",
    "print(f\"test1: total sounds(alarm+other+siren)test result is {100*(test1_correct_num/test1_len)}\");\n",
    "print(f\"test1: total alarm sounds result is {100*(test1_p_correct_num/len_alarm)}\");\n",
    "print(f\"test1 total other sounds(other+siren) result is {100*n_correct_num/n_total_len}\");\n",
    "print(\"**************************************************************************************\")\n",
    "print(f\"test2: total sounds(alarm+siren)test result is {100*(test2_correct_num/test2_len)}\")\n",
    "print(f\"test1: total alarm sounds result is {100*(test2_p_correct_num/len_alarm)}\");\n",
    "print(f\"test2 total siren sounds result is {100*n_correct_siren_num/len_siren}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "780eb0f5-3acb-4070-8164-00bcf30593c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.predict(sound, batch_size=len(sound), verbose=0);\n",
    "# print(type(scores))\n",
    "# print(scores.shape)\n",
    "\n",
    "# for res in scores:\n",
    "#     max_value = res.max()的\n",
    "#     max_index = np.argmax(res)\n",
    "#     print(f\"max value:{max_value:.5f} and index is {max_index}\")\n",
    "#     print('\\n'.join('{}: {:.5f}'.format(*k) for k in enumerate(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024bdc4-6093-4182-ab95-9f3688b6ff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999d37e-c098-4d06-a68c-a436d82a59f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
