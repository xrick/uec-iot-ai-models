{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d8b8dd-0a37-48f5-a3e6-ab9394b2e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import wavio\n",
    "import wave\n",
    "import subprocess\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dce1b0-d527-48ca-9127-c7d1f2b463db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../src/\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504fa365-3077-4983-a770-7f7c0caae1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.tlopts import display_info;\n",
    "import common.utils as U;\n",
    "from Libs.SharedLibs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1baae9-c56a-4100-a83f-0d8f5422e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Libs.SharedLibs import getFileList;\n",
    "from Libs.datetime_util import genDataTimeStr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7777f9bf-fa50-422f-882f-3b162b96d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_src_dir = \"../../datasets/CurrentUse/wav_files/\"\n",
    "wav_root_src_dir = \"../../datasets/CurrentUse/wav_files/Single_Fold/\"\n",
    "version_string = \"version4_home\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdaf164-e3ff-41e2-b69c-46fbe8646657",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## acdnet original dataset processing codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84828af0-81db-4170-821f-487ac2215ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_sr(src_path, dst_path, sr):\n",
    "#     print('* {} -> {}'.format(src_path, dst_path))\n",
    "#     if not os.path.exists(dst_path):\n",
    "#         os.mkdir(dst_path);\n",
    "#     for src_file in sorted(glob.glob(os.path.join(src_path, '*.wav'))):\n",
    "#         dst_file = src_file.replace(src_path, dst_path);\n",
    "#         subprocess.call('ffmpeg -i {} -ac 1 -ar {} -loglevel error -y {}'.format(\n",
    "#             src_file, sr, dst_file), shell=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f97bca-de87-4f26-bc38-9b29bd3619e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dataset(to_process_dir):\n",
    "#     wavlst = getFileList(to_process_dir);\n",
    "#     save_dir = os.path.join(to_process_dir, '20K');\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.mkdir(save_dir)\n",
    "#     sr = 20000;\n",
    "#     for src_file in wavlst:\n",
    "#         print(f\"original scc_file:{src_file}\")\n",
    "#         wav_name = os.path.basename(src_file)[:-4];\n",
    "#         wav_name = \"{}_20K.wav\".format(wav_name);\n",
    "#         save_path = os.path.join(save_dir, wav_name)\n",
    "#         print(f\"save dir:{save_path}\")\n",
    "#         framerate = 0;\n",
    "#         with wave.open(src_file, 'rb') as f:\n",
    "#             framerate = f.getframerate();\n",
    "#         if framerate != 20000:\n",
    "#             subprocess.call('ffmpeg -i {} -ac 1 -ar {} -loglevel error -y {}'.format(\n",
    "#                 src_file, sr, save_path), shell=True);\n",
    "#             print(f\"converted {src_file} sampling rate from {framerate} to 20K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fce383-e591-41e4-a342-f019f145f5a9",
   "metadata": {},
   "source": [
    "## Generating Single-Fold Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f11b8e-ce57-4b06-9a94-e76879d06e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_base_train_dataset(upper_level_dir=None, fold_dirs=None, p_classes=None, n_classes=None, export_path=None):\n",
    "def create_base_train_dataset(upper_level_dir=None, p_classes=None, n_classes=None, export_path=None):\n",
    "    total_counter = 0;\n",
    "    p_counter = 0;\n",
    "    n_counter = 0;\n",
    "    train_dataset = {};\n",
    "    dict_key = 'fold1';\n",
    "    # for fold in fold_dirs:\n",
    "    train_dataset[dict_key] = {}\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    for t in p_classes:\n",
    "    #Dealing with positive wav files\n",
    "        p_current_dir = os.path.join(upper_level_dir,'positive', t);\n",
    "        print(f\"work on dir:{p_current_dir}\");\n",
    "        lbl = t[t.rfind('_')+1:]\n",
    "        tmp_list = getFileList(p_current_dir)\n",
    "        for f in tmp_list:\n",
    "            sound = wavio.read(f).data.T[0]\n",
    "            start = sound.nonzero()[0].min()\n",
    "            end = sound.nonzero()[0].max()\n",
    "            sound = sound[start: end + 1]  # Remove silent sections\n",
    "            train_sounds.append(sound)\n",
    "            train_labels.append(lbl)\n",
    "            total_counter += 1;\n",
    "            p_counter += 1;\n",
    "            \n",
    "    # n_classes_root_dir = os.path.join(upper_level_dir,'negative')\n",
    "    for c in n_classes:\n",
    "        n_current_dir = os.path.join(upper_level_dir,'negative',c)\n",
    "        print(f\"work on dir:{n_current_dir}\");\n",
    "        n_lbl = 99;#c[:c.find('_')]\n",
    "        tmp_list2 = getFileList(n_current_dir)\n",
    "        for f in tmp_list2:\n",
    "            sound = wavio.read(f).data.T[0]\n",
    "            start = sound.nonzero()[0].min()\n",
    "            end = sound.nonzero()[0].max()\n",
    "            sound = sound[start: end + 1]  # Remove silent sections\n",
    "            train_sounds.append(sound)\n",
    "            train_labels.append(n_lbl)\n",
    "            total_counter += 1;\n",
    "            n_counter += 1;\n",
    "\n",
    "    train_dataset[dict_key]['sounds'] = train_sounds\n",
    "    train_dataset[dict_key]['labels'] = train_labels\n",
    "    np.savez(export_path, **train_dataset)\n",
    "    print(f\"Training Data is generated and save at {export_path}\")\n",
    "    print(f\"Total wav files for trainset is {total_counter}\");\n",
    "    print(f\"Total positive wav files for trainset is {p_counter}\");\n",
    "    print(f\"Total negative wav files for trainset is {n_counter}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f2b15c-e544-43bb-b632-c65faa5a4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_classes = [\"alarm_52\",\"help_mandrain_71\"]\n",
    "def main():\n",
    "    upper_level_dir = \"../../datasets/CurrentUse/wav_files/Single_Fold/train/\"\n",
    "    save_dir = os.path.join(\"../CurrentUse/generated_datasets/train/\",\"version4_home\");\n",
    "    if not pathlib.Path(save_dir).is_dir():\n",
    "        os.makedirs(save_dir);\n",
    "    output_path = os.path.join(save_dir,\"single_fold_train_{}.npz\".format(genDataTimeStr()));\n",
    "    p_dirs = ['alarm_52','refined_moan_wail_edited_20240530_56'];#[f for f in os.listdir(upper_level_dir+\"positive\")]\n",
    "    n_dirs = [f for f in os.listdir(upper_level_dir+\"negative\")]\n",
    "    print(f\"positive classes:{p_dirs}\")\n",
    "    print(\"negative classes:\")\n",
    "    count1 = 0\n",
    "    for c in n_dirs:\n",
    "        count1 += 1\n",
    "        print(f\"{count1}:{c}\")\n",
    "    dataset = create_base_train_dataset(upper_level_dir=upper_level_dir, p_classes=p_dirs, n_classes=n_dirs, export_path=output_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "396044d7-f429-466d-a9f8-a5efc5dd72d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive classes:['alarm_52', 'refined_moan_wail_edited_20240530_56']\n",
      "negative classes:\n",
      "1:20_crying_baby_esc50\n",
      "2:99_other_sounds\n",
      "3:17_pouring_water_esc50\n",
      "4:25_footsteps_esc50\n",
      "5:30_door_wood_knock_esc50\n",
      "6:28_snoring_esc50\n",
      "7:10_rain_esc50\n",
      "8:39_glass_breaking_esc50\n",
      "9:22_clap_esc50\n",
      "10:44_engine_esc50\n",
      "11:0_dog_esc50\n",
      "12:14_chirping_birds_esc50\n",
      "13:33_door_wood_creaks_esc50\n",
      "14:18_toilet_flush_esc50\n",
      "15:24_coughing_esc50\n",
      "16:27_brushing_teeth_esc50\n",
      "17:26_laughing_esc50\n",
      "18:21_sneezing_esc50\n",
      "19:35_wash_machine_esc50\n",
      "20:29_drinking_sipping_esc50\n",
      "21:43_car_horn_esc50\n",
      "22:5_cat_esc50\n",
      "23:15_water_drop_esc50\n",
      "24:36_vacuum_cleaner_esc50\n",
      "25:19_thunderstorm_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/positive/alarm_52\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/positive/refined_moan_wail_edited_20240530_56\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/20_crying_baby_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/99_other_sounds\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/17_pouring_water_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/25_footsteps_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/30_door_wood_knock_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/28_snoring_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/10_rain_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/39_glass_breaking_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/22_clap_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/44_engine_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/0_dog_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/14_chirping_birds_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/33_door_wood_creaks_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/18_toilet_flush_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/24_coughing_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/27_brushing_teeth_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/26_laughing_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/21_sneezing_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/35_wash_machine_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/29_drinking_sipping_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/43_car_horn_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/5_cat_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/15_water_drop_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/36_vacuum_cleaner_esc50\n",
      "work on dir:../../datasets/CurrentUse/wav_files/Single_Fold/train/negative/19_thunderstorm_esc50\n",
      "Training Data is generated and save at ../CurrentUse/generated_datasets/train/version4_home/single_fold_train_20240506160623.npz\n",
      "Total wav files for trainset is 655\n",
      "Total positive wav files for trainset is 345\n",
      "Total negative wav files for trainset is 310\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648bf9f0-f1f3-487b-a757-0ec646a6a2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '52', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', '56', 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99]\n"
     ]
    }
   ],
   "source": [
    "ds = np.load(\"../CurrentUse/generated_datasets/train/version4_home/single_fold_train_20240506160458.npz\", allow_pickle=True);\n",
    "print(ds['fold1'].item()['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f20e5-9dcc-4f5b-8def-8e22ef916031",
   "metadata": {},
   "source": [
    "## Generating Validation DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93313e93-ced4-427e-8aef-ca44b1608623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d299358-bb67-4ee2-86e1-d395ad658b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(len(samples));\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = len(samples);#88;#options.batchSize // options.nCrops;\n",
    "        print(f\"batch_size:{self.batch_size}\");\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.map_dict= {\n",
    "            '52':1, #alarm\n",
    "            '56':2, #moaning\n",
    "            '99':3, #other_sounds \n",
    "        };\n",
    "\n",
    "    def get_data(self):\n",
    "        #Generate one batch of data\n",
    "        x, y = self.generate();\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        x = np.expand_dims(x, axis=3)\n",
    "        # print(x.shape);\n",
    "        # print(y.shape);\n",
    "        return x, y\n",
    "\n",
    "    def generate(self):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            sound, target = self.data[i];\n",
    "            target = self.map_dict[str(target)] - 1;\n",
    "            sound = self.preprocess(sound).astype(np.float32)\n",
    "            # print(sound)\n",
    "            label = np.zeros((self.opt.nCrops, self.opt.nClasses));\n",
    "            label[:,target] = 1;\n",
    "            print(f\"nCrops:{self.opt.nCrops}, nClasses:{self.opt.nClasses}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "        \"\"\"\n",
    "        #dtype=\"object\" for ValueError: setting an array element with a sequence. \n",
    "        The requested array has an inhomogeneous shape after 1 dimensions. \n",
    "        The detected shape was (58,) + inhomogeneous part.\n",
    "        \"\"\"\n",
    "        sounds = np.asarray(sounds,dtype=\"object\")\n",
    "        # expand_sounds = np.expand_dims(np.asarray(sounds,dtype=\"object\"),axis=1); \n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"shape of sounds:{expand_sounds.shape}\")\n",
    "        sounds = sounds.reshape(sounds.shape[0]*sounds.shape[1], sounds.shape[2]);\n",
    "        labels = labels.reshape(labels.shape[0]*labels.shape[1], labels.shape[2]);\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.normalize(32768.0),\n",
    "                  U.multi_crop(self.opt.inputLength, 2)] # we use single crop here.\n",
    "\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5948757-00d1-4f41-81dd-1ee5ef912d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='ACDNet_TL_Model_Extend',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args();\n",
    "\n",
    "    \"\"\"\n",
    "    current best setting for accuracy: 96.5\n",
    "    opt.batchSize = 32;\n",
    "    opt.LR = 0.1;\n",
    "    opt.weightDecay = 5e-3;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.schedule = [0.3, 0.5, 0.9];\n",
    "    \"\"\"\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 64;\n",
    "    opt.LR = 0.1;\n",
    "    opt.weightDecay = 5e-4;#1e-2;#5e-3;#5e-4;\n",
    "    opt.momentum = 0.9;\n",
    "    opt.nEpochs = 800;\n",
    "    opt.schedule = [0.3, 0.5, 0.8];\n",
    "    opt.warmup = 10;\n",
    "    opt.device = 'cpu';\n",
    "    # if torch.backends.mps.is_available():\n",
    "    #     opt.device=\"mps\"; #for apple m2 gpu\n",
    "    # elif torch.cuda.is_available():\n",
    "    #     opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    # else:\n",
    "    #     opt.device=\"cpu\"\n",
    "    print(f\"***Use device:{opt.device}\");\n",
    "    # opt.device = torch.device(\"cuda:0\" if  else \"cpu\");\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 3#50;\n",
    "    opt.nFolds = 1;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.TLAcdnetConfig = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0b26c8-4efc-46bf-8068-9d26982eec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val_dataset_src_npy(dict_key='fold1', upper_level_dir=None, p_classes=None, n_classes=None, export_path=None):\n",
    "    train_dataset = {};\n",
    "    # dict_key = 'fold1';\n",
    "    # for fold in fold_dirs:\n",
    "    train_dataset[dict_key] = {}\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    total_counter = 0;\n",
    "    p_counter = 0;\n",
    "    n_counter = 0;\n",
    "    for t in p_classes:\n",
    "    #Dealing with positive wav files\n",
    "        p_current_dir = os.path.join(upper_level_dir,'positive', t);\n",
    "        print(f\"work on dir:{p_current_dir}\");\n",
    "        lbl = t[t.rfind('_')+1:]\n",
    "        tmp_list = getFileList(p_current_dir)\n",
    "        print(f\"class:{t}, item number:{len(tmp_list)}\")\n",
    "        for f in tmp_list:\n",
    "            sound = wavio.read(f).data.T[0]\n",
    "            start = sound.nonzero()[0].min()\n",
    "            end = sound.nonzero()[0].max()\n",
    "            sound = sound[start: end + 1]  # Remove silent sections\n",
    "            train_sounds.append(sound)\n",
    "            train_labels.append(lbl)\n",
    "            total_counter += 1;\n",
    "            p_counter += 1;\n",
    "    for c in n_classes:\n",
    "        n_current_dir = os.path.join(upper_level_dir,'negative',c)\n",
    "        print(f\"work on dir:{n_current_dir}\");\n",
    "        n_lbl = 99;#c[:c.find('_')]\n",
    "        tmp_list2 = getFileList(n_current_dir)\n",
    "        print(f\"class:{c}, item number:{len(tmp_list2)}\")\n",
    "        for f in tmp_list2:\n",
    "            sound = wavio.read(f).data.T[0]\n",
    "            start = sound.nonzero()[0].min()\n",
    "            end = sound.nonzero()[0].max()\n",
    "            sound = sound[start: end + 1]  # Remove silent sections\n",
    "            train_sounds.append(sound)\n",
    "            train_labels.append(n_lbl)\n",
    "            total_counter += 1;\n",
    "            n_counter += 1;\n",
    "\n",
    "    train_dataset[dict_key]['sounds'] = train_sounds\n",
    "    train_dataset[dict_key]['labels'] = train_labels\n",
    "    np.savez(export_path, **train_dataset)\n",
    "    print(f\"Test Data is generated and save at {export_path}\")\n",
    "    print(f\"Total wav files for trainset is {total_counter}\");\n",
    "    print(f\"Total positive wav files for trainset is {p_counter}\");\n",
    "    print(f\"Total negative wav files for trainset is {n_counter}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2879da4-29e7-4b36-9758-424e7938dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_main1():\n",
    "    val_upper_level_dir = \"../CurrentUse/wav_files/Single_Fold/val/\"\n",
    "    p_dirs = ['alarm_52','moaning_56'];#[f for f in os.listdir(upper_level_dir+\"positive\")]\n",
    "    n_dirs = [f for f in os.listdir(val_upper_level_dir+\"negative\")]\n",
    "    srcnpz_save_dir = \"version4_home\"\n",
    "    save_path = os.path.join(\"../CurrentUse/generated_datasets/val/\",srcnpz_save_dir);\n",
    "    if not pathlib.Path(save_path).is_dir():\n",
    "        os.makedirs(save_path)\n",
    "    output_fullname = os.path.join(\"../CurrentUse/generated_datasets/val/\", srcnpz_save_dir, \"single_fold_val_src_{}.npz\".format(genDataTimeStr()));\n",
    "    print(f\"positive classes:{p_dirs}\")\n",
    "    print(\"negative directories:\")\n",
    "    count1 = 0\n",
    "    for c in n_dirs:\n",
    "        count1 += 1\n",
    "        print(f\"{count1}:{c}\")\n",
    "    create_val_dataset_src_npy(upper_level_dir=val_upper_level_dir,p_classes=p_dirs,n_classes=n_dirs,export_path=output_fullname);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f89285b-772c-4ecc-b8bf-a743e51d1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive classes:['alarm_52', 'moaning_56']\n",
      "negative directories:\n",
      "1:29_drinking_sipping_esc50\n",
      "2:15_water_drop_esc50\n",
      "3:17_pouring_water_esc50\n",
      "4:19_thunderstorm_esc50\n",
      "5:5_cat_esc50\n",
      "6:30_door_wood_knock_esc50\n",
      "7:27_brushing_teeth_esc50\n",
      "8:36_vacuum_cleaner_esc50\n",
      "9:28_snoring_esc50\n",
      "10:26_laughing_esc50\n",
      "11:35_wash_machine_esc50\n",
      "12:33_door_wood_creaks_esc50\n",
      "13:21_sneezing_esc50\n",
      "14:39_glass_breaking_esc50\n",
      "15:10_rain_esc50\n",
      "16:14_chirping_birds_esc50\n",
      "17:99_other_sounds\n",
      "18:22_clap_esc50\n",
      "19:0_dog_esc50\n",
      "20:25_footsteps_esc50\n",
      "21:24_coughing_esc50\n",
      "22:44_engine_esc50\n",
      "23:43_car_horn_esc50\n",
      "24:20_crying_baby_esc50\n",
      "25:18_toilet_flush_esc50\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/positive/alarm_52\n",
      "class:alarm_52, item number:44\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/positive/moaning_56\n",
      "class:moaning_56, item number:41\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/29_drinking_sipping_esc50\n",
      "class:29_drinking_sipping_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/15_water_drop_esc50\n",
      "class:15_water_drop_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/17_pouring_water_esc50\n",
      "class:17_pouring_water_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/19_thunderstorm_esc50\n",
      "class:19_thunderstorm_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/5_cat_esc50\n",
      "class:5_cat_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/30_door_wood_knock_esc50\n",
      "class:30_door_wood_knock_esc50, item number:8\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/27_brushing_teeth_esc50\n",
      "class:27_brushing_teeth_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/36_vacuum_cleaner_esc50\n",
      "class:36_vacuum_cleaner_esc50, item number:8\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/28_snoring_esc50\n",
      "class:28_snoring_esc50, item number:8\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/26_laughing_esc50\n",
      "class:26_laughing_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/35_wash_machine_esc50\n",
      "class:35_wash_machine_esc50, item number:8\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/33_door_wood_creaks_esc50\n",
      "class:33_door_wood_creaks_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/21_sneezing_esc50\n",
      "class:21_sneezing_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/39_glass_breaking_esc50\n",
      "class:39_glass_breaking_esc50, item number:7\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/10_rain_esc50\n",
      "class:10_rain_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/14_chirping_birds_esc50\n",
      "class:14_chirping_birds_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/99_other_sounds\n",
      "class:99_other_sounds, item number:4\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/22_clap_esc50\n",
      "class:22_clap_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/0_dog_esc50\n",
      "class:0_dog_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/25_footsteps_esc50\n",
      "class:25_footsteps_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/24_coughing_esc50\n",
      "class:24_coughing_esc50, item number:10\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/44_engine_esc50\n",
      "class:44_engine_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/43_car_horn_esc50\n",
      "class:43_car_horn_esc50, item number:6\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/20_crying_baby_esc50\n",
      "class:20_crying_baby_esc50, item number:5\n",
      "work on dir:../CurrentUse/wav_files/Single_Fold/val/negative/18_toilet_flush_esc50\n",
      "class:18_toilet_flush_esc50, item number:5\n",
      "Test Data is generated and save at ../CurrentUse/generated_datasets/val/version4/single_fold_val_src_20240502120352.npz\n",
      "Total wav files for trainset is 236\n",
      "Total positive wav files for trainset is 85\n",
      "Total negative wav files for trainset is 151\n"
     ]
    }
   ],
   "source": [
    "val_main1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9e96fd8-7cc4-45dd-ae69-d9717039fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = np.load('../CurrentUse/generated_datasets/val/version4/single_fold_val_src_20240502120352.npz',allow_pickle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a392cbf-d3b5-492d-bfd0-b3116848e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset['fold1'].item()['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec33ed5a-f909-4b0d-83e1-02b379260cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_compress_npz(val_src_sounds=None, val_src_labels=None, export_path=None):\n",
    "    opt = getOpts();#opts.parse();\n",
    "    # display_info(opt);\n",
    "    # opt.batchSize=88;\n",
    "    opt.nCrops = 2;\n",
    "    opt.nClasses= 3;\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    val_sounds = [];\n",
    "    val_labels = [];\n",
    "   \n",
    "    start_time = time.perf_counter();\n",
    "    val_sounds.extend(val_src_sounds);\n",
    "    val_labels.extend(val_src_labels);\n",
    "    print(f\"len of val_sounds:{len(val_sounds)}, len of val_labels:{len(val_labels)}\")\n",
    "    \n",
    "    valGen = ValGenerator(val_src_sounds, val_src_labels, opt);\n",
    "    valX, valY = valGen.get_data();\n",
    "\n",
    "    np.savez_compressed(export_path, x=valX, y=valY);\n",
    "    print('compressed npz generated with\\n  shape x:{}\\n  y:{}\\n  took {:.2f} secs'.format(valX.shape, valY.shape, time.perf_counter()-start_time));\n",
    "    sys.stdout.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bf4e2-6290-4832-8a49-48d610c7d612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef50f6e-b053-4f8a-aaeb-db094556e67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7cd88e2-6fc7-443d-9239-66803b420fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dest_path is ../CurrentUse/generated_datasets/val/version4/final_single_val_20240502120516.npz\n",
      "***Use device:cpu\n",
      "len of val_sounds:236, len of val_labels:236\n",
      "236\n",
      "batch_size:236\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "nCrops:2, nClasses:3\n",
      "compressed npz generated with\n",
      "  shape x:(472, 1, 30225, 1)\n",
      "  y:(472, 3)\n",
      "  took 2.91 secs\n"
     ]
    }
   ],
   "source": [
    "src_val_data_npz = \"../CurrentUse/generated_datasets/val/version4/single_fold_val_src_20240502120352.npz\"\n",
    "val_data = np.load(src_val_data_npz, allow_pickle=True);\n",
    "dest_path = \"../CurrentUse/generated_datasets/val/version4/final_single_val_{}.npz\"\n",
    "#call to create validation set\n",
    "dest_path2 = dest_path.format(genDataTimeStr())\n",
    "print(f\"dest_path is {dest_path2}\")\n",
    "_sounds = val_data['fold1'].item()['sounds']\n",
    "_labels = val_data['fold1'].item()['labels']\n",
    "create_test_compress_npz(_sounds,_labels,dest_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd99bef3-2726-40b4-8847-e3a85d7f0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.load(\"../CurrentUse/generated_datasets/val/version4/final_single_val_20240502120516.npz\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc960f45-8f3d-46b7-be0d-7f8a47ba4780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(val_data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a22eb-e816-42cf-af14-57386d8fee4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05052a5-66dd-4923-9900-257d7cdb3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list = [52 for _ in range(87)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf31e394-fe95-41a5-8fc4-8374f548f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca78cdec-8267-4c39-a4db-7699497c98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fafe4d-8aad-4452-91f2-8f8c89d407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_lbl = 'alarm_52'\n",
    "# print(test_lbl[test_lbl.rfind('_')+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59b1e21-92cf-4125-b6e7-54d386ae81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_lbl2 = '12_rainfall'\n",
    "# print(test_lbl2[:test_lbl2.find('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002a424-e8a1-45fa-b94b-77d014b111d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
